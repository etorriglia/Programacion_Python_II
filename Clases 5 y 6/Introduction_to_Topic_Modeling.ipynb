{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGhqYGgU3ApZ"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oycZhFrc3Apa"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1CN7RIp3Apa"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqGPIn183Apa"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60RH6Mix4gM7",
        "outputId": "8456bf5a-57e3-46c0-82ed-ba656ffefad6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/'NIPS Papers'.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVl0VntP3qZa",
        "outputId": "3b68d203-6923-48fa-f0b5-912f4a97f223"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/NIPS Papers.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "W-cJH3wG3Apa",
        "outputId": "e59368ac-f992-4be5-e370-f6dd2a53aa69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-254f3aac-7766-4786-9f33-80d903f90a4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-254f3aac-7766-4786-9f33-80d903f90a4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-254f3aac-7766-4786-9f33-80d903f90a4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-254f3aac-7766-4786-9f33-80d903f90a4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f6d468e8-14c1-42c9-95f9-b3f4d09371ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6d468e8-14c1-42c9-95f9-b3f4d09371ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f6d468e8-14c1-42c9-95f9-b3f4d09371ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSoU0Z-e3Apa"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hwUZqvuu3Apb",
        "outputId": "a0745032-5643-4d5e-bf33-b26a518bba5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "3340  2010       Variable margin losses for classifier design   \n",
              "3144  2009  Hierarchical Learning of Dimensional Biases in...   \n",
              "3698  2011              Sparse Recovery with Brownian Sensing   \n",
              "569   1988  Simulation and Measurement of the Electric Fie...   \n",
              "358   1996  Compositionality, MDL Priors, and Object Recog...   \n",
              "\n",
              "                                               abstract  \\\n",
              "3340  The problem of controlling the margin of a cla...   \n",
              "3144  Existing models of categorization typically re...   \n",
              "3698  We consider the problem of recovering the para...   \n",
              "569                                    Abstract Missing   \n",
              "358                                    Abstract Missing   \n",
              "\n",
              "                                             paper_text  \n",
              "3340  Variable margin losses for classifier design\\n...  \n",
              "3144  Hierarchical Learning of Dimensional Biases in...  \n",
              "3698  Sparse Recovery with Brownian Sensing\\n\\nAlexa...  \n",
              "569   436\\n\\nSIMULATION AND MEASUREMENT OF\\nTHE ELEC...  \n",
              "358   Compositionality, MDL Priors, and\\nObject Reco...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-623dbd16-da4f-4384-afb0-b56f07a2c173\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3340</th>\n",
              "      <td>2010</td>\n",
              "      <td>Variable margin losses for classifier design</td>\n",
              "      <td>The problem of controlling the margin of a cla...</td>\n",
              "      <td>Variable margin losses for classifier design\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3144</th>\n",
              "      <td>2009</td>\n",
              "      <td>Hierarchical Learning of Dimensional Biases in...</td>\n",
              "      <td>Existing models of categorization typically re...</td>\n",
              "      <td>Hierarchical Learning of Dimensional Biases in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>2011</td>\n",
              "      <td>Sparse Recovery with Brownian Sensing</td>\n",
              "      <td>We consider the problem of recovering the para...</td>\n",
              "      <td>Sparse Recovery with Brownian Sensing\\n\\nAlexa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>1988</td>\n",
              "      <td>Simulation and Measurement of the Electric Fie...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>436\\n\\nSIMULATION AND MEASUREMENT OF\\nTHE ELEC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>1996</td>\n",
              "      <td>Compositionality, MDL Priors, and Object Recog...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Compositionality, MDL Priors, and\\nObject Reco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-623dbd16-da4f-4384-afb0-b56f07a2c173')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-623dbd16-da4f-4384-afb0-b56f07a2c173 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-623dbd16-da4f-4384-afb0-b56f07a2c173');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2b3ef2c6-1427-4599-b716-7263a4de4a1c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b3ef2c6-1427-4599-b716-7263a4de4a1c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2b3ef2c6-1427-4599-b716-7263a4de4a1c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1988,\n        \"max\": 2016,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          2016,\n          1992,\n          2008\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Robust Spatial Filtering with Beta Divergence\",\n          \"Avoiding False Positive in Multi-Instance Learning\",\n          \"Temporal Difference Learning in Continuous Time and Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function.  In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods.  A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.\",\n          \"Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models.  However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer.  This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties.  Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians.  This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.\",\n          \"We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Robust Spatial Filtering with Beta Divergence\\n\\nWojciech Samek1,4\\n\\n1\\n\\nDuncan Blythe1,4\\n\\n1,2\\n?\\nKlaus-Robert Muller\\n\\nMotoaki Kawanabe3\\n\\nMachine Learning Group, Berlin Institute of Technology (TU Berlin), Berlin, German\\n2\\nDepartment of Brain and Cognitive Engineering, Korea University, Seoul, Korea\\n3\\nATR Brain Information Communication Research Laboratory Group, Kyoto, Japan\\n4\\nBernstein Center for Computational Neuroscience, Berlin, Germany\\n\\nAbstract\\nThe efficiency of Brain-Computer Interfaces (BCI) largely depends upon a reliable\\nextraction of informative features from the high-dimensional EEG signal. A crucial step in this protocol is the computation of spatial filters. The Common Spatial\\nPatterns (CSP) algorithm computes filters that maximize the difference in band\\npower between two conditions, thus it is tailored to extract the relevant information in motor imagery experiments. However, CSP is highly sensitive to artifacts\\nin the EEG data, i.e. few outliers may alter the estimate drastically and decrease\\nclassification performance. Inspired by concepts from the field of information geometry we propose a novel approach for robustifying CSP. More precisely, we\\nformulate CSP as a divergence maximization problem and utilize the property of\\na particular type of divergence, namely beta divergence, for robustifying the estimation of spatial filters in the presence of artifacts in the data. We demonstrate the\\nusefulness of our method on toy data and on EEG recordings from 80 subjects.\\n\\n1\\n\\nIntroduction\\n\\nSpatial filtering is a crucial step in the reliable decoding of user intention in Brain-Computer Interfacing (BCI) [1, 2]. It reduces the adverse effects of volume conduction and simplifies the classification problem by increasing the signal-to-noise-ratio. The Common Spatial Patterns (CSP)\\n[3, 4, 5, 6] method is one of the most widely used algorithms for computing spatial filters in motor imagery experiments. A spatial filter computed with CSP maximizes the differences in band\\npower between two conditions, thus it aims to enhance detection of the synchronization and desynchronization effects occurring over different locations of the sensorimotor cortex after performing\\nmotor imagery. It is well known that CSP may provide poor results when artifacts are present in\\nthe data or when the data is non-stationary [7, 8]. Note that artifacts in the data are often unavoidable and can not always be removed by preprocessing, e.g. with Independent Component Analysis.\\nThey may be due to eye movements, muscle movements, loose electrodes, sudden changes of attention, circulation, respiration, external events, among the many possibilities. A straight forward way\\nto robustify CSP against overfitting is to regularize the filters or the covariance matrix estimation\\n[3, 7, 9, 10, 11]. Several other strategies have been proposed for estimating spatial filters under\\nnon-stationarity [12, 8, 13, 14].\\nIn this work we propose a novel approach for robustifying CSP inspired from recent results in the\\nfield of information geometry [15, 16]. We show that CSP may be formulated as a divergence\\nmaximization problem, in particular we prove by using Cauchy?s interlacing theorem [17] that the\\nspatial filters found by CSP span a subspace with maximum symmetric Kullback-Leibler divergence\\nbetween the distributions of both classes. In order to robustify the CSP algorithm against the influence of outliers we propose solving the divergence maximization problem with a particular type of\\n1\\n\\n\\fdivergence, namely beta divergence. This divergence has been successfully used for robustifying\\nalgorithms such as Independent Component Analysis (ICA) [18] and Non-negative Matrix Factorization (NMF) [19]. In order to capture artifacts on a trial-by-trial basis we reformulate the CSP\\nproblem as sum of trial-wise divergences and show that our method downweights the influence of\\nartifactual trials, thus it robustly integrates information from all trials.\\nThe remainder of this paper is organized as follows. Section 2 introduces the divergence-based\\nframework for CSP. Section 3 describes the beta-divergence CSP method and discusses its robustness property. Section 4 evaluates the method on toy data and EEG recordings from 80 subjects\\nand interprets the performance improvement. Section 5 concludes the paper with a discussion. An\\nimplementation of our method is available at http://www.divergence-methods.org.\\n\\n2\\n\\nDivergence-Based Framework for CSP\\n\\nSpatial filters computed by the Common Spatial Patterns (CSP) [3, 4, 5] algorithm have been widely\\nused in Brain-Computer Interfacing as they are well suited to discriminate between distinct motor\\nimagery patterns. A CSP spatial filter w maximizes the variance of band-pass filtered EEG signals\\nin one condition while minimizing it in the other condition. Mathematically the CSP solution can\\nbe obtained by solving the generalized eigenvalue problem\\n?1 wi = ?i ?2 wi\\n(1)\\nwhere ?1 and ?2 are the estimated (average) D ? D covariance matrices of class 1 and 2,\\nrespectively. Note that the spatial filters W = [w1 . . . wD ] can be sorted by importance\\n?1 = max{?1 , ?11 } > . . . > ?D = max{?D , ?1D }.\\n2.1\\n\\ndivCSP Algorithm\\n\\nInformation geometry [15] has provided useful frameworks for developing various machine learning\\n(ML) algorithms, e.g. by optimizing divergences between two different probability distributions [20]\\n[21]. In particular, a series of robust ML methods have been successfully obtained from Bregman\\ndivergences which are generalization of the Kullback-Leibler (KL) divergence [22]. Among them,\\nwe employ in this work the beta divergence. Before proposing our novel algorithm, we show that\\nCSP can also be interpreted as maximization of the symmetric KL divergence.\\nTheorem 1: Let W = [w1 . . . wd ] be the d top (sorted by ?i ) spatial filters computed by CSP and let\\n? be a d ? D dimensional\\n?1 and ?2 denote the covariance matrices of class 1 and 2. Let V> = RP\\nmatrix that can be decomposed into a whitening projection P ? RD?D (P(?1 + ?2 )P> = I) and\\n? ? Rd?D . Then\\nan orthogonal projection R\\nspan(W) = span(V? )\\n(2)\\n\\u0001\\n?\\n>\\n>\\n?\\nwith V = argmax Dkl V ?1 V || V ?2 V\\n(3)\\nV\\n\\n? kl (? || ?) denotes the symmetric Kullback-Leibler Divergence1 between zero mean Gauswhere D\\nsians and span(M) stands for the subspace spanned by the columns of matrix M. Note that [23]\\nhas provided a proof for the special case of one spatial filter, i.e. for V ? RD?1 .\\nProof: See appendix and supplement material.\\nThe objective function that is maximized in Eq. (3) can be written as\\n\\u0001\\n\\u0001\\n1\\n1\\nLkl (V) =\\ntr (V> ?1 V)?1 (V> ?2 V) + tr (V> ?2 V)?1 (V> ?1 V) ? d. (4)\\n2\\n2\\nIn order to cater for artifacts on a trial-by-trial basis we need to reformulate the above objective\\nfunction. Instead of maximizing the divergence between the average class distributions we propose\\nto optimize the sum of trial-wise divergences\\nLsumkl (V)\\n\\n=\\n\\nN\\nX\\n\\n\\u0001\\n? kl V> ?i V || V> ?i V ,\\nD\\n1\\n2\\n\\ni=1\\n1\\nThe symmetric Kullback-Leibler Divergence between distributions f (x) and g(x) is defined as\\nR\\nR\\n(x)\\n?\\nDkl (f (x) || g(x)) = f (x) ? log fg(x)\\ndx + g(x) ? log fg(x)\\ndx.\\n(x)\\n\\n2\\n\\n(5)\\n\\n\\fwhere ?i1 and ?i2 denote the covariance matrices estimated from the i-th trial of class 1 and class\\n2, respectively, and N is the number of trials per class. Note that the reformulated problem is\\nnot equivalent to CSP; in Eq. (4) averaging is performed w.r.t. the covariance matrices, whereas in\\nEq. (5) it is performed w.r.t. the divergences. We denote the former approach by kl-divCSP and the\\nlatter one by sumkl-divCSP. The following theorem relates both approaches in the asymptotic case.\\nTheorem 2: Suppose that the number of discriminative sources is one; then let c be such that\\nD/n ? c as D, n ? ? (D dimensions, n data points per trial). Then if there exists ?(c) with\\nN/D ? ?(c) for N ? ? (N the number of trials) then the empirical maximizer of Lsumkl (v)\\n(and of course also of Lkl (v)) converges almost surely to the true solution.\\nSketched Proof: See appendix.\\nThus Theorem 2 says that both divergence-based CSP variants kl-divCSP and sumkl-divCSP almost\\nsurely converge to the same (true) solution in the asymptotic case. The theorem can be easily\\nextended to multiple discriminative sources.\\n2.2\\n\\nOptimization Framework\\n\\nWe use the methods developed in [24], [25] and [26] for solving the maximization problems in\\nEq. (4) and Eq. (5). The projection V ? RD?d to the d-dimensional subspace can be decomposed\\ninto three parts, namely V> = Id RP where Id is an identity matrix truncated to the first d rows, R\\nis a rotation matrix with RR> = I and P is a whitening matrix. The optimization process consists\\nof finding the rotation R that maximizes our objective function and can be performed by gradient\\ndescent on the manifold of orthogonal matrices. More precisely, we start with an orthogonal matrix\\nR0 and find an orthogonal update U in the k-th step such that Rk+1 = URk . The update matrix\\nis chosen by identifying the direction of steepest descent in the set of orthogonal transformations\\nand then performing a line search along this direction to find the optimal step. Since the basis of\\nthe extracted subspace is arbitrary (one can right multiply a rotation matrix to V without changing\\nthe divergence), we select the principal axes of the data distribution of one class (after projection)\\nas basis in order to maximally separate the two classes. The optimization process is summarized in\\nAlgorithm 1 and explained in the supplement material of the paper.\\nAlgorithm 1 Divergence-based Framework for CSP\\n1: function DIV CSP(?1 , ?2 , d)\\n1\\n2:\\nCompute the whitening matrix P = ?? 2\\n3:\\nInitialise R0 with a random rotation matrix\\n4:\\nWhiten and rotate the data ?c = (R0 P)?c (R0 P)> with c = {1, 2}\\n5:\\nrepeat\\n6:\\nCompute the gradient matrix and determine the step size (see supplement material)\\n7:\\nUpdate the rotation matrix Rk+1 = URk\\n8:\\nApply the rotation to the data ?c = U?c U>\\n9:\\nuntil convergence\\n10:\\nLet V> = Id Rk+1 P\\n11:\\nRotate V by G ? Rd?d where G are eigenvectors of V> ?1 V\\n12:\\nreturn V\\n13: end function\\n\\n3\\n\\nBeta Divergence CSP\\n\\nRobustness is a desirable property of algorithms that work in data setups which are known to be\\ncontaminated by outliers. For example, in the biomedical fields, signals such as EEG may be highly\\naffected by artifacts, i.e. outliers, which may drastically influence statistical estimation. Note that\\nboth of the above approaches kl-divCSP and sumkl-divCSP are not robust w.r.t. artifacts as they\\nboth perform simple (non-robust) averaging of the covariance matrices and of the divergence terms,\\nrespectively. In this section we show that by using beta divergence we robustify the averaging of the\\ndivergence terms as beta divergence downweights the influence of outlier trials.\\n3\\n\\n\\fBeta divergence was proposed in [16, 27] and is defined (for ? > 0) as\\nZ\\nZ\\n1\\n1\\n?\\n?\\nD? (f (x) || g(x)) =\\n(f (x) ? g (x))f (x)dx ?\\n(f ?+1 (x) ? g ?+1 (x))dx,\\n?\\n?+1\\n\\n(6)\\n\\nwhere f (x) and g(x) are two probability distributions. Like every statistical divergence it is\\nalways positive and equals zero iff g = f [15]. The symmetric version of beta divergence\\n? ? (f (x) || g(x)) = D? (f (x) || g(x)) + D? (g(x) || f (x)) can be interpreted as discrepancy\\nD\\nbetween two probability distributions. One can show easily that beta and Kullback-Leibler divergence coincide as ? ? 0.\\nIn the context of parameter estimation, one can show that minimizing the divergence function from\\nan empirical distribution p to the statistical model q(?) is equivalent to maximizing the ?-likelihood\\n? ? (?)\\nL\\n?\\n? ? (q(?))\\nargmin D? (p || q(?)) = argmax L\\n?\\nq(?)\\n\\n(7)\\n\\nq(?)\\nn\\n\\nX\\nexp(?z) ? 1\\n? ? (q(?)) = 1\\nwith L\\n?? (`(xi , q(?))) ? b?? (?) and ?? (z) =\\n,\\n?\\nn i=1\\n?\\n\\n(8)\\n\\nwhere `(xi R, q(?)) denotes the log-likelihood of observation xi and distribution q(?), and b?? (?) :=\\n(? + 1)?1 q(?)?+1 dx. Basu et al. [27] showed that the ?-likelihood method weights each observation according to the magnitude of likelihood evaluated at the observation; if an observation is an\\noutlier, i.e. of lower likelihood, then it is downweighted. Thus, beta divergence allows to construct\\nrobust estimators as samples with low likelihood are downweighted (see also M-estimators [28]).\\n?-divCSP Algorithm\\nWe propose applying beta divergence to the objective function in Eq. (5) in order to downweight the\\ninfluence of artifacts in the computation of spatial filters. An overview over the different divergencebased CSP variants is provided in Figure 1. The objective function of our ?-divCSP approach is\\nX\\n\\u0001\\n? ? VT ?i1 V || VT ?i2 V\\nL? (V) =\\nD\\n(9)\\ni\\n\\n=\\n\\n\\u0012Z\\n\\u0013\\nZ\\nZ\\nZ\\n1X\\n?+1\\n?+1\\n?\\n?\\ngi dx + fi dx ? fi gi dx ? fi gi dx ,\\n? i\\n\\n(10)\\n\\n\\u0001\\n\\u0001\\n? i and gi = N 0, ?\\n? i being the zero-mean Gaussian distributions with prowith fi = N 0, ?\\n1\\n2\\n? i = VT ?i V ? Rd?d and ?\\n? i = VT ?i V ? Rd?d , respectively.\\njected covariances ?\\n1\\n1\\n2\\n2\\nOne can show easily (see the supplement file to this paper) that L? (V) has an explicit form\\n\\u0010\\n\\u0011\\u0011\\nX\\u0010\\n? i |? ?2 + |?\\n? i |? ?2 ? (? + 1) d2 |?\\n? i | 1??\\n?i + ?\\n? i |? 12 + |?\\n? i | 1??\\n?i + ?\\n? i |? 12\\n2 |? ?\\n2 |? ?\\n?\\n|?\\n, (11)\\n1\\n2\\n2\\n1\\n2\\n1\\n2\\n1\\ni\\n\\nq\\nwith ? = ?1 (2?)?d1(?+1)d . We use Algorithm 1 to maximize the objective function of ?-divCSP.\\nIn the following we show that the robustness property of ?-divCSP can be directly understood from\\ninspection of its objective function.\\n? i and ?\\n? i are full rank d ? d covariance matrices. We investigate the behaviour of the\\nAssume ?\\n1\\n2\\n? i is constant and ?\\n? i becomes very\\nobjective functions of ?-divCSP and sumkl-divCSP when ?\\n1\\n2\\nlarge, e.g. because it is affected by artifacts. It is not hard to see that for ? > 0 the objective\\n? i becomes arbitrarily large. The first term\\nfunction L? does not go to infinity but is constant as ?\\n2\\n?\\n? i |? 2 is constant with respect to changes of ?\\n? i and all the other terms\\nof the objective function |?\\n1\\n2\\n? i increases. Thus the influence function of the ?-divCSP estimator is bounded w.r.t.\\ngo to zero as ?\\n2\\n? i (the same argument holds for changes of ?\\n? i ). Note that this robustness property\\nchanges in ?\\n2\\n1\\n\\u0001\\n? i )?1 ?\\n? i is\\nvanishes when applying Kullback-Leibler divergences Eq. (4) as the trace term tr (?\\n1\\n2\\n? i becomes arbitrarily large, thus this artifactual term will dominate the solution.\\nnot bounded when ?\\n2\\n4\\n\\n\\fCSP\\n\\nrobust\\n\\nsum\\n-divCSP\\n\\n-divCSP\\n\\n-divCSP\\n\\nFigure 1: Relation between the different CSP formulations outlined in this paper.\\n\\n4\\n4.1\\n\\nExperimental Evaluation\\nSimulations\\n\\nIn order to investigate the effects of artifactual trials on CSP and ?-divCSP we generate data x(t)\\nusing the following mixture model\\n\\u0014 dis \\u0015\\ns (t)\\nx(t) = A ndis\\n+ \\u000f,\\n(12)\\ns\\n(t)\\nwhere A ? R10?10 is a random orthogonal mixing matrix, sdis is a discriminative source sampled\\nfrom a zero mean Gaussian with variance 1.8 in one condition and 0.2 in the other one, sndis are 9\\nsources with variance 1 in both conditions and \\u000f is a noise variable with variance 2. We generate\\n100 trials per condition, each consisting of 200 data points. Furthermore we randomly add artifacts\\nwith variance 10 independently to each data dimension (i.e. virtual electrode) and trial with varying\\nprobability and evaluate the angle between the true filter extracting the source activity of sdis and\\nthe spatial filter computed by CSP and ?-divCSP. The median angles are shown in Figure 2 using\\n100 repetitions. One can clearly see that the angle error between the spatial filter extracted by CSP\\nand the true one increases with larger artifact probability. Furthermore one can see from the figure\\nthat using very small ? values does not attenuate the artefact problem, but it rather increases the\\nerror by adding up trial-wise divergences without downweighting outliers. However, as the ? value\\nincreases the artifactual trials are downweighted and a robust average is computed over the trial-wise\\ndivergence terms. This increased robustness significantly reduces the angle error.\\nprob. of outlier 0\\n\\nprob. of outlier 0.001\\n\\n40\\n\\n80\\nangle error (in ?)\\n\\n60\\n\\n60\\n\\n40\\n\\nCSP\\n?-divCSP\\n\\n60\\n\\n40\\n\\n60\\n\\n40\\n\\n80\\nangle error (in ?)\\n\\nangle error (in ?)\\n\\n80\\n\\n60\\n\\n40\\n\\n60\\n\\n40\\n\\n0.75\\n\\n1\\n\\n0.5\\n\\n0.25\\n\\n0.1\\n\\n0.01\\n\\n0.001\\n\\nbeta value\\n\\n2\\n\\n1.5\\n\\n0.75\\n\\n1\\n\\n0.5\\n\\n0.25\\n\\n0.1\\n\\n0.01\\n\\n0.001\\n\\n2\\n\\n1.5\\n\\n0.75\\n\\n1\\n\\n0\\n\\n0.5\\n\\n0\\n\\n0.25\\n\\n0\\n\\n0.1\\n\\n20\\n\\n0.01\\n\\n20\\n\\n0.001\\n\\n20\\n\\nbeta value\\n\\n2\\n\\nbeta value\\nprob. of outlier 0.05\\n\\n1.5\\n\\nbeta value\\nprob. of outlier 0.02\\n\\n2\\n\\nbeta value\\nprob. of outlier 0.01\\n\\n1.5\\n\\n1\\n\\n0.75\\n\\n0.5\\n\\n0.25\\n\\n0.1\\n\\n0.01\\n\\n0.001\\n\\n2\\n\\n1.5\\n\\n1\\n\\n0.75\\n\\n0.5\\n\\n0.25\\n\\n0.1\\n\\n0.01\\n\\n0.001\\n\\n2\\n\\n1.5\\n\\n1\\n\\n0.75\\n\\n0\\n\\n0.5\\n\\n0\\n\\n0.25\\n\\n0\\n\\n0.1\\n\\n20\\n\\n0.01\\n\\n20\\n\\n0.001\\n\\n20\\n\\n80\\nangle error (in ?)\\n\\nprob. of outlier 0.005\\n\\n80\\nangle error (in ?)\\n\\nangle error (in ?)\\n\\n80\\n\\nbeta value\\n\\nFigure 2: Angle between the true spatial filter and the filter computed by CSP and ?-divCSP for\\ndifferent probabilities of artifacts. The robustness of our approach increases with the ? value and\\nsignificantly outperforms the CSP solution.\\n\\n5\\n\\n\\f4.2\\n\\nData Sets and Experimental Setup\\n\\nThe data set [29] used for the evaluation contains EEG recordings from 80 healthy BCIinexperienced volunteers performing motor imagery tasks with the left and right hand or feet. The\\nsubjects performed motor imagery first in a calibration session and then in a feedback mode in which\\nthey were required to control a 1D cursor application. Activity was recorded from the scalp with\\nmulti-channel EEG amplifiers using 119 Ag/AgCl electrodes in an extended 10-20 system sampled\\nat 1000 Hz (downsampled to 100 Hz) and a band-pass from 0.05 to 200 Hz. Three runs with 25\\ntrials of each motor condition were recorded in the calibration session and the two best classes were\\nselected; the subjects performed feedback with three runs of 100 trials. Both sessions were recorded\\non the same day.\\nFor the offline analysis we manually select 62 electrodes densely covering the motor cortex, extract\\na time segment located from 750ms to 3500ms after the cue indicating the motor imagery class and\\nfilter the signal in 8-30 Hz using a 5-th order Butterworth filter. We do not apply manual or automatic\\nrejection of trials or electrodes and use six spatial filters for feature extraction. For classification\\nwe apply Linear Discriminant Analysis (LDA) after computing the logarithm of the variance on\\nthe spatially filtered data. We measure performance as misclassification rate and normalize the\\ncovariance matrices by dividing them by their traces. The parameter ? is selected from the set of\\n15 candidates {0, 0.0001, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1, 1.5, 2, 5} by 5-fold\\ncross-validation on the calibration data using minimal training error rate as selection criterion. For\\nfaster convergence we use the rotation part of the CSP solution as initial rotation matrix R0 .\\n4.3\\n\\nResults\\n\\nWe compare our ?-divCSP method with three CSP baselines using different estimators for the covariance matrices. The first baseline uses the standard empirical estimator, the second one applies a\\nstandard analytic shrinkage estimator [9] and the third one relies on the minimum covariance determinant (MCDE) estimate [30]. Note that the shrinkage estimator usually provides better estimates\\nin small-sample settings, whereas MCDE is robust to outliers. In order to perform a fair comparison\\nwe applied MCDE over various ranges [0, 0.05, 0.1 . . . 0.5] of parameters and selected the best one\\nby cross-validation (as with ?-divCSP). The MCDE parameter determines the expected proportion\\nof artifacts in the data. The results are shown in Figure 3. Each circle denotes the error rate of one\\nsubject. One can see that the ?-divCSP method outperforms the baselines as most circles are below the solid line. Furthermore the performance increases are significant according to the one-sided\\nWilcoxon sign rank test as the p-values are smaller than 0.05.\\n\\n40\\n\\n20\\n\\n60\\n\\n?-divCSP error rate [%]\\n\\n60\\n\\n?-divCSP error rate [%]\\n\\n?-divCSP error rate [%]\\n\\n60\\n\\n40\\n\\n20\\n\\np = 0.0005\\n0\\n\\n0\\n\\n20\\n40\\nCSP error rate [%]\\n\\n40\\n\\n20\\n\\np = 0.0178\\n60\\n\\n0\\n\\n0\\n\\n20\\n40\\nshrinkCSP error rate [%]\\n\\np = 0.0407\\n60\\n\\n0\\n\\n0\\n\\n20\\n40\\nMCDE+CSP error rate [%]\\n\\n60\\n\\nFigure 3: Performance results of the CSP, shrinkage + CSP and MCDE + CSP baselines compared to\\n?-divCSP. Each circle represents the error rate of one subject. Our method outperforms the baselines\\nfor circles that are below the solid line. The p-values of the one-sided Wilcoxon sign rank test are\\nshown in the lower right corner.\\nWe made an interesting observation when analysing the subject with largest improvement over the\\nCSP baseline; the error rates were 48.6% (CSP), 48.6% (MCDE+CSP) and 11.0% (?-divCSP).\\nOver all ranges of MCDE parameters this subject has an error rate higher than 48% i.e. MCDE\\nwas not able help in this case. This example shows that ?-divCSP and MCDE are not equivalent.\\nEnforcing robustness on the CSP algorithm may in some cases be better than enforcing robustness\\nwhen estimating the covariance matrices.\\n6\\n\\n\\fIn the following we study the robustness property of the ?-divCSP method on subject 74, the user\\nwith the largest improvement (CSP error rate: 48.6 % and ?-divCSP error rate: 11.0 %). The left\\npanel of Figure 4 displays the activity pattern associated with the most important CSP filter of subject\\n74. One can clearly see that the pattern does not encode neurophysiologically relevant activity,\\nbut focuses on a single electrode, namely FFC6. When analysing the (filtered) EEG signal of this\\nelectrode one can identify a strong artifact in one of the trials. Since neither the empirical covariance\\nestimator nor the CSP algorithm is robust to this kind of outliers, it dominates the solution. However,\\nthe resulting pattern is meaningless as it does not capture motor imaginary related activity. The right\\npanel of Figure 4 shows the relative importance of the divergence term of the artifactual trial with\\nrespect to the average divergence terms of the other trials. One can see that the divergence term\\ncomputed from the artifactual trial is over 1800 times larger than the average of the other trials. This\\nratio decreases rapidly for larger ? values, thus the influence of the artifact decreases. Thus, our\\nexperiments provide an excellent example of the robustness property of the ?-divCSP approach.\\n2000\\n1800\\n\\nartefact in FFC6\\n\\nPercentage of artefact term\\n\\n1600\\n1400\\n1200\\n1000\\n800\\n600\\n400\\n200\\n5\\n\\n1.5\\n\\n2\\n\\n0.75\\n\\n1\\n\\n0.25\\n\\n0.5\\n\\n0.15\\n\\n0.2\\n\\n0.05\\n\\n0.01\\n\\n0.1\\n\\n0.001\\n\\n0\\n\\nCSP pattern\\n\\n0.0001\\n\\n0\\n\\nbeta value\\n\\nFigure 4: Left: The CSP pattern of subject 74 does not reflect neurophysiological activity but it represents the artifact (red ellipse) in electrode FFC6. Right: The relative importance of this artifactual\\ntrial decreases with the ? parameters. The relative importance is measured as quotient between the\\ndivergence term of the artifactual trial and the average divergence terms of the other trials.\\n\\n5\\n\\nDiscussion\\n\\nAnalysis of EEG data is challenging because the signal of interest is typically present with a low\\nsignal to noise ratio. Moreover artifacts and non-stationarity require robust algorithms. This paper\\nhas placed its focus on a robust estimation and proposed a novel algorithm family giving rise to a beta\\ndivergence algorithm which allows robust spatial filter computation for BCI. In the very common\\nsetting where EEG electrodes become loose or movement related artifacts occur in some trials, it\\nis a practical necessity to either ignore these trials (which reduces an already small sample size\\nfurther) or to enforce intrinsic invariance to these disturbances into the learning procedures. Here,\\nwe have used CSP, the standard filtering technique in BCI, as a starting point and reformulated it\\nin terms of an optimization problem maximizing the divergence between the class-distributions that\\ncorrespond to two cognitive states. By borrowing the concept of beta divergences, we could adapt\\nthe optimization problem and arrive at a robust spatial filter computation based on CSP. We showed\\nthat our novel method can reduce the influence of artifacts in the data significantly and thus allows\\nto robustly extract relevant filters for BCI applications.\\nIn future work we will investigate the properties of other divergences for Brain-Computer Interfacing\\nand consider also further applications like ERP-based BCIs [31] and beyond the neurosciences.\\nAcknowledgment We thank Daniel Bartz and Frank C. Meinecke for valuable discussions. This\\nwork was supported by the German Research Foundation (GRK 1589/1), by the Federal Ministry\\nof Education and Research (BMBF) under the project Adaptive BCI (FKZ 01GQ1115) and by the\\nBrain Korea 21 Plus Program through the National Research Foundation of Korea funded by the\\nMinistry of Education.\\n7\\n\\n\\fAppendix\\nSketch of proof of Theorem 1\\nCauchy?s interlacing theorem [17] establishes a relation between the eigenvalues ?1 ? . . . ? ?D of\\nthe original covariance matrix ? and the eigenvalues ?1 ? . . . ? ?d of the projected one V?V> .\\nThe theorem says that\\n?j ? ?j ? ?D?d+j .\\nIn the proof we split the optimal projection V? into two parts U1 ? Rk?D and U2 ? Rd?k?D based\\non whether the first or second trace term in Eq. (4) is larger when applying the spatial filters. By\\nusing Cauchy?s theorem we then show that Lkl (U) ? Lkl (W) where W consists of k eigenvectors\\nwith largest eigenvalues; equality only holds if U and W coincide (up to linear transformations).\\nWe show an analogous relation for U2 and conclude that V? must be the CSP solution (up to linear\\ntransformations). See the full proof in the supplement material.\\nSketch of the proof of Theorem 2\\nSince there is only one discriminative direction we may perform analysis in a basis whereby the\\ncovariances of both classes have the form diag(a, 1, . . . , 1) and diag(b, 1, . . . , 1). If we show in this\\nbasis that consistency holds then it is a simple matter to prove consistency in the original basis. We\\nwant to show that as the number of trials N increases the filter provided by sumkl-divCSP converges\\nto the true solution v? . If the support of the density of the eigenvalues includes a region around 0,\\nthen there is no hope of showing that the matrix inversion is stable. However, it has been shown\\nin the random matrix theory literature [32] that if D and n tend to ? in a ratio c = D\\nn then all of\\n?\\n?\\nthe eigenvalues apart from the largest lie between (1 ? c)2 and (1 + c)2 whereas the largest\\n?\\nsample eigenvalue (? denotes the true non-unit eigenvalue) converges almost surely to ? + c ??1\\n?\\nprovided ? > 1 + c, independently of the distribution of the data; a similar result applies if one\\ntrue eigenvalue is smaller than the rest. This implies that for sufficient discriminability in the true\\ndistribution and sufficiently many data points per trial, each filter maximizing each term in the sum\\nhas non-zero dot-product with the true maximizing filter. But since the trials are independent, this\\nimplies that in the limit of N trials the maximizing filter corresponds to the true filter. Note that the\\nfull proof goes well beyond the scope of this contribution.\\n\\nReferences\\n[1] G. Dornhege, J. del R. Mill?an, T. Hinterberger, D. McFarland, and K.-R. M?uller, Eds., Toward\\nBrain-Computer Interfacing. Cambridge, MA: MIT Press, 2007.\\n[2] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M. Vaughan, ?Braincomputer interfaces for communication and control,? Clin. Neurophysiol., vol. 113, no. 6, pp.\\n767?791, 2002.\\n[3] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. M?uller, ?Optimizing Spatial\\nfilters for Robust EEG Single-Trial Analysis,? IEEE Signal Proc. Magazine, vol. 25, no. 1, pp.\\n41?56, 2008.\\n[4] H. Ramoser, J. M?uller-Gerking, and G. Pfurtscheller, ?Optimal spatial filtering of single trial\\neeg during imagined hand movement,? IEEE Trans. Rehab. Eng., vol. 8, no. 4, pp. 441?446,\\n1998.\\n[5] L. C. Parra, C. D. Spence, A. D. Gerson, and P. Sajda, ?Recipes for the linear analysis of eeg,?\\nNeuroImage, vol. 28, pp. 326?341, 2005.\\n[6] S. Lemm, B. Blankertz, T. Dickhaus, and K.-R. M?uller, ?Introduction to machine learning for\\nbrain imaging,? NeuroImage, vol. 56, no. 2, pp. 387?399, 2011.\\n[7] F. Lotte and C. Guan, ?Regularizing common spatial patterns to improve bci designs: Unified\\ntheory and new algorithms,? IEEE Trans. Biomed. Eng., vol. 58, no. 2, pp. 355 ?362, 2011.\\n[8] W. Samek, C. Vidaurre, K.-R. M?uller, and M. Kawanabe, ?Stationary common spatial patterns\\nfor brain-computer interfacing,? Journal of Neural Engineering, vol. 9, no. 2, p. 026013, 2012.\\n[9] O. Ledoit and M. Wolf, ?A well-conditioned estimator for large-dimensional covariance matrices,? Journal of Multivariate Analysis, vol. 88, no. 2, pp. 365 ? 411, 2004.\\n8\\n\\n\\f[10] H. Lu, H.-L. Eng, C. Guan, K. Plataniotis, and A. Venetsanopoulos, ?Regularized common\\nspatial pattern with aggregation for eeg classification in small-sample setting,? IEEE Transactions on Biomedical Engineering, vol. 57, no. 12, pp. 2936?2946, 2010.\\n[11] D. Devlaminck, B. Wyns, M. Grosse-Wentrup, G. Otte, and P. Santens, ?Multi-subject learning\\nfor common spatial patterns in motor-imagery bci,? Computational Intelligence and Neuroscience, vol. 2011, no. 217987, pp. 1?9, 2011.\\n[12] B. Blankertz, M. K. R. Tomioka, F. U. Hohlefeld, V. Nikulin, and K.-R. M?uller, ?Invariant\\ncommon spatial patterns: Alleviating nonstationarities in brain-computer interfacing,? in Ad.\\nin NIPS 20, 2008, pp. 113?120.\\n[13] W. Samek, F. C. Meinecke, and K.-R. M?uller, ?Transferring subspaces between subjects in\\nbrain-computer interfacing,? IEEE Transactions on Biomedical Engineering, vol. 60, no. 8,\\npp. 2289?2298, 2013.\\n[14] M. Arvaneh, C. Guan, K. K. Ang, and C. Quek, ?Optimizing spatial filters by minimizing\\nwithin-class dissimilarities in electroencephalogram-based brain-computer interface,? IEEE\\nTrans. Neural Netw. Learn. Syst., vol. 24, no. 4, pp. 610?619, 2013.\\n[15] S. Amari, H. Nagaoka, and D. Harada, Methods of information geometry. American Mathematical Society, 2000.\\n[16] S. Eguchi and Y. Kano, ?Robustifying maximum likelihood estimation,? Tokyo Institute of\\nStatistical Mathematics, Tokyo, Japan, Tech. Rep, 2001.\\n[17] R. Bhatia, Matrix analysis, ser. Graduate Texts in Mathematics. Springer, 1997, vol. 169.\\n[18] M. Mihoko and S. Eguchi, ?Robust blind source separation by beta divergence,? Neural Comput., vol. 14, no. 8, pp. 1859?1886, Aug. 2002.\\n[19] C. F?evotte and J. Idier, ?Algorithms for nonnegative matrix factorization with the &#946;divergence,? Neural Comput., vol. 23, no. 9, pp. 2421?2456, Sep. 2011.\\n[20] A. Hyv?arinen, ?Survey on independent component analysis,? Neural Computing Surveys,\\nvol. 2, pp. 94?128, 1999.\\n[21] M. Kawanabe, W. Samek, P. von B?unau, and F. Meinecke, ?An information geometrical view\\nof stationary subspace analysis,? in Artificial Neural Networks and Machine Learning - ICANN\\n2011, ser. LNCS. Springer Berlin / Heidelberg, 2011, vol. 6792, pp. 397?404.\\n[22] N. Murata, T. Takenouchi, and T. Kanamori, ?Information geometry of u-boost and bregman\\ndivergence,? Neural Computation, vol. 16, pp. 1437?1481, 2004.\\n[23] H. Wang, ?Harmonic mean of kullbackleibler divergences for optimizing multi-class eeg\\nspatio-temporal filters,? Neural Processing Letters, vol. 36, no. 2, pp. 161?171, 2012.\\n[24] P. von B?unau, F. C. Meinecke, F. C. Kir?aly, and K.-R. M?uller, ?Finding Stationary Subspaces\\nin Multivariate Time Series,? Physical Review Letters, vol. 103, no. 21, pp. 214 101+, 2009.\\n[25] P. von B?unau, ?Stationary subspace analysis - towards understanding non-stationary data,?\\nPh.D. dissertation, Technische Universit?at Berlin, 2012.\\n[26] W. Samek, M. Kawanabe, and K.-R. M?uller, ?Divergence-based framework for common spatial patterns algorithms,? IEEE Reviews in Biomedical Engineering, 2014, in press.\\n[27] A. Basu, I. R. Harris, N. L. Hjort, and M. C. Jones, ?Robust and efficient estimation by minimising a density power divergence,? Biometrika, vol. 85, no. 3, pp. 549?559, 1998.\\n[28] P. J. Huber, Robust Statistics, ser. Wiley Series in Probability and Statistics.\\nWileyInterscience, 1981.\\n[29] B. Blankertz, C. Sannelli, S. Halder, E. M. Hammer, A. K?ubler, K.-R. M?uller, G. Curio,\\nand T. Dickhaus, ?Neurophysiological predictor of smr-based bci performance,? NeuroImage,\\nvol. 51, no. 4, pp. 1303?1309, 2010.\\n[30] P. J. Rousseeuw and K. V. Driessen, ?A fast algorithm for the minimum covariance determinant\\nestimator,? Technometrics, vol. 41, no. 3, pp. 212?223, 1999.\\n[31] B. Blankertz, S. Lemm, M. S. Treder, S. Haufe, and K.-R. M?uller, ?Single-trial analysis and\\nclassification of ERP components ? a tutorial,? NeuroImage, vol. 56, no. 2, pp. 814?825, 2011.\\n[32] J. Baik and J. Silverstein, ?Eigenvalues of large sample covariance matrices of spiked population models,? Journal of Multivariate Analysis, vol. 97, no. 6, pp. 1382?1408, 2006.\\n\\n9\\n\\n\\f\",\n          \"Avoiding False Positive in Multi-Instance Learning\\n\\nYanjun Han, Qing Tao, Jue Wang\\nInstitute of Automation, Chinese Academy of Sciences\\nBeijing, 100190, China\\nyanjun.han, qing.tao, jue.wang@ia.ac.cn\\n\\nAbstract\\nIn multi-instance learning, there are two kinds of prediction failure, i.e., false\\nnegative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive\\nbags to avoid both the former and the latter. Based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place\\npositive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.\\n\\n1 Introduction\\nMulti-instance Learning (MIL) was first proposed by Dietterich et.al. in [1] to predict the binding\\nability of a drug from its biochemical structure. A certain drug molecule corresponds to a set of\\nconformations which cannot be differentiated via chemical experiments. A drug is labeled positive\\nif any of its constituent conformations has the binding ability greater than the threshold, otherwise\\nnegative. Therefore, each sample (a drug) is a bag of instances (its constituent conformations). In\\nmulti-instance learning the label information for positive samples is incomplete in that the instances\\nin a certain positive bag are all labeled positive. Generally, methods for multi-instance learning are\\nmodified versions of approaches for supervised learning by shifting the focus from discrimination\\non instances to discrimination on bags.\\nThe earliest exploration were the APR algorithms proposed in [1]. From then on, a number of\\napproaches emerged. Examples include Diverse Density [2], Citation k?NN [3], MI-SVMs [4], MIkernels [5], reg-SVM [6], MissSVM [7], sbMIL, stMIL [8], PPMM [9], MIGraphs [10], etc. Many\\nreal-world applications can be regarded as Multi-instance learning problems. Examples include\\nimage classification [11], document categorization [12], computer aided diagnosis [13], etc.\\nAs far as positive bags are concerned, current research usually treat them as labyrinths in which\\nwitnesses (responsible positive instances) are encaged, and consider nonwitnesses (other instances)\\ntherein to be useless or even distractive. The information carried by nonwitnesses is not well utilized.\\nFactually, nonwitnesses are indispensable for characterizing the overall instance distribution, and\\nthus help to improve the learner. Several researchers realized the importance of nonwitnesses and\\nattempted to utilize them. In MI-kernels [5] and reg-SVM [6], nonwitnesses together with witnesses\\nare squeezed into the kernel matrix. In mi-SVM [4], the labels of all nonwitnesses are treated as\\nunknown integer variables to be optimized. mi-SVM tends to misclassify negative instances in\\npositive bags since the resulted margin will be larger. And we will elaborate on this flaw in section\\n3.1. In MissSVM [7] and stMIL [8], multi-instance learning is addressed from the view of semisupervised learning, and nonwitnesses are treated as unlabeled data, whose labels should be assigned\\nto maximize the margin. sbMIL [8] attempt to estimate the ratio of positive instances inside positive\\nbags and utilize this information in the subsequent classification. MissSVM, sbMIL and stMIL\\nsuffer from the same flaw as mi-SVM.\\n1\\n\\n\\fFigure 1: Illustration of the False Positive Phenomenon: The top image is a positive training sample,\\nand the bottom image is a negative testing sample. The symbol ? and ? respectively denote positive\\nand negative instances. Enveloped points are instances in a positive bag. The Point not enveloped\\nis a negative bag of just one instance. Separating plane Fi corresponds to f (x) = i, and Gi corresponds to g(x) = i. The learners f and g are obtained with and without the projection constraint,\\nrespectively. Instances are labeled according to f . For details, please refer to the passage below.\\nThe neglect of nonwitnesses in positive bags may lead to false positive and cause a model to misclassify unseen negative samples. For example, in natural scene classification, each image is segmented\\nto a bag of instances beforehand, and each instance is a patch (ROI, Regions Of Interest) characterized by one feature vector describing its color. The task is to predict whether an image contains\\na waterfall or not (Figure 1). A positive image contains some positive instances corresponding to\\nwaterfall and some negative instances from other categories such as sky, stone, grass, etc., while\\na negative bag exclusively contains negative instances from other categories. Naturally, some negative instances (patches) only exist in positive bags. For instance, the end of a waterfall is often\\nsurrounded by mist. The aforementioned approaches tend to misclassify negative instances in positive bags. Therefore, the patch corresponding to mist is misclassified as positive. Given an unseen\\nimage with cirrus cloud and without waterfall, the obtained learner will misclassify this image as\\npositive because cirrus cloud and mist are similar to each other.\\nTo avoid both false negative and false positive, we attempt to classify instances inside positive bags\\nfar from the separating hyperplane and place positive and negative instances at opposite sides. We\\nachieve this by introducing projection constraints based on kernel principal component analysis into\\nMI-SVM [4]. Each constraint is defined on a positive bag to encourage large variance of its constituent instances along the normal direction of the separating hyperplane. We apply the Constrained\\nConcave-Convex Procedure (CCCP) to solve the resulted optimization problems.\\nThe remainder of the paper is organized as follows: Section 2 introduces notation convention and\\nthe CCCP. In Section 3 we bring out the projection constraint and the corresponding formulation\\nfor multi-instance learning. In Section 4, the algorithm is evaluated on real world data sets. Finally,\\nconclusions are drawn in Section 5.\\n\\n2\\n2.1\\n\\nPreliminaries\\nNotation Convention\\n\\nThe origin of multi-instance learning [1] has been presented in section 1. Let X ? Rp be the\\nth\\nspace containing instances and D = {(Bi , yi )}m\\nbag of\\ni=1 be the training data, where Bi is the i\\ninstances {xi1 , ? ? ? , xini } and yi ? Y is the label for Bi . Y is {+1, ?1} for classification and R\\nfor regression. In addition, denote the index set for instances xij of Bi by Ii . The task is to train\\n2\\n\\n\\fa learner to predict the label of an unseen bag. Compared with traditional supervised learning, the\\nlearner is a mapping from 2X to Y instead of from X to Y. Denote the index sets for positive and\\nnegative bags by I+ and I? respectively. Without loss of generality, assume that the instances are\\nordered in the sequence {x11 , ? ? ? , x1n1 , ? ? ? , xm1 , ? ? ? , xmnm }. We index instances by a function\\ni?1\\ni?1\\ni?1\\n?\\n?\\n?\\nI(xij ) =\\nnk + j. And I(Bi ) returns a vector (\\nnk + 1, ? ? ? ,\\nnk + ni ).\\nk=1\\n\\n2.2\\n\\nk=1\\n\\nk=1\\n\\nConstrained Concave-Convex Procedure\\n\\nNon-convex optimizations are undesirable because few algorithms effectively converge even to a\\nlocal optimum. However, if both objective function and constraints take the form of a difference between two convex functions, then a non-convex problem can be solved efficiently by the constrained\\nconcave-convex procedure [14]. The fundamental is to eliminate the non-convexity by changing\\nnon-convex parts to their first-order Taylor expansions. The original problem is as follows:\\nmin f0 (x) ? g0 (x)\\nx\\n\\ns.t. fi (x) ? gi (x) ? ci ,\\n\\ni = 1, ? ? ? , m\\n\\n(1)\\n\\nwhere fi , gi (i = 0, ? ? ? , m) are real-valued, convex and differentiable functions on Rn . Starting\\nfrom a random x(0) , (1) is approximated by a sequence of successive convex optimization problems.\\nAt the t + 1th iteration, the non-convex parts in the objective and constraints are substituted by their\\nfirst-order Taylor expansions, and the resulted optimization problem is as follows:\\n[\\n]\\nmin f0 (x) ? g0 (x(t) ) + ?g0 (x(t) )T (x ? x(t) )\\n(2)\\nx\\n[\\n]\\ns.t. fi (x) ? gi (x(t) ) + ?gi (x(t) )T (x ? x(t) ) ? ci\\nwhere x(t) is the optimal solution to (2) at the tth iteration. The above procedure is repeated until\\nconvergence. In [14] it is proved that the CCCP converges to a local optimum of (1).\\n\\n3\\n3.1\\n\\nMulti-Instance Classification\\nSupport Vector Machine Formulation\\n\\nOur work is based on the support vector machine (SVM) formulations for multi-instance learning,\\nto be exact, the MI-SVM [4] as follows:\\n[?\\n]\\n?\\n1\\nmin ?w?2 + C\\n?i +\\n?ij\\n(3)\\nw,b,? 2\\ni?I+\\n\\nj?Ii ,i?I?\\n\\ns.t. max(w xij + b) ? 1 ? ?i , ?i ? 0, i ? I+\\nT\\n\\nj?Ii\\n\\n? wT xij ? b ? 1 ? ?ij , ?ij ? 0, j ? Ii , i ? I?\\nCompared with the conventional SVM, in MI-SVM the notion of slack variables for positive samples\\nis extended from individual instances to bags while that for negative samples remains unchanged.\\nAs shown by the first set of max constraints, only the ?most positive? instance in a positive bag, or\\nthe witness, could affect the margin. And other instances, or nonwitnesses, become irrelevant for\\ndetermining the position of the separating plane once the witness is specified.\\nThe max constraint at first sight seems to well embody the characteristic of multi-instance learning.\\nIndeed, it helps to avoid the false negative, i,e., the misclassification of positive samples. However,\\nit may incur false positive due to the following two reasons. Firstly, the max constraint aims at\\ndiscovering the witness, and tends to skip nonwitnesses. Thus each positive bag is approximately\\noversimplified to a single pattern, i.e., the witness. Most information in positive bags is wasted.\\nSecondly, due to the characteristic of the max function and the greediness of optimization methods,\\nthe predictions of nonwitnesses are often adjusted above zero in the learning process. Besides, there\\nis no mechanism to draw the predictions of nonwitenesses below zero. Nevertheless, many nonwitnesses in positive bags are factually negative instances. For example, in natural scene classification,\\n3\\n\\n\\fmany image patches in a positive bag are from the irrelevant background; in document categorization, many posts in a positive bag are not from the target category. Hence, many nonwitnesses are\\nmislabeled as positive, and we obtain a falsely large margin.\\nAs shown in Figure 1, MI-SVM classifies half instances in the training sample as positive, and some\\nnegative instances are mislabeled. This false positive will impair the generalization performance.\\n3.2\\n\\nProjection Constraint\\n\\nThe above problem is not unique for MI-SVM. Any approach without properly utilizing nonwitnesses has the same problem. In our preliminary work before this paper, we tried three solutions.\\nFirstly, we treat the labels of all nonwitnesses as unknown integer variables to be optimized. In the\\nSVM framework, it is exactly the mi-SVM [4] as follows:\\n?\\n1\\n?ij\\n(4)\\nmin min ?w?2 + C\\nj?Ii , i?I+ ?I?\\n{yij } w,b,? 2\\ns.t. yij (wT xij + b) ? 1 ? ?ij , ?ij ? 0, j ? Ii , i ? I+\\n? yij + 1\\n? 1,\\ni ? I+\\n2\\nj?Ii\\n\\n? wT xij ? b ? 1 ? ?ij , ?ij ? 0, j ? Ii , i ? I?\\nIt seems that assigning labels over all nonwitnesses should lead to a reasonable model. Nevertheless,\\nnonwitnesses are usually labeled positive since the consequent margin will be larger. Thus, many\\nof nonwitnesses are misclassified. As far as the example in Figure 1 is concerned, the obtained\\nlearner is g(x) instead of f (x). MissSVM [7] takes an unsupervised approach. For every instance\\nin positive bags, two slack variables are introduced, measuring the distances from the instance to\\nthe positive boundary f (x) = +1 and the negative boundary f (x) = ?1 respectively, and the label\\nof the instance depends on the smaller slack variable. stMIL [8] takes a similar approach. As miSVM, MissSVM and stMIL also suffers from misclassification of nonwitnesses. sbMIL [8] tackles\\nmulti-instance learning in two steps. The first step is similar to MI-SVM, and the second step is a\\ntraditional SVM. Still, there is no mechanism in sbMIL to avoid false positive.\\nIn the second solution, we simultaneously seek for the ?most positive? instance and the ?most negative? instance in a positive bag by adding the following constraints to (3):\\n(?1) ? min(wT xij + b) ? ?1 ? ?i , ?i ? 0, i ? I+\\nj?Ii\\n\\n(5)\\n\\n?\\n?\\nAnd the term i?I+ ?i in the objective of (3) is changed to i?I+ (?i + ?i ). Although misclassification of nonwitnesses is alleviated since at least the ?most negative? nonwitness is classified\\ncorrectly, the information carried by most nonwitnesses are not fully utilized. As far as the example\\nin Figure 1 is concerned, the obtained learner is still g(x) instead of f (x). Besides, this solution is\\nnot appropriate for applications which involve positive bags only with positive instances.\\nThe third solution is the projection constraint proposed in this paper. In a maximum margin framework we want to classify instances in a positive bag far away from the separating hyperplane while\\nplace positive instances and negative instances at opposite sides. From another point of view, in the\\nfeature (kernel) space, we want to maximize the variance of instances in a positive bag along w, the\\nnormal vector of the separating hyperplane. Therefore, the principal component analysis (PCA) [15]\\nis just the technique that we need. To tackle complicated real world datasets, we directly develop our\\napproach in the Reproducing Kernel Hilbert Space (RKHS). Let X be the space of instances, and H\\nbe a RKHS of functions f : X ? R with associated kernel function k(?, ?). Note that f is both a\\nfunction on X and a vector in H. With an abuse of notation, we will not differentiate them unless\\nnecessary. Denote the RKHS norm of H by ?f ?H . Then MI-SVM can be rewritten as follows:\\n[?\\n]\\n?\\n1\\nmin ?f ?2 + C\\n?i +\\n?ij\\nf ?H,? 2\\ni?I+\\n\\nj?Ii ,i?I?\\n\\ns.t. max(f (xij )) ? 1 ? ?i , ?i ? 0, i ? I+\\nj?Ii\\n\\n? (f (xij )) ? 1 ? ?ij , ?ij ? 0, j ? Ii , i ? I?\\n4\\n\\n(6)\\n\\n\\f?\\n\\n?\\n\\n?\\n?\\n\\n?\\n??\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n\\n??\\n\\n?\\n\\n?\\n? ?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\nG+1\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\nF?1\\nF0\\nF+1\\n\\nG0\\nG?1\\n\\n?\\nFigure 2: Illustration of the Effect of the Projection Constraint: Please note that the projection\\nconstraint is effective for datasets with any geometric distribution once an appropriate kernel is\\nselected. Enveloped points are instances in a positive bag. Points not enveloped are negative bags of\\njust one instance. Separating plane Fi corresponds to f (x) = i, and Gi corresponds to g(x) = i.\\nThe learner f and g are obtained with and without the projection constraint, respectively. Instances\\nare labeled according to f . ? and ? denote positive instances and negative instance respectively.\\nAccording to the representer theorem [16], each minimizer f ? H of (6) has the following form:\\n? ?\\nf=\\n?ij ?(xij )\\n(7)\\ni?I+ ?I? j?Ii\\n\\nwhere all ?i ? R, and ?(?) induced by k(?, ?) is the feature mapping from X to H.\\nNext, we will propose our key contribution, i.e., the projection constraint. Given a positive bag\\ni\\nBi , denote its instances by {xij }nj=1\\n, and denote the normal vector of the separating plane in the\\nRKHS by f . According to the theory of PCA [15, 17], maximizing the variance of mapped instances\\ni\\n{?(xij )}nj=1\\nalong f equals to minimizing the sum of the Euclidean distances from the centralized\\ndata points to their projections on the normalized vector ?ff?2 , as follows:\\nJi (f ) =\\n\\nni\\n?\\nj=1\\n\\nwhere ?(mi ) =\\n\\n1\\nni\\n\\nni\\n?\\nj=1\\n\\n?cj\\n\\nf\\n? (?(xij ) ? ?(mi ))?22\\n?f ?2\\n\\n(8)\\n\\ni\\n?(xij ), the mean of {?(xij )}nj=1\\n. |cj | is the distance from ?(mi ) to the\\n\\nprojection point of ?(xij ). After simple algebra, we get:\\ncj =\\n\\nfT\\n(?(xij ) ? ?(mi ))\\n?f ?2\\n\\n(9)\\n\\nSubstituting (9) and (7) into (8), we arrive at:\\n?T L2i ?\\n(10)\\n?T K?\\nwhere K is a n ? n kernel matrix defined on all the instances of both positive bags and negative\\nbags, oi = trace(KI(Bi ) ) ? n1i 1T KI(Bi ) 1 where KI(Bi ) is a ni ? ni matrix formed by extracting\\nthe I(Bi ) columns (Please refer to section 2.1) and the I(Bi ) rows of the overall kernel matrix K,\\nand L2i is the ?centralized? L2i as follows:\\nJi (?) = oi ?\\n\\nL2i = LTi Li ? 1n LTi Li ? LTi Li 1n + 1n LTi Li 1n\\n5\\n\\n(11)\\n\\n\\fwhere 1n is a matrix with all elements equal to n1 , and Li is a n ? n matrix formed by keeping the\\nI(Bi ) rows of K and setting all the elements in other rows to 0:\\n{\\nK(p, q) if p ? I(Bi ), ?q ? {1, ? ? ? , n}\\nLi (p, q) =\\n0\\notherwise\\nGenerally, the optimal normal vector f varies for different positive bags. Hence it is meaningless to\\nsolve (10) for its optimum. Instead, we average (10) by the bag size ni , and use a common threshold\\n? to bound the averaged projection distance for different bags from above. We name the obtained\\ninequality ?the projection constraint?, as follows:\\n?T L2 ? )\\n1(\\noi ? T i\\n??\\nni\\n? K?\\n\\n(12)\\n\\nThis is equivalent to bounding variance of instances in positive bags along f from below [15].\\nSubstituting (7) into (6), and adding the projection constraint (12) for each positive bag to the resulted problem, we arrive at the following optimization problem:\\n[?\\n]\\n?\\n1\\nmin ?T K? + C\\n?i +\\n?ij\\n(13)\\n?,b,? 2\\ni?I+\\n\\ns.t. 1 ? ?i ?\\n\\nj?Ii ,i?I?\\n\\nmax(kTI(xij ) ?\\nj?Ii\\n\\n+ b) ? 0, ?i ? 0, i ? I+\\n\\nkTI(xij ) ? + b ? ?1 + ?ij , ?ij ? 0, j ? Ii , i ? I?\\n?T (oi ? K ? L2i )? ? ?ni ? ?T K? ? 0, i ? I+\\n3.3\\n\\nOptimization via the CCCP\\n\\nIn the problem (13), the objective function and the second set of constraints are convex. The first\\nset of constraints are all in the form of difference of two convex functions since the max function\\nis convex. According to the definition of Ji (f ) in (8), J(?) in (10) is not less than 0 for any ?.\\nThus for any i ? I+ , oi ? K ? L2i is semi-definite positive. Consequently, the third set of constraints\\nare all in the form of difference of two convex functions. Therefore, we can apply the Constrained\\nConcave-Convex Procedure (CCCP) introduced in section 2.2 to solve the problem (13).\\nSince the function max in the first set of constraints is nonsmooth, we have to change gradients to\\nsubgradients to use the CCCP. The subgradient is usually not unique, and we adopt the definition\\nused in [6] for the subgradient of max kTI(xij ) ?:\\nj?Ii\\n\\n?(max kTI(xij ) ?) =\\nj?Ii\\n\\n{\\n\\nwhere\\n?ij =\\n\\n?\\n\\n?ij kTI(xij )\\n\\n(14)\\n\\nj?Ii\\n\\nif kTI(xij ) ? ?= max kTI(xij ) ?\\n\\n0\\n\\nj?Ii\\n\\n1\\nna\\n\\notherwise\\n\\n(15)\\n\\nwhere na is the number of xij that maximize kTI(xij ) ?. At the tth iteration, denote the current\\n(t)\\n\\nestimate for ? and ?ij by ?(t) and ?ij respectively. Then the first order Taylor expansion of\\nmax kTI(xij ) ? is as follows:\\nj?Ii\\n\\nmax kTI(xij ) ?(t) +\\nj?Ii\\n\\nAccording to (15), we have\\n\\n?\\n\\n?\\n\\n(t)\\n\\n?ij kTI(xij ) (? ? ?(t) )\\n\\n(16)\\n\\nj?Ii\\n\\n(t)\\n\\n?ij kTI(xij ) ?(t) = max(kTI(xij ) ?(t) )\\nj?Ii\\n\\nj?Ii\\n\\n6\\n\\n(17)\\n\\n\\fUsing (17), (16) reduces to\\n\\n?\\n\\n(t)\\n\\n?ij kTI(xij ) ?\\n\\n(18)\\n\\nj?Ii\\n\\nReplacing max kTI(xij ) ? in the first set of constraints by (18) and ?T K? in the third set of conj?Ii\\n\\nstraints by their first order Taylor expansions, finally we get:\\n[?\\n]\\n?\\n1\\nmin ?T K? + C\\n?i +\\n?i\\n?,b,? 2\\ni?I+\\ni?Ii ,I?I?\\n? (t)\\nT\\ns.t. 1 ? ?i ? (\\n?ij kI(xij ) ? + b) ? 0, ?i ? 0, i ? I+\\n\\n(19)\\n\\nj?Ii\\n\\nkTI(xij ) ?\\n\\n+ b ? ?1 + ?ij , ?ij ? 0, j ? Ii , i ? I?\\nT\\n\\n?T Si ? ? 2?ni ? ?(t) K(? ? ?(t) ) ? 0, i ? I+\\nwhere Si = oi ? K ? L2i . The problem (19) is a quadratically constrained quadratic program (QCQP)\\nwith a convex objective function and convex constraints, and thus can be readily solved via interior\\npoint methods [18]. Following the CCCP, we can do the iteration until (19) converges.\\n\\n4\\n4.1\\n\\nExperiments\\nClassification: Benchmark\\n\\nBenchmark data sets comes from two areas. Musk 1 and Musk 2 data sets [1] are two biochemical\\ntasks which directly promoted the research of multi-instance learning. The aim is to predict activity of drugs from structural information. Each drug molecule is a bag of potential conformations\\n(instances). The Musk 1 data set consists of 47 positive bags, 45 negative bags, and totally 476\\ninstances. The Musk 2 data set consists of 39 positive bags, 63 negative bags, and totally 6598 instances. Each instance is represented by a 166 dimensional vector. Elephant, tiger and fox are three\\ndata sets from image categorization. The aim is to differentiate images with elephant, tiger, and fox\\n[4] from those without, respectively. A bag here is a group of ROIs (Region Of Interests) drawn\\nfrom a certain image. Each data set contains 100 positive bags and 100 negative bags, and each\\nROI as an instance is a 230 dimensional vector. Related methods for comparison includes Diverse\\nTable 1: Test Accuracy(%) On Benchmark:\\nrespectively.\\nAlgorithm Musk 1\\n90.6\\nPC-SVM\\n?2.7\\n90.0\\nMIGraph\\n?3.8\\n88.9\\nmiGraph\\n?3.3\\n88.0\\nMI-Kernel\\n?3.1\\nMI-SVM\\n77.9\\nstMIL\\n79.5\\nsbMIL\\n91.8\\nDD\\n88.0\\nEM-DD\\n84.8\\n\\nRows and columns correspond to methods and datasets\\nMusk 2\\n91.3\\n?3.2\\n90.0\\n?2.7\\n90.3\\n?2.6\\n89.3\\n?1.5\\n84.3\\n68.4\\n87.7\\n84.0\\n84.9\\n\\nElep\\n89.8\\n?1.2\\n85.1\\n?2.8\\n86.8\\n?0.7\\n84.3\\n?1.6\\n81.4\\n81.6\\n88.6\\nN/A\\n78.3\\n\\nFox\\n65.7\\n?1.4\\n61.2\\n?1.7\\n61.6\\n?2.8\\n60.3\\n?1.9\\n59.4\\n60.7\\n69.8\\nN/A\\n56.1\\n\\nTiger\\n83.8\\n?1.3\\n81.9\\n?1.5\\n86.0\\n?1.0\\n84.2\\n?1.0\\n84.0\\n74.7\\n83.0\\nN/A\\n72.1\\n\\nDensity (DD,[2]), EM-DD [19], MI-SVM [4], MI-Kernel [5], stMIL [8], sbMIL [8], MIGraph and\\nmiGraph [10]. When applied for multi-instance classification, our approach involves three parameters, namely, the bias/variance trade-off factor C, the kernel parameter (e.g.: ? in RBF kernel), and\\nthe bound parameter ? in the projection constraint. In the experiment, C, ?, and ? are selected from\\n7\\n\\n\\f{0.01,0.1,1,10,50,100}, {0.2,0.4,0.6,0.8,1.0} and {0.01,0.1,1,10,100} respectively. We employ the\\nMOSEK toolbox 1 to solve the resulted QCQP problem (19). The other experiment uses the same\\nparameter setting.\\nThe ten-times 10-fold cross validation results (except Diverse Density) are shown in Table 1. The\\nresults for other methods are replicated from their original papers. The results not available are\\nmarked by N/A. The bolded figure indicates that result is better than all other methods. Table 1\\nshows that the performance of our approach (PC-SVM) is competitive. Recall that the difference\\nbetween our approach and MI-SVM is just the projection constraint. Therefore, as discussed in\\nsection 3.2, the results in Table 1 demonstrates that the strength of nonwitnesses is well utilized via\\nthe projection constraint.\\n4.2\\n\\nClassification: COREL Image Data Sets\\n\\nTable 2: Test Accuracy(%) On COREL: Rows and columns correspond to methods and datasets\\nrespectively.\\nAlgorithm 1000-Image\\n2000-Image\\nPC-SVM\\n\\n85.6 : [84.3, 86.9]\\n\\n75.8 : [74.4, 77.2]\\n\\nreg-SVM\\nMIGraph\\nmiGraph\\nMI-Kernel\\nMI-SVM\\nDD-SVM\\n\\n84.4 : [83.0, 85.8]\\n83.9 : [81.2, 85.7]\\n82.4 : [80.2, 82.6]\\n81.8 : [80.1, 83.6]\\n74.7 : [74.1, 75.3]\\n81.5 : [78.5, 84.5]\\n\\nN/A\\n72.1 : [71.0, 73.2]\\n70.5 : [68.7, 72.3]\\n72.0 : [71.2, 72.8]\\n54.6 : [53.1, 56.1]\\n67.5 : [66.1, 68.9]\\n\\nCOREL is a collection of natural scene images which have been categorized according to the presence of certain objects. Each image is regarded as a bag, and the nine dimensional ROIs (Region Of\\nInterests) in it are regarded as its constituent instances. In experiments, we use the 1000-Image data\\nset and the 2000-Image data set which contain ten and twenty categorizes, respectively. Following\\nthe methodology in [10], on both of the two data sets the related methods are compared by their five\\ntimes 2-fold cross validation results. The algorithm for comparison include Diverse Density (DD),\\nMI-SVM, MIGraph, miGraph , MI-Kernel and reg-SVM. In the last four algorithms one-against-all\\nstrategy is employed to tackle this multi-class task. In our approach this strategy is also used. Table\\n2 shows the overall accuracy as well as the 95% interval. As in benchmark data sets, our approach is\\ncompetitive with the latest methods. The results again suggest that fully utilizing the nonwitnesses\\nis important for multi-instance classification.\\n\\n5\\n\\nConclusion\\n\\nWe design a projection constraint to fully exploit nonwitnesses to avoid false positive. Since our\\napproach is basically MI-SVM with projection constraints, the improved results on real world data\\nsets validate the strength of nonwitnesses. We will introduce the universal projection constraint\\ninto other existing approaches for multi-instance learning, and related learning tasks, such as multiinstance regression, multi-label multi-instance learning, generalized multi-instance learning, etc.\\nAcknowledgments\\nWe gratefully acknowledge reviewers for their insightful remarks and editors for their assiduous\\nwork. We also deeply appreciate Kuijun Ma?s careful proof-reading. Finally, we are extremely\\nthankful to Runing Liu for the fascinating illustrations. This work was partially supported by National Basic Research Program of China under Grant No.2004CB318103 and National Natural Science Foundation of China under award No.60835002 and 60975040.\\n1\\n\\nhttp://www.mosek.com/\\n\\n8\\n\\n\\fReferences\\n\\n[1] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P?erez. Solving the multiple-instance problem with axisparallel rectangles. Artificial Intelligence, 89(1-2):31?71, 1997.\\n[2] O. Maron and T. Lozano-P?erez. A framework for multiple-instance learning. Advances in neural information processing systems, pages 570?576, 1998.\\n[3] J. Wang and J.D. Zucker. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 1119?1126. Citeseer,\\n2000.\\n[4] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning.\\nAdvances in neural information processing systems, pages 577?584, 2003.\\n[5] T. G?artner, P.A. Flach, A. Kowalczyk, and A.J. Smola. Multi-instance kernels. In Proceedings of the\\nNineteenth International Conference on Machine Learning, pages 179?186. Citeseer, 2002.\\n[6] P.M. Cheung and J.T. Kwok. A regularization framework for multiple-instance learning. In Proceedings\\nof the 23rd international conference on Machine learning, page 200. ACM, 2006.\\n[7] Z.H. Zhou and J.M. Xu. On the relation between multi-instance learning and semi-supervised learning.\\nIn Proceedings of the 24th international conference on Machine learning, page 1174. ACM, 2007.\\n[8] R.C. Bunescu and R.J. Mooney. Multiple instance learning for sparse positive bags. In Proceedings of\\nthe 24th international conference on Machine learning, page 112. ACM, 2007.\\n[9] H.Y. Wang, Q. Yang, and H. Zha. Adaptive p-posterior mixture-model kernels for multiple instance\\nlearning. In Proceedings of the 25th international conference on Machine learning, pages 1136?1143.\\nACM, 2008.\\n[10] Z. H. Zhou, Y. Y. Sun, and Yu. F. Li. Multi-instance learning by treating instances as non-I.I.D. samples. In\\nL?eon Bottou and Michael Littman, editors, Proceedings of the 26th International Conference on Machine\\nLearning, pages 1249?1256, Montreal, June 2009. test, Omnipress.\\n[11] Y. Chen and J.Z. Wang. Image categorization by learning and reasoning with regions. The Journal of\\nMachine Learning Research, 5:913?939, 2004.\\n[12] B. Settles, M. Craven, and S. Ray. Multiple-instance active learning. Advances in Neural Information\\nProcessing Systems (NIPS), 20:1289?1296, 2008.\\n[13] G. Fung, M. Dundar, B. Krishnapuram, and R.B. Rao. Multiple instance learning for computer aided\\ndiagnosis. In NIPS2007, page 425. The MIT Press, 2007.\\n[14] A.J. Smola, SVN Vishwanathan, and T. Hofmann. Kernel methods for missing variables. In Proceedings\\nof the Tenth International Workshop on Artificial Intelligence and Statistics. Citeseer, 2005.\\n[15] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. John Wiley & Sons, 2001.\\n[16] B. Sch?olkopf and A.J. Smola. Learning with kernels. Citeseer, 2002.\\n[17] Q. Tao, D.J. Chu, and J. Wang. Recursive support vector machines for dimensionality reduction. IEEE\\nTransactions on Neural Networks, 19(1):189?193, 2008.\\n[18] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Pr, 2004.\\n[19] Q. Zhang and S.A. Goldman. Em-dd: An improved multiple-instance learning technique. Advances in\\nneural information processing systems, 2:1073?1080, 2002.\\n\\n9\\n\\n\\f\",\n          \"Temporal Difference Learning in\\nContinuous Time and Space\\n\\nKenji Doya\\ndoya~hip.atr.co.jp\\n\\nATR Human Information Processing Research Laboratories\\n2-2 Hikaridai, Seika.-cho, Soraku-gun, Kyoto 619-02, Japan\\n\\nAbstract\\nA continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application\\nof reinforcement learning to real-world control tasks and neurobiological modeling. An optimal nonlinear feedback control law was\\nalso derived using the derivatives of the value function. The performance of the algorithms was tested in a task of swinging up a\\npendulum with limited torque. Both the \\\"critic\\\" that specifies the\\npaths to the upright position and the \\\"actor\\\" that works as a nonlinear feedback controller were successfully implemented by radial\\nbasis function (RBF) networks.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe temporal-difference (TD) algorithm (Sutton, 1988) for delayed reinforcement\\nlearning has been applied to a variety of tasks, such as robot navigation, board\\ngames, and biological modeling (Houk et al., 1994). Elucidation of the relationship\\nbetween TD learning and dynamic programming (DP) has provided good theoretical\\ninsights (Barto et al., 1995). However, conventional TD algorithms were based on\\ndiscrete-time, discrete-state formulations. In applying these algorithms to control\\nproblems, time, space and action had to be appropriately discretized using a priori\\nknowledge or by trial and error. Furthermore, when a TD algorithm is used for\\nneurobiological modeling, discrete-time operation is often very unnatural.\\nThere have been several attempts to extend TD-like algorithms to continuous cases.\\nBradtke et al. (1994) showed convergence results for DP-based algorithms for a\\ndiscrete-time, continuous-state linear system with a quadratic cost. Bradtke and\\nDuff (1995) derived TD-like algorithms for continuous-time, discrete-state systems\\n(semi-Markov decision problems). Baird (1993) proposed the \\\"advantage updating\\\"\\nalgorithm by modifying Q-Iearning so that it works with arbitrary small time steps .\\n\\n\\fK.DOYA\\n\\n1074\\n\\nIn this paper, we derive a TD learning algorithm for continuous-time, continuousstate, nonlinear control problems. The correspondence of the continuous-time version to the conventional discrete-time version is also shown. The performance of\\nthe algorithm was tested in a nonlinear control task of swinging up a pendulum\\nwith limited torque.\\n\\n2\\n\\nCONTINUOUS-TIME TD LEARNING\\n\\nWe consider a continuous-time dynamical system (plant)\\nx(t) = f(x(t), u(t))\\n\\n(1)\\n\\nwhere x E X eRn is the state and u E U C Rm is the control input (action). We\\ndenote the immediate reinforcement (evaluation) for the state and the action as\\nr(t) = r(x(t), u(t)).\\n\\n(2)\\n\\nOur goal is to find a feedback control law (policy)\\nu(t)\\n\\n= JL(x(t))\\n\\n(3)\\n\\nthat maximizes the expected reinforcement for a certain period in the future. To\\nbe specific, for a given control law JL, we define the \\\"value\\\" of the state x(t) as\\nV!L(x(t)) =\\n\\n1\\n\\n00\\n\\n1\\n\\n,-t\\n\\n-e-- r(x(s), u(s))ds,\\n(4)\\nt\\nr\\nwhere x(s) and u(s) (t < s < 00) follow the system dynamics (1) and the control\\nlaw (3). Our problem now is to find an optimal control law JL* that maximizes\\nV!L(x) for any state x E X. Note that r is the time scale of \\\"imminence-weighting\\\"\\nand the scaling factor ~ is used for normalization, i.e., ft OO ~e- ':;:t ds = 1.\\n\\n2.1\\n\\nT\\n\\nTD ERROR\\n\\nThe basic idea in TD learning is to predict future reinforcement in an on-line manner. We first derive a local consistency condition for the value function V!L(x). By\\ndifferentiating (4) by t, we have\\nd\\nr dt V!L(x(t)) = V!L(x(t)) - r(t).\\n\\n(5)\\n\\nLet P(t) be the prediction of the value function V!L(x(t)) from x(t) (output of the\\n\\\"critic\\\"). If the prediction is perfect, it should satisfy rP(t) = P(t) - r(t). If this\\nis not satisfied, the prediction should be adjusted to decrease the inconsistency\\nf(t) = r(t) - P(t)\\n\\n+ rP(t).\\n\\n(6)\\n\\nThis is a continuous version of the temporal difference error.\\n\\n2.2\\n\\nEULER DIFFERENTIATION: TD(O)\\n\\nThe relationship between the above continuous-time TD error and the discrete-time\\nTD error (Sutton, 1988)\\nf(t)\\n\\n= r(t) + ,,(P(t) -\\n\\nP(t - ~t)\\n\\n(7)\\n\\ncan be easily seen by a backward Euler approximation of p(t). By substituting\\np(t) = (P(t) - P(t - ~t))/~t into (6), we have\\nf=r(t)+\\n\\n~t\\n\\n[(1- ~t)P(t)-P(t-~t)] .\\n\\n\\f1075\\n\\nTemporal Difference in Learning in Continuous Time and Space\\n\\nThis coincides with (7) if we make the \\\"discount factor\\\" '\\\"Y = 1- ~t ~ e-'?, except\\nfor the scaling factor It '\\nNow let us consider a case when the prediction of the value function is given by\\n(8)\\n\\nwhere bi O are basis functions (e.g., sigmoid, Gaussian, etc) and Vi are the weights.\\nThe gradient descent of the squared TD error is given by\\n\\n~Vi\\n\\nex: _\\n\\no~r2(t)\\n\\nex: - r et)\\n\\n[(1 _~t) oP(t) _ oP(t - ~t)] .\\n\\nOVi\\nT\\nOVi\\nOVi\\nIn order to \\\"back-up\\\" the information about the future reinforcement to correct the\\nprediction in the past, we should modify pet - ~t) rather than pet) in the above\\nformula. This results in the learning rule\\n\\n~Vi ex: ret) OP(~~ ~t)\\n\\n= r(t)bi(x(t -\\n\\n~t)) .\\n\\n(9)\\n\\nThis is equivalent to the TD(O) algorithm that uses the \\\"eligibility trace\\\" from the\\nprevious time step.\\n\\n2.3\\n\\nSMOOTH DIFFERENTIATION: TD(-\\\\)\\n\\nThe Euler approximation of a time derivative is susceptible to noise (e.g., when\\nwe use stochastic control for exploration) . Alternatively, we can use a \\\"smooth\\\"\\ndifferentiation algorithm that uses a weighted average of the past input, such as\\n\\npet)\\n\\n~\\n\\npet) - Pet)\\n\\nwhere\\n\\n~\\n\\nTc\\n\\ndd pet) = pet) - pet)\\n\\nt\\n\\nand Tc is the time constant of the differentiation. The corresponding gradient descent algorithm is\\n\\n~Vi ex: _ O~;2(t)\\n\\nex: ret) o~(t) = r(t)bi(t) ,\\n\\nVi\\n\\n(10)\\n\\nUVi\\n\\nwhere bi is the eligibility trace for the weight\\ndTc dtbi(t) = bi(x(t)) - bi(t) .\\n\\n(11)\\n\\nNote that this is equivalent to the TD(-\\\\) algorithm (Sutton, 1988) with -\\\\ = 1if we discretize the above equation with time step ~t.\\n\\n3\\n\\nAt\\nTc\\n\\nOPTIMAL CONTROL BY VALUE GRADIENT\\n\\n3.1\\n\\nHJB EQUATION\\n\\nThe value function V * for an optimal control J..L* is defined as\\n\\nV*(x(t)) = max\\n\\nU[t,oo)\\n\\n[1\\nt\\n\\n00\\n\\n1 . -t r(x(s), u(s))ds ] .\\n-e--\\n\\n(12)\\n\\nT\\n\\nT\\n\\nAccording to the principle of dynamic programming (Bryson and Ho, 1975), we\\nconsider optimization in two phases, [t, t + ~t] and [t + ~t , 00), resulting in the\\nexpression\\n\\nV * (x(t)) =\\n.\\n\\nmax\\nU[t,HAt)\\n\\n[I +\\n\\nt At\\n\\nt\\n\\n1 ? :;:- t r(x(s), u(s))ds + e--'?V* (x(t\\n_eT\\n\\n1.\\n\\n+ ~t))\\n\\n\\fK.DOYA\\n\\n1076\\n\\nBy Taylor expanding the value at t\\nV*(x(t\\n\\n+ f:l.t as\\nav*\\n\\n+ f:l.t)) = V*(x(t)) + ax(t) f(x(t), u(t))f:l.t + O(f:l.t)\\n\\nand then taking f:l.t to zero, we have a differential constraint for the optimal value\\nfunction\\nav*\\n]\\nV*(t) = max [r(x(t), u(t)) + T\\n- f(x(t), u(t)) .\\n(13)\\nax\\nU(t)EU\\nThis is a variant of the Hamilton-Jacobi-Bellman equation (Bryson and Ho, 1975)\\nfor a discounted case.\\n\\n3.2\\n\\nOPTIMAL NONLINEAR FEEDBACK CONTROL\\n\\nWhen the reinforcement r(x, u) is convex with respect to the control u, and the\\nvector field f(x, u) is linear with respect to u, the optimization problem in (13) has\\na unique solution. The condition for the optimal control is\\nar(x, u)\\n\\nau\\n\\nav* af(x, u) _ 0\\n\\nau\\n\\n+T ax\\n\\n(14)\\n\\n-.\\n\\nNow we consider the case when the cost for control is given by a convex potential\\nfunction GjO for each control input\\n\\n2:=\\n\\nf(x, u) = rx(x) -\\n\\nGj(Uj),\\n\\nj\\n\\nwhere reinforcement for the state r x (x) is still unknown. We also assume that the\\ninput gain of the system\\nb -(x) = af(x, u)\\nJ\\nau-J\\nis available. In this case, the optimal condition (14) for\\n-Gj(Uj)\\n\\n+T\\n\\nUj\\n\\nis given by\\n\\nav*\\nax bj(x) = O.\\n\\nNoting that the derivative G'O is a monotonic function since GO is convex, we have\\nthe optimal feedback control law\\nUj\\n\\n= (G')-1 ( T av*\\nax b(x) ) .\\n\\nParticularly, when the amplitude of control is bounded as\\nenforce this constraint using a control cost\\n\\n(15)\\nIUj\\n\\nI < uj&X,\\n\\nwe can\\n\\n~\\n\\nGj(Uj)\\n\\n=\\n\\nCj\\n\\nIoUi\\n\\ng-l(s)ds,\\n\\n(16)\\n\\nwhere g-10 is an inverse sigmoid function that diverges at ?1 (Hopfield, 1984). In\\nthis case, the optimal feedback control law is given by\\nmax\\n\\nUj\\n\\n= ujax g ( u ~j\\n\\nT\\n\\nav*\\n)\\nax bj(x) .\\n\\n(17)\\n\\nIn the limit of Cj -70, this results in the \\\"bang-bang\\\" control law\\nUj\\n\\n=\\n\\nUjmax'\\nSIgn\\n\\n[av*\\nax b j (x )] .\\n\\n(18)\\n\\n\\f1077\\n\\nTemporal Difference in Learning in Continuous Time and Space\\n\\nFigure 1: A pendulum with limited torque. The dynamics is given by m18\\n-f-tiJ + mglsinO + T. Parameters were m = I = 1, 9 = 9.8, and f-t = 0.0l.\\n\\nth\\n\\ntrials\\n\\n(b)\\n\\n(a)\\n20\\n\\n~\\\\~iii\\n\\n17 .5\\n15\\n12.5\\n0.\\n\\n.-\\\"\\n\\n10\\n7 .5\\n\\nI, I\\ni~!\\n\\n' :1\\n\\ntrials\\n\\n(c)\\n\\nth\\n\\n(d)\\n\\nFigure 2: Left: The learning curves for (a) optimal control and (c) actor-critic.\\nLup: time during which 101 < 90?. Right: (b) The predicted value function P after\\n100 trials of optimal control. (d) The output of the controller after 100 trials with\\nactor-critic learning. The thick gray line shows the trajectory of the pendulum. th:\\n(degrees), om: iJ (degrees/sec).\\n\\no\\n\\n\\f1078\\n\\n4\\n\\nK.DOYA\\n\\nACTOR-CRITIC\\n\\nWhen the information about the control cost, the input gain of the system, or the\\ngradient of the value function is not available, we cannot use the above optimal\\ncontrol law. However, the TD error (6) can be used as \\\"internal reinforcement\\\" for\\ntraining a stochastic controller, or an \\\"actor\\\" (Barto et al., 1983).\\nIn the simulation below, we combined our TD algorithm for the critic with a reinforcement learning algorithm for real-valued output (Gullapalli, 1990). The output\\nof the controller was given by\\n\\nu;(t) = ujU g\\n\\n(~W;,b'(X(t)) + <1n;(t)) ,\\n\\n(19)\\n\\nwhere nj(t) is normalized Gaussian noise and Wji is a weight. The size of this perturbation was changed based on the predicted performance by (Y = (Yo exp( -P(t)).\\nThe connection weights were changed by\\n!:l.Wji\\n\\n5\\n\\nex f(t)nj(t)bi(x(t)).\\n\\n(20)\\n\\nSIMULATION\\n\\nThe performance of the above continuous-time TD algorithm was tested on a task\\nof swinging up a pendulum with limited torque (Figure 1). Control of this onedegree-of-freedom system is trivial near the upright equilibrium. However, bringing\\nthe pendulum near the upright position is not if we set the maximal torque Tmax\\nsmaller than mgl. The controller has to swing the pendulum several times to\\nbuild up enough momentum to bring it upright. Furthermore, the controller has to\\ndecelerate the pendulum early enough to avoid falling over.\\nWe used a radial basis function (RBF) network to approximate the value function\\nfor the state of the pendulum x = (8,8). We prepared a fixed set of 12 x 12 Gaussian\\nbasis functions. This is a natural extension of the \\\"boxes\\\" approach previously used\\nto control inverted pendulums (Barto et al., 1983). The immediate reinforcement\\nwas given by the height of the tip of the pendulum, i.e., rx = cos 8.\\n\\n5.1\\n\\nOPTIMAL CONTROL\\n\\nFirst, we used the optimal control law (17) with the predicted value function P\\ninstead of V?. We added noise to the control command to enhance exploration.\\nThe torque was given by\\n\\nTmax aP(x)\\n)\\nT = Tmaxg ( - - r - - b + (Yn(t) ,\\nc\\nax\\nwhere g(x) = ~ tan- 1 ( ~x) (Hopfield, 1984). Note that the input gain b =\\n(0, 1/mI2)T was constant. Parameters were rm ax = 5, c = 0.1, (Yo = 0.01, r = 1.0,\\nand rc = 0.1.\\nEach run was started from a random 8 and was continued for 20 seconds. Within\\nten trials, the value function P became accurate enough to be able to swing up and\\nhold the pendulum (Figure 2a). An example of the predicted value function P after\\n100 trials is shown in Figure 2b. The paths toward the upright position, which were\\nimplicitly determined by the dynamical properties of the system, can be seen as the\\nridges of the value function. We also had successful results when the reinforcement\\nwas given only near the goal: rx = 1 if 181 < 30?, -1 otherwise.\\n\\n\\fTemporal Difference in Learning in Continuous Time and Space\\n\\n5.2\\n\\n1079\\n\\nACTOR-CRITIC\\n\\nNext, we tested the actor-critic learning scheme as described above. The controller\\nwas also implemented by a RBF network with the same 12 x 12 basis functions as\\nthe critic network. It took about one hundred trials to achieve reliable performance\\n(Figure 2c). Figure 2d shows an example of the output of the controller after 100\\ntrials. We can see nearly linear feedback in the neighborhood of the upright position\\nand a non-linear torque field away from the equilibrium.\\n\\n6\\n\\nCONCLUSION\\n\\nWe derived a continuous-time, continuous-state version of the TD algorithm and\\nshowed its applicability to a nonlinear control task. One advantage of continuous\\nformulation is that we can derive an explicit form of optimal control law as in (17)\\nusing derivative information, whereas a one-ply search for the best action is usually\\nrequired in discrete formulations.\\n\\nReferences\\nBaird III, L. C. (1993). Advantage updating. Technical Report WL-TR-93-1146,\\nWright Laboratory, Wright-Patterson Air Force Base, OH 45433-7301, USA.\\nBarto, A. G. , Bradtke, S. J., and Singh, S. P. (1995). Learning to act using real-time\\ndynamic programming. Artificial Intelligence, 72:81-138.\\nBarto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike adaptive\\nelements that can solve difficult learning control problems. IEEE Transactions\\non System, Man, and Cybernetics, SMC-13:834-846.\\nBradtke, S. J. and Duff, M. O. (1995). Reinforcement learning methods for\\ncontinuous-time Markov decision problems. In Tesauro, G., Touretzky, D. S.,\\nand Leen, T. K., editors, Advances in Neural Information Processing Systems\\n7, pages 393-400. MIT Press, Cambridge, MA.\\nBradtke, S. J ., Ydstie, B. E., and Barto, A. G. (1994). Adaptive linear quadratic\\ncontrol using policy iteration. CMPSCI Technical Report 94-49, University of\\nMassachusetts, Amherst, MA.\\nBryson, Jr., A. E .. and Ho, Y.-C. (1975). Applied Optimal Control. Hemisphere\\nPublishing, New York, 2nd edition.\\nGuUapalli, V. (1990) . A stochastic reinforcement learning algorithm for learning\\nreal-valued functions. Neural Networks, 3:671-192.\\nHopfield, J. J. (1984). Neurons with graded response have collective computational\\nproperties like those of two-state neurons. Proceedings of National Academy of\\nScience, 81 :3088-3092.\\nHouk, J . C., Adams, J. L., and Barto, A. G. (1994). A model of how the basal\\nganglia generate and use neural signlas that predict renforcement. In Houk,\\nJ. C., Davis, J. L., and Beiser, D. G. , editors, Models of Information Processing\\nin the Basal Ganglia, pages 249--270. MIT Press, Cambrigde, MA.\\nSutton, R. S. (1988). Learning to predict by the methods of temporal difference.\\nMachine Learning, 3:9--44.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a4EfV3J3Apb"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deHcVP-e3Apb",
        "outputId": "82f3148b-2c7b-4407-dc0b-ef5085da30bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3340    variable margin losses for classifier design\\n...\n",
              "3144    hierarchical learning of dimensional biases in...\n",
              "3698    sparse recovery with brownian sensing\\n\\nalexa...\n",
              "569     436\\n\\nsimulation and measurement of\\nthe elec...\n",
              "358     compositionality mdl priors and\\nobject recogn...\n",
              "Name: paper_text_processed, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVfiVGTp3Apb"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "3p_SGZ6M3Apb",
        "outputId": "150a01ed-f5c0-4434-d50e-b453f8a6c371"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ0nR_Yt3Apb"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE2YGg7h3Apb",
        "outputId": "22edd44b-b003-4ff9-83f1-8c9fd7dc6d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['variable', 'margin', 'losses', 'classifier', 'design', 'nuno', 'vasconcelos', 'statistical', 'visual', 'computing', 'laboratory', 'university', 'california', 'san', 'diego', 'la', 'jolla', 'ca', 'nuno', 'ucsdedu', 'hamed', 'masnadi', 'shirazi', 'statistical', 'visual', 'computing', 'laboratory', 'university', 'california', 'san']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtT6WhyO3Apb",
        "outputId": "4f202ef3-07ff-4f8b-ceb5-24b6917cc142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 8), (6, 2), (7, 1), (8, 1), (9, 2), (10, 1), (11, 3), (12, 12), (13, 1), (14, 2), (15, 3), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 2), (22, 4), (23, 1), (24, 3), (25, 1), (26, 1), (27, 1), (28, 2), (29, 3)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oncxcyla3Apc"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdScMf-_3Apc",
        "outputId": "d5f322e7-fffa-4e83-8fac-a58245db21a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.005*\"model\" + 0.005*\"learning\" + 0.005*\"set\" + 0.004*\"function\" + '\n",
            "  '0.004*\"two\" + 0.004*\"data\" + 0.004*\"using\" + 0.004*\"algorithm\" + '\n",
            "  '0.003*\"time\" + 0.003*\"figure\"'),\n",
            " (1,\n",
            "  '0.008*\"model\" + 0.006*\"learning\" + 0.005*\"data\" + 0.004*\"function\" + '\n",
            "  '0.004*\"one\" + 0.003*\"two\" + 0.003*\"algorithm\" + 0.003*\"using\" + '\n",
            "  '0.003*\"network\" + 0.003*\"set\"'),\n",
            " (2,\n",
            "  '0.006*\"data\" + 0.005*\"model\" + 0.005*\"one\" + 0.005*\"set\" + '\n",
            "  '0.005*\"algorithm\" + 0.004*\"using\" + 0.004*\"learning\" + 0.004*\"time\" + '\n",
            "  '0.003*\"number\" + 0.003*\"based\"'),\n",
            " (3,\n",
            "  '0.007*\"model\" + 0.005*\"data\" + 0.005*\"learning\" + 0.004*\"function\" + '\n",
            "  '0.004*\"matrix\" + 0.004*\"using\" + 0.004*\"one\" + 0.004*\"set\" + 0.003*\"two\" + '\n",
            "  '0.003*\"algorithm\"'),\n",
            " (4,\n",
            "  '0.006*\"learning\" + 0.006*\"algorithm\" + 0.005*\"function\" + 0.004*\"model\" + '\n",
            "  '0.004*\"using\" + 0.004*\"data\" + 0.004*\"two\" + 0.004*\"set\" + 0.004*\"one\" + '\n",
            "  '0.003*\"results\"'),\n",
            " (5,\n",
            "  '0.006*\"model\" + 0.005*\"algorithm\" + 0.005*\"set\" + 0.004*\"one\" + '\n",
            "  '0.004*\"data\" + 0.004*\"using\" + 0.004*\"learning\" + 0.004*\"function\" + '\n",
            "  '0.003*\"time\" + 0.003*\"models\"'),\n",
            " (6,\n",
            "  '0.005*\"algorithm\" + 0.005*\"model\" + 0.005*\"learning\" + 0.004*\"function\" + '\n",
            "  '0.004*\"one\" + 0.004*\"matrix\" + 0.004*\"set\" + 0.004*\"data\" + 0.003*\"problem\" '\n",
            "  '+ 0.003*\"two\"'),\n",
            " (7,\n",
            "  '0.007*\"model\" + 0.005*\"set\" + 0.004*\"data\" + 0.004*\"learning\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"algorithm\" + 0.003*\"network\" + 0.003*\"test\" + '\n",
            "  '0.003*\"one\" + 0.003*\"two\"'),\n",
            " (8,\n",
            "  '0.006*\"learning\" + 0.005*\"one\" + 0.004*\"figure\" + 0.004*\"two\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"function\" + 0.004*\"model\" + 0.004*\"data\" + '\n",
            "  '0.003*\"time\" + 0.003*\"set\"'),\n",
            " (9,\n",
            "  '0.008*\"model\" + 0.007*\"data\" + 0.005*\"learning\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"set\" + 0.004*\"one\" + 0.003*\"using\" + 0.003*\"two\" + '\n",
            "  '0.003*\"models\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhaMAi0b3Apc"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyldavis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RH1FF8l5GIt",
        "outputId": "ebaa8012-7818-4cf3-86ce-be107367a329"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.10.0)\n",
            "Collecting funcy (from pyldavis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyldavis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyldavis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyldavis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyldavis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-2.0 pyldavis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "cp75fUp93Apc",
        "outputId": "f0e49ac5-5335-4546-9d52-9a89e7f992f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "2      0.006775  0.006591       1        1  16.440955\n",
              "9      0.004899  0.000676       2        1  13.524971\n",
              "0      0.003504 -0.001007       3        1  13.252758\n",
              "6     -0.007327  0.001880       4        1  11.270625\n",
              "1      0.005446 -0.004120       5        1   8.987660\n",
              "7     -0.005287  0.005026       6        1   8.654654\n",
              "5     -0.001302 -0.001403       7        1   8.294188\n",
              "8     -0.003192 -0.008516       8        1   7.591633\n",
              "4     -0.004436  0.002142       9        1   6.899136\n",
              "3      0.000920 -0.001270      10        1   5.083419, topic_info=           Term         Freq        Total Category  logprob  loglift\n",
              "1054      model  1360.000000  1360.000000  Default  30.0000  30.0000\n",
              "331    learning  1128.000000  1128.000000  Default  29.0000  29.0000\n",
              "259    function   824.000000   824.000000  Default  28.0000  28.0000\n",
              "1028     matrix   628.000000   628.000000  Default  27.0000  27.0000\n",
              "11    algorithm   960.000000   960.000000  Default  26.0000  26.0000\n",
              "...         ...          ...          ...      ...      ...      ...\n",
              "352         log    22.431730   433.580779  Topic10  -6.2356   0.0176\n",
              "449     problem    24.394491   593.854007  Topic10  -6.1517  -0.2131\n",
              "15         also    23.612467   554.897970  Topic10  -6.1843  -0.1778\n",
              "1272       time    24.437933   693.232397  Topic10  -6.1499  -0.3660\n",
              "613        used    22.629664   521.824895  Topic10  -6.2268  -0.1589\n",
              "\n",
              "[851 rows x 6 columns], token_table=       Topic      Freq      Term\n",
              "term                            \n",
              "2287       1  0.286868    accept\n",
              "5953       1  0.230847  accepted\n",
              "5953       5  0.230847  accepted\n",
              "5953       8  0.230847  accepted\n",
              "3127       1  0.117048    action\n",
              "...      ...       ...       ...\n",
              "15243      4  0.107638        zy\n",
              "15243      6  0.215276        zy\n",
              "15243      7  0.107638        zy\n",
              "15243      8  0.107638        zy\n",
              "15243      9  0.107638        zy\n",
              "\n",
              "[3921 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 10, 1, 7, 2, 8, 6, 9, 5, 4])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el148135162240738336100392217\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el148135162240738336100392217_data = {\"mdsDat\": {\"x\": [0.00677499324172152, 0.004899121459409001, 0.003503754786595572, -0.007327474978702468, 0.0054461295554497895, -0.005286827504806727, -0.0013017140467601375, -0.003192291043194595, -0.004435908734775153, 0.000920217265063209], \"y\": [0.0065914191557459026, 0.0006764508120339091, -0.0010069363706970318, 0.0018800646794970499, -0.0041199774210059405, 0.005025516494541532, -0.0014034186153777906, -0.008515646913196754, 0.0021421954702029546, -0.0012696672917438315], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.44095511626092, 13.524971133282113, 13.25275761777331, 11.270625499923732, 8.987659800284518, 8.654654102317613, 8.294188077127622, 7.591632996207304, 6.899136281177159, 5.083419375645707]}, \"tinfo\": {\"Term\": [\"model\", \"learning\", \"function\", \"matrix\", \"algorithm\", \"means\", \"data\", \"error\", \"distribution\", \"test\", \"one\", \"two\", \"using\", \"figure\", \"network\", \"set\", \"functions\", \"bound\", \"neural\", \"max\", \"large\", \"mean\", \"algorithms\", \"results\", \"value\", \"method\", \"dual\", \"networks\", \"also\", \"see\", \"cpm\", \"workspace\", \"manipulator\", \"xs\", \"kinematics\", \"ejk\", \"kreutz\", \"mtm\", \"distances\", \"speeds\", \"spectrometry\", \"dkv\", \"kinematic\", \"bijk\", \"mine\", \"ijk\", \"decoding\", \"jacobian\", \"replicates\", \"ekft\", \"warping\", \"demers\", \"emg\", \"viterbi\", \"djk\", \"degraded\", \"mn\", \"fxy\", \"bethe\", \"loglog\", \"nearness\", \"trajectory\", \"em\", \"hs\", \"xi\", \"based\", \"clustering\", \"metric\", \"target\", \"test\", \"series\", \"data\", \"dynamics\", \"center\", \"priors\", \"interpreted\", \"one\", \"using\", \"time\", \"number\", \"approach\", \"sample\", \"problem\", \"set\", \"linear\", \"used\", \"let\", \"algorithm\", \"first\", \"image\", \"work\", \"non\", \"information\", \"model\", \"results\", \"matrix\", \"given\", \"learning\", \"models\", \"two\", \"also\", \"log\", \"case\", \"function\", \"method\", \"figure\", \"probability\", \"cjk\", \"alto\", \"mussa\", \"proprioceptive\", \"lippman\", \"cursor\", \"utj\", \"selectbatch\", \"staged\", \"policy\", \"ofil\", \"bourlard\", \"barlow\", \"kti\", \"stmil\", \"humieres\", \"kolen\", \"somewhere\", \"cues\", \"werbos\", \"sbmil\", \"xreal\", \"bmi\", \"stateedu\", \"hairston\", \"proprioception\", \"action\", \"vis\", \"gains\", \"juti\", \"cause\", \"vw\", \"schedule\", \"pk\", \"schedules\", \"causal\", \"xij\", \"tree\", \"data\", \"model\", \"integration\", \"modality\", \"uniform\", \"figure\", \"cpe\", \"mi\", \"despot\", \"dd\", \"probability\", \"value\", \"may\", \"information\", \"output\", \"target\", \"single\", \"models\", \"network\", \"following\", \"learning\", \"choice\", \"estimate\", \"different\", \"networks\", \"state\", \"algorithm\", \"experiments\", \"performance\", \"also\", \"results\", \"vector\", \"since\", \"set\", \"case\", \"used\", \"one\", \"using\", \"two\", \"number\", \"time\", \"given\", \"function\", \"image\", \"matrix\", \"distribution\", \"problem\", \"first\", \"fish\", \"electrosensory\", \"inverter\", \"template\", \"eqk\", \"fmite\", \"nlpd\", \"assad\", \"gbp\", \"pap\", \"residue\", \"impedance\", \"fahlman\", \"nik\", \"elecbic\", \"eod\", \"electric\", \"textures\", \"animals\", \"ptv\", \"rcc\", \"video\", \"grammar\", \"citeseer\", \"lissmann\", \"sprite\", \"rasnow\", \"equipotential\", \"nij\", \"bags\", \"xk\", \"games\", \"agp\", \"circuit\", \"td\", \"xi\", \"latent\", \"cover\", \"online\", \"posterior\", \"inference\", \"em\", \"vertex\", \"state\", \"process\", \"two\", \"environment\", \"function\", \"belief\", \"st\", \"set\", \"transition\", \"learning\", \"however\", \"using\", \"ij\", \"time\", \"given\", \"different\", \"section\", \"figure\", \"experiments\", \"new\", \"methods\", \"model\", \"also\", \"models\", \"number\", \"method\", \"data\", \"algorithm\", \"used\", \"bound\", \"one\", \"network\", \"problem\", \"approach\", \"first\", \"matrix\", \"probability\", \"results\", \"logitboost\", \"alg\", \"submatrix\", \"adaboost\", \"shraibman\", \"losses\", \"ktr\", \"elicitation\", \"guaranteeing\", \"xr\", \"boosting\", \"vut\", \"eav\", \"ramanujan\", \"dna\", \"risks\", \"masnadi\", \"soft\", \"vasconcelos\", \"rror\", \"savageboost\", \"incoherence\", \"fw\", \"kvk\", \"usher\", \"singular\", \"quic\", \"sensing\", \"oki\", \"schwing\", \"canonical\", \"risk\", \"bidder\", \"recovery\", \"dual\", \"program\", \"loss\", \"max\", \"auction\", \"margin\", \"optimal\", \"brownian\", \"link\", \"primal\", \"bound\", \"matrix\", \"norm\", \"svd\", \"algorithm\", \"rank\", \"function\", \"functions\", \"error\", \"one\", \"problem\", \"since\", \"algorithms\", \"classifier\", \"distribution\", \"learning\", \"consider\", \"set\", \"model\", \"case\", \"given\", \"probability\", \"time\", \"method\", \"two\", \"data\", \"results\", \"number\", \"information\", \"models\", \"using\", \"first\", \"figure\", \"used\", \"teacher\", \"pmlp\", \"mlp\", \"aic\", \"cnn\", \"saccadic\", \"horiuchi\", \"restored\", \"pretraining\", \"pretrained\", \"delgado\", \"marcaurelio\", \"deep\", \"emp\", \"kpca\", \"wiesel\", \"mikio\", \"colliculus\", \"dof\", \"eemp\", \"lengauer\", \"primate\", \"postponing\", \"annealed\", \"replica\", \"convolve\", \"hasnt\", \"plasticity\", \"oculomotor\", \"ultrasonic\", \"planar\", \"ot\", \"zt\", \"ltp\", \"braun\", \"tanh\", \"spike\", \"rbf\", \"rin\", \"motor\", \"entity\", \"system\", \"space\", \"layers\", \"eye\", \"estimation\", \"breaking\", \"model\", \"mean\", \"xi\", \"tensor\", \"experts\", \"xj\", \"architecture\", \"prediction\", \"learning\", \"training\", \"input\", \"error\", \"network\", \"may\", \"local\", \"networks\", \"learned\", \"used\", \"two\", \"neural\", \"data\", \"control\", \"image\", \"function\", \"matrix\", \"let\", \"method\", \"number\", \"case\", \"one\", \"problem\", \"figure\", \"using\", \"models\", \"first\", \"algorithm\", \"set\", \"time\", \"results\", \"also\", \"regreti\", \"fptas\", \"modifiers\", \"oco\", \"retton\", \"viz\", \"payoff\", \"serial\", \"univariate\", \"investors\", \"exponentiated\", \"zy\", \"bivariate\", \"regret\", \"sublinear\", \"kale\", \"satyen\", \"portfolio\", \"bonf\", \"papadimitriou\", \"minp\", \"maker\", \"portfolios\", \"realization\", \"zinkevich\", \"hommel\", \"fxy\", \"wealth\", \"dts\", \"aggregates\", \"edist\", \"axes\", \"test\", \"multivariate\", \"equivalence\", \"parallel\", \"equilibrium\", \"margin\", \"fat\", \"tests\", \"parties\", \"convergence\", \"xu\", \"points\", \"displays\", \"distribution\", \"lp\", \"network\", \"decision\", \"set\", \"matching\", \"model\", \"max\", \"dual\", \"figure\", \"yn\", \"analysis\", \"also\", \"point\", \"et\", \"ie\", \"error\", \"consider\", \"approach\", \"data\", \"two\", \"given\", \"algorithms\", \"probability\", \"learning\", \"linear\", \"algorithm\", \"problem\", \"neural\", \"one\", \"number\", \"using\", \"matrix\", \"function\", \"time\", \"based\", \"let\", \"models\", \"log\", \"tokamak\", \"transistors\", \"plasmas\", \"italy\", \"lucid\", \"plasma\", \"tokamaks\", \"eubank\", \"trotman\", \"moody\", \"gs\", \"sinb\", \"magnetic\", \"windsor\", \"toroidal\", \"vacuum\", \"vessel\", \"cached\", \"vertex\", \"schnurrenberger\", \"pelf\", \"jt\", \"noncircular\", \"exemplar\", \"bank\", \"dekker\", \"cover\", \"marcel\", \"haynes\", \"cy\", \"predictor\", \"brief\", \"descriptor\", \"intensities\", \"aggressive\", \"vertices\", \"shape\", \"generative\", \"table\", \"hierarchical\", \"manifold\", \"svd\", \"interest\", \"attentional\", \"recognition\", \"features\", \"decay\", \"tree\", \"algorithm\", \"set\", \"error\", \"neural\", \"using\", \"one\", \"model\", \"parameters\", \"time\", \"function\", \"following\", \"models\", \"value\", \"example\", \"linear\", \"probability\", \"given\", \"network\", \"data\", \"information\", \"vector\", \"number\", \"learning\", \"figure\", \"performance\", \"based\", \"problem\", \"two\", \"results\", \"distribution\", \"also\", \"matrix\", \"klc\", \"marginalised\", \"neuronl\", \"ife\", \"chiang\", \"bde\", \"nats\", \"nnc\", \"whilst\", \"gates\", \"pointer\", \"sato\", \"compo\", \"chuang\", \"lkl\", \"tnk\", \"polack\", \"etann\", \"vbem\", \"reeves\", \"glaus\", \"ccd\", \"bruck\", \"scenarios\", \"collapsed\", \"lmf\", \"roychowdhury\", \"genesis\", \"tmi\", \"kurihara\", \"lawrence\", \"despot\", \"ascent\", \"parity\", \"threshold\", \"interface\", \"riemannian\", \"bounds\", \"sigmoidal\", \"covariance\", \"clusters\", \"al\", \"power\", \"means\", \"kernel\", \"bound\", \"figure\", \"variational\", \"gradient\", \"learning\", \"two\", \"neural\", \"one\", \"error\", \"networks\", \"time\", \"function\", \"also\", \"test\", \"network\", \"algorithm\", \"recognition\", \"input\", \"results\", \"number\", \"models\", \"set\", \"given\", \"data\", \"first\", \"value\", \"probability\", \"distribution\", \"model\", \"using\", \"method\", \"used\", \"based\", \"matrix\", \"problem\", \"romma\", \"iiwt\", \"stevens\", \"aggressive\", \"subbands\", \"mistake\", \"odelia\", \"wavelet\", \"mistakes\", \"symptoms\", \"hallucinations\", \"dnf\", \"wq\", \"friess\", \"rosenblatt\", \"decorrelated\", \"percep\", \"lemmas\", \"schizophrenia\", \"wtll\", \"frontal\", \"men\", \"hex\", \"sahar\", \"monochromatic\", \"awt\", \"iiwtll\", \"lowpass\", \"iixtll\", \"genesis\", \"spontaneous\", \"modules\", \"wt\", \"coefficients\", \"arithmetic\", \"game\", \"descent\", \"retrieval\", \"display\", \"dual\", \"xr\", \"algorithm\", \"mcts\", \"solution\", \"function\", \"learning\", \"functions\", \"convergence\", \"results\", \"human\", \"using\", \"smooth\", \"let\", \"algorithms\", \"two\", \"method\", \"network\", \"max\", \"experiments\", \"distribution\", \"also\", \"simulation\", \"values\", \"performance\", \"neural\", \"one\", \"problem\", \"large\", \"set\", \"models\", \"model\", \"data\", \"used\", \"matrix\", \"bound\", \"first\", \"image\", \"time\", \"figure\", \"based\", \"error\", \"number\", \"bkm\", \"hamerly\", \"lilliefors\", \"pg\", \"pgmeans\", \"saund\", \"xmeans\", \"wrapper\", \"repairing\", \"erf\", \"eccentric\", \"means\", \"feng\", \"ecc\", \"accept\", \"kenichi\", \"greg\", \"eccentricity\", \"centers\", \"kcjk\", \"kauffman\", \"harmonium\", \"dasguptas\", \"fitness\", \"kurihara\", \"responsibility\", \"dimensionwise\", \"wilkinson\", \"gmeans\", \"pelleg\", \"accepted\", \"terrorism\", \"cjk\", \"csp\", \"tensor\", \"clusters\", \"estimates\", \"mta\", \"see\", \"matrix\", \"satises\", \"datasets\", \"operators\", \"spherical\", \"model\", \"task\", \"threshold\", \"sparse\", \"vol\", \"cluster\", \"nd\", \"functions\", \"data\", \"large\", \"function\", \"distribution\", \"using\", \"loss\", \"basis\", \"structure\", \"random\", \"learning\", \"one\", \"first\", \"number\", \"parameters\", \"well\", \"linear\", \"set\", \"two\", \"mean\", \"algorithms\", \"models\", \"method\", \"figure\", \"probability\", \"error\", \"algorithm\", \"results\", \"based\", \"image\", \"log\", \"problem\", \"also\", \"time\", \"used\"], \"Freq\": [1360.0, 1128.0, 824.0, 628.0, 960.0, 227.0, 1108.0, 525.0, 508.0, 408.0, 889.0, 762.0, 821.0, 707.0, 572.0, 933.0, 346.0, 390.0, 443.0, 297.0, 327.0, 286.0, 419.0, 530.0, 411.0, 500.0, 192.0, 362.0, 554.0, 288.0, 8.085775167186517, 5.281234945956361, 4.345435185115962, 3.930504751595495, 6.32196398110186, 3.1164584715996684, 2.7917216113071497, 2.808781108241331, 29.96533841273242, 6.889880067550916, 2.043825622143787, 2.39951112016152, 2.716984703424222, 2.3270044849203133, 1.656530624948575, 2.306633403702286, 10.137067014432533, 2.3271440959448944, 3.04244603173619, 3.319972265133283, 2.99347975577875, 3.6384228560487024, 2.000866393514283, 1.6512497975158795, 1.2936709260470183, 1.286462361445754, 13.195396899687415, 3.386372939444196, 3.752852957047857, 1.2437663184639407, 8.537981744270224, 18.78859294959033, 62.79441697246587, 15.256593583136844, 122.53635694193518, 125.1320913571459, 35.928624422446035, 36.93190050933307, 59.90639420988436, 97.12306239857679, 23.759092565615187, 232.47345605914563, 22.85159718813288, 31.655401557438378, 16.75993479852636, 10.53944215843736, 179.01872123796707, 165.53652663915744, 141.77433783522105, 128.48975913795488, 92.49550567510406, 74.7379751795634, 117.16352961731154, 169.52827642937038, 92.41947103153898, 103.11252412780266, 78.90731450535355, 166.8790176269132, 94.52710374326482, 90.1744395100768, 68.48014979684714, 72.14842461637585, 90.6166323651387, 198.21827945236427, 95.52843929081403, 108.33729261326094, 96.18699796973226, 159.6384082527172, 98.94196010607486, 114.15600412999797, 89.5617293072466, 78.42970896353143, 77.45604981924903, 100.87057159542314, 81.6475285427288, 90.34241259515775, 81.74908343474092, 2.7447317502953066, 0.8649661012552621, 1.1454297406919063, 3.1510758473184097, 0.7698013377995537, 8.057903079214178, 1.5498334802096387, 4.441782636560734, 2.3460271388695353, 53.65420568102396, 1.5656974520645226, 0.7289441356343203, 0.7239163115973082, 4.344252372993639, 2.013755189115382, 0.7214190006492717, 2.1300768667269465, 1.20135356446593, 7.889712895083002, 1.9128033867237568, 1.9199930082545402, 2.6079225225579052, 2.0021440844820333, 0.689240971155821, 0.9527116550870818, 0.7069795059563178, 27.464356908198194, 5.160253464468314, 2.1356601476159645, 0.9331508825847895, 14.15417267680055, 7.575294052458763, 11.195392100510865, 9.167944991451856, 7.447017386937746, 7.069135561945657, 10.57732188619775, 68.9170003935308, 219.4000382189872, 252.54417440822505, 8.0911659706483, 10.617557338087666, 23.98124280098573, 129.19011173667974, 10.47405580035288, 14.27621731709607, 20.02034245813444, 7.180140906098527, 90.36933578103124, 71.25936919967582, 54.881198947536085, 81.12402215194994, 53.728539454511996, 41.76649561896025, 54.83804747188903, 93.82212075793751, 89.14019984012822, 62.4227033086484, 156.12354302010272, 30.274116076964635, 39.38010975347384, 62.126709159927636, 58.560781042254575, 67.13863740867603, 129.63015462102402, 60.25807005272279, 58.93085354632406, 80.2435461611041, 76.66657120890581, 54.05858226019267, 52.8389287177116, 114.89613931142735, 63.4043248826617, 73.72410457939611, 108.62789437478416, 101.82085196821113, 94.20685142522416, 81.39654621164728, 85.4380912845121, 74.11308512072299, 86.74928030266902, 65.92629521043277, 74.58700638326765, 68.10763706963834, 72.14053340050815, 62.414302889844365, 16.163099111548696, 1.4589411920596187, 1.8838706231763123, 7.000030928431323, 1.6006710928058052, 1.8402908526016288, 3.1674198206372512, 1.2908422202405052, 2.207246629360208, 2.580132807465679, 2.592620607202623, 2.2861113863419247, 3.06656171934847, 1.586999798130326, 0.7530229056980839, 2.5085940959768203, 12.311138649057837, 2.0216235681451042, 2.456331307098615, 2.1890304751405916, 4.151705893003433, 10.378168030819785, 5.088686276814963, 1.243273436252982, 0.7195668779216258, 5.4408061651177935, 1.1838820863975916, 0.9381140187980881, 1.3890186382298262, 8.34960708582708, 12.726408827909317, 22.11656769269396, 6.261288499567075, 27.842664464599586, 12.833337242567096, 98.425524827038, 44.163523505768694, 16.5855743250292, 29.764827965403498, 26.641233847653076, 45.6694878466551, 44.07568656448746, 14.193922564261216, 77.44502015934812, 40.831496973870294, 125.61956932342189, 9.693403833887377, 131.0753525925457, 29.82988271058844, 25.595345424423797, 138.95255586043254, 27.597469080370065, 161.0548316704213, 59.17203732623168, 121.55599583808177, 26.749610916601842, 102.97821683735665, 83.67199849431435, 62.45671971549483, 61.07269261245735, 99.46349475122742, 59.96304602004922, 54.11215120050107, 62.935848000683805, 161.6322633589675, 78.61609595062926, 82.14700959449677, 84.89759999935295, 69.99777957067131, 122.47605729544385, 108.2658414982482, 69.68986188763778, 57.26337316301397, 94.08866927079488, 70.031295886625, 70.36489481237435, 59.855385022567624, 62.92429352710674, 66.79766313603598, 61.943386851485656, 61.12525086771764, 3.431358893341214, 7.4517825244898015, 1.5425669162932767, 2.6876748422051793, 1.4796378756507014, 7.618868049979292, 0.8783975551544132, 1.4181439681442307, 0.8518711953882525, 23.318352855307605, 12.492202897684221, 1.60711668276397, 1.333252725297283, 0.8004915815030497, 1.638718249754547, 1.507480659439359, 0.7494825063374423, 15.274416384637636, 0.7380646757920031, 2.2502078319771464, 0.9653559994295303, 2.256694245753679, 3.924667779177865, 1.2435824475358224, 2.5089475093251585, 6.571083952524696, 5.935221033572309, 18.016369652615875, 1.5003694754407808, 1.9744287316044677, 19.86455729283012, 24.393231822944813, 12.804894342886445, 11.164320720495558, 43.31449416582725, 17.12680919127322, 39.32994747737178, 58.9864916680433, 10.783951962487087, 25.29185149869744, 69.8618768079144, 8.780085218320597, 18.84288513562134, 13.655699281006791, 68.08177147720215, 102.82976075334403, 32.30688203905985, 20.178485899471593, 129.5183689950421, 33.51429543634051, 110.18995194480586, 53.930294160795675, 73.20942577371977, 109.65574652219283, 79.96174944282384, 50.7621143885095, 59.7605733817586, 29.722547868330167, 67.71208339847685, 119.47082075075316, 42.28068541055675, 99.4679334368778, 128.66873913857552, 56.50106537648768, 66.24410340444685, 64.43611538151242, 74.64625354015888, 60.46607762577816, 76.26834512355771, 90.5089217130321, 59.22727255030546, 64.27130833195913, 56.03858279936815, 62.07735550178209, 72.17122709076162, 54.93997752895579, 61.68187179409889, 54.365780899771345, 4.938074922518559, 3.5056138053726196, 4.916730913058704, 3.2632118291764214, 4.6011694293916525, 2.860824224551039, 1.6219391849162301, 1.1859162673286223, 0.7855825296690593, 1.5641839743614718, 1.4416160007749315, 0.38501526409408554, 21.93967684884532, 0.7530268228899072, 1.6665863336095788, 0.5539377922085081, 0.9039618080687445, 2.6506653076661686, 1.518547315586169, 1.0966512161824358, 0.35763098511742825, 0.5458596417062165, 0.5368545253491579, 0.5260992388184819, 0.5299673672154603, 0.5313804321080309, 0.5253249501260772, 4.105367122532651, 1.6177797359698853, 0.8878313645715145, 1.9470670622865578, 1.9155609787688475, 13.60647319812749, 1.412158480118968, 1.2029439884314364, 3.1792228145519354, 6.7706593811958555, 6.642737356556882, 3.221868105966619, 8.493824887618358, 5.070946265601994, 42.65908444265155, 50.200242733892416, 10.092605617473293, 8.012182770598018, 22.78742590929081, 4.028912587743034, 163.09428838988117, 39.642095901983055, 62.169592423170364, 15.247943714936829, 9.309006749059206, 14.845919736112855, 13.701725477749756, 27.01669690295513, 121.48929208561049, 49.05912014113214, 43.68585526636968, 61.41782407263975, 63.84475578773749, 37.978396474351115, 30.25651203439567, 42.2169626157299, 17.486918019434594, 52.85815050099702, 70.76973748757543, 46.29691247154752, 93.81829504855949, 28.858828748962775, 47.50640850187064, 72.07797943406985, 57.946445901675, 40.11807912930302, 48.632495727620295, 57.15107074353035, 42.58593495404394, 71.54627718281402, 53.285340685609924, 58.99986925901213, 63.97504815755194, 52.76742439018322, 46.01080202574257, 65.67816061451133, 62.627244666816054, 48.88714458544726, 44.022460782944044, 43.741296119514175, 0.9517962156344313, 1.6145645861827849, 3.113999792570526, 2.611085959299569, 1.7842007764627246, 0.6470803694760465, 9.218903876575647, 8.596152610318509, 17.69876484903071, 1.0124357392706504, 0.8012338550002045, 2.032499506501207, 1.6480439179006452, 30.119622738008374, 1.2103120198194894, 0.7951056819194608, 0.5939610116541639, 0.7931440167360212, 0.805603743871758, 0.789631420879398, 2.4257364428661203, 0.9733982261456057, 1.161047713959284, 1.1705876791420613, 0.7739700217831439, 1.4307568493427572, 2.2007799182042866, 1.1575213560137163, 1.5236939385126642, 0.7789541680164087, 1.5371275817263472, 5.265001809430279, 66.40145149333459, 13.129125924673826, 2.8114582855267978, 13.454255134114739, 12.490760220675492, 18.72771282499969, 3.1671580602160483, 14.784495813963622, 8.688110037383643, 27.19606244849995, 7.6544171201500655, 45.713743663359715, 10.76679781005011, 62.146669868732076, 14.381668971639554, 67.01706236358753, 20.04882682796575, 98.25704285861634, 20.84780660886174, 127.91922170522699, 36.084941784532134, 24.79956349265821, 71.72931966008183, 20.519141819456895, 37.409312153121476, 55.69972424486788, 37.2251214897035, 29.50903545585187, 30.951926636959286, 51.710550369388905, 31.656307237221586, 44.269728814783825, 87.52713085199092, 65.3868592205386, 51.42030819729997, 42.18374798623991, 48.943005065280914, 82.0167927085754, 44.02247657101396, 71.52240140334192, 51.813019762372484, 42.831089409774286, 66.12304982531748, 52.73896715710006, 60.85803439567547, 51.400227465126, 59.932018899889194, 52.35538113389272, 41.714018663853544, 38.13753720406831, 41.0420382772832, 38.32690808801772, 3.8919933513306915, 1.2502741245534945, 1.222125072196844, 0.7867618459484107, 8.187600171309363, 7.752002128281379, 1.2762252537825083, 0.8656292151835097, 1.4729089848523627, 3.099154550463221, 0.9088719420515015, 0.5326652083264989, 4.158482522674978, 1.0693247493298048, 0.7115095945677877, 1.0551850944674004, 1.0465422869444663, 1.224956606321993, 11.815404930282442, 0.5115458388416051, 1.1470319713897377, 1.4655476759671011, 0.3391868894591347, 3.400068378368061, 2.5987277688198067, 0.48211080090244984, 13.325899828281658, 0.47911484744280675, 1.355088340024813, 1.4237183367212425, 4.07527308253858, 10.062688977951813, 4.613566959204997, 2.4745247848885596, 2.178299406429261, 7.536265469773882, 6.602786782342703, 13.51284281002495, 25.995513543421527, 15.826585677030167, 14.61651593785316, 13.682192617049331, 12.026176036509005, 7.538636752133671, 25.523900289874, 29.791660314416077, 4.8140675846787495, 37.47732146770556, 94.55842144072055, 90.90050302739445, 55.01791898288011, 46.92405353653922, 76.44428686480722, 80.686698098472, 112.9274884497068, 37.34048759810373, 63.99158239642432, 72.5698638455672, 38.82099882718204, 55.80680328970217, 40.618304677574514, 30.72449668889613, 43.479624502199826, 46.70294994814034, 47.93448523139445, 49.39441198919907, 78.07787851088452, 43.60533292410628, 33.65414425329782, 51.2151609026274, 75.16486473081581, 54.12774393599222, 34.91281222792008, 41.53297572388554, 45.675105644103695, 50.221637103643985, 40.847366202553296, 39.91522163338555, 41.23098747664544, 39.340100817089045, 5.972696896399705, 1.6059511510728, 1.1935927309028658, 3.274452579641678, 2.207033792926177, 0.737221993554517, 0.9202973256999769, 6.414331897974391, 2.056458134367742, 4.503013654829627, 0.571421942680392, 1.2621826116039832, 1.055720524575166, 1.53204472701872, 4.836662592093322, 0.550735648335797, 0.5271205882375284, 0.54638073725082, 5.001492859164125, 1.040266084731668, 0.8448068280956119, 4.019576335534904, 0.8317025194477448, 10.457045402070218, 6.637909471291743, 1.4501775508892905, 1.2817360914385045, 4.136670464963636, 0.6855261387276886, 1.230461316303501, 2.7720173294245973, 16.689586363631225, 4.5591793511671455, 3.3826200933957544, 17.572000144442853, 2.045218722773811, 1.9230051802512709, 17.83238532220946, 6.883247039276202, 23.187858914532885, 21.907219111259085, 30.05342595814206, 14.98187878800912, 28.293574442151407, 31.355642564326985, 43.91739597180966, 71.16382443340453, 22.667749642292996, 21.651807987910342, 98.72137621552886, 70.05482933184186, 44.865301800992306, 77.92597375711752, 50.65291080717463, 36.37550288604675, 59.37913461293316, 66.43650143384099, 48.93494648443226, 38.47993809503998, 47.749839190688455, 67.77179012562492, 22.678786945590925, 32.54798991486157, 42.69058467657355, 47.84874282677146, 45.882139502007476, 59.14322345552283, 42.18878263264345, 63.83839630191149, 39.466198702524025, 35.021337478537696, 39.91389147713824, 39.12605940081877, 64.06107384616683, 49.222708873644514, 37.367251281351315, 37.24108791549225, 36.39304947032627, 38.5397263087913, 36.9998880917353, 7.123251442619748, 1.9146034268863765, 1.695474797807944, 2.358945847801486, 0.4955026204112791, 4.057790195346557, 0.4909777673960415, 1.9432230543485314, 2.0502791223434205, 1.0843470728702695, 2.028610541329889, 1.797387023571629, 0.6251792777744234, 0.46289796005590556, 0.6198698590381624, 0.612093757531897, 0.7590438671087043, 0.44963083156679806, 3.410282688357958, 0.5974811220164691, 2.336322487514943, 0.5873842916198275, 0.9940149104783393, 0.698883725012541, 0.28534777985716114, 0.43399306938345084, 0.5810262216848742, 0.4275220421162397, 0.5777767827731367, 3.573739497473543, 2.531664536119327, 3.2988044609725518, 12.555687188610383, 8.626307906462802, 5.058398873462403, 12.23818926382114, 15.80979481491667, 5.297105906375438, 5.0756572835482165, 23.12743850321084, 9.759913424093908, 91.55029335820821, 4.117119966745913, 25.90816471113935, 76.47405855765065, 99.22639117936755, 35.11737248899401, 20.771874759102804, 46.89940210054255, 22.52057075411979, 63.45320905979875, 13.048854546947611, 34.1259691407276, 36.150291205337034, 57.98529017829551, 41.47104062232388, 45.58022241711835, 27.075468214313247, 32.42899635056345, 40.77178661214908, 42.36876928905189, 12.138941982858444, 25.928701626504246, 30.54146038450303, 34.56381856495192, 56.03513553047572, 42.05545511158167, 27.077111312308784, 56.26180705152939, 40.53763067115099, 68.27846671385007, 58.207980624910654, 36.66446868402356, 41.019434776335174, 30.186484263033687, 34.320502533586655, 33.385901879240784, 39.344321703997096, 39.25961167826252, 33.20936656200242, 33.187210073172764, 33.42079780627372, 3.153150702985164, 0.6609492723565892, 0.6437380185193935, 7.5643276864433835, 0.7234990779685408, 0.741009915205861, 0.46593135387794943, 0.5735463778703851, 0.3424379305121406, 0.6737947752204542, 0.9131904535929778, 29.00733180042794, 0.3253401350335735, 0.3229499053340125, 0.4278931370641137, 0.21261015156744018, 0.428078053772007, 1.2600570309258048, 2.5283397303521635, 0.31876186273363477, 0.4290247206497301, 0.3195816962497474, 0.20589333756994732, 0.9327224579475775, 0.7717220887197115, 0.3093685689727624, 0.20367650374237778, 0.30071408787660603, 0.5981668833594503, 0.39312933764696584, 0.4876696920577778, 0.689682987096015, 0.9174609545450727, 4.857175082892112, 9.682281491967945, 15.306292745823802, 7.496393004630891, 6.025001343255353, 23.92776533462103, 47.37310922835436, 1.1226372502884912, 11.28669851633441, 2.0371862875176943, 2.174945984897458, 82.86105314028133, 19.244717623883407, 9.270030066724436, 13.05391357699619, 6.605420139324588, 13.08137091645105, 3.9867268888283878, 23.26254918631315, 62.65096330117118, 22.14809693784948, 48.41680012770867, 31.603623045929684, 46.40771701312316, 13.137672262336594, 11.733189282544807, 18.37462276323179, 18.15045604658785, 55.508951042215045, 45.48096480650663, 28.579643705076208, 34.201503625119095, 21.542554136178925, 21.018491352674086, 25.671035056610936, 43.64334306348123, 37.55513903946837, 18.169104050745364, 23.58455192476933, 29.925966671901403, 26.36721921182303, 31.282806974404735, 25.971739123651975, 25.3855096596206, 35.247954525088744, 25.486581682154295, 23.902237328929637, 23.16110649703015, 22.431730334418, 24.39449077200935, 23.612466597659484, 24.437933448361424, 22.629663711879964], \"Total\": [1360.0, 1128.0, 824.0, 628.0, 960.0, 227.0, 1108.0, 525.0, 508.0, 408.0, 889.0, 762.0, 821.0, 707.0, 572.0, 933.0, 346.0, 390.0, 443.0, 297.0, 327.0, 286.0, 419.0, 530.0, 411.0, 500.0, 192.0, 362.0, 554.0, 288.0, 21.09472078727291, 13.986005898147784, 11.879232352344747, 10.976923934640299, 17.70467751117967, 8.819382923972208, 7.973442126289532, 8.034006664893756, 86.78569607411778, 20.002363166753756, 5.9473215163710815, 6.992165758376772, 7.931995015977557, 6.811312104980501, 4.868271084644643, 6.828025638140135, 30.090521983862047, 6.931006712511046, 9.076382518355965, 9.950970445613299, 8.974892943445422, 10.911503863113586, 6.0020679529491145, 4.960207043588296, 3.932778036701491, 3.9458504222403565, 40.99453588074517, 10.534996957826475, 11.720333340691708, 3.8893969487096527, 27.01786962404514, 62.2427296113232, 223.00351402703157, 51.80926737379922, 487.2537300854063, 498.9571718002326, 132.37186445647026, 136.716940838829, 230.5894277142383, 408.8061099256945, 87.72645427501664, 1108.9791179260374, 86.06665360776948, 124.7262325320294, 61.075408200366056, 36.43623385307588, 889.1891306064423, 821.4456059008129, 693.2323973783047, 635.6314567423365, 435.6804915763389, 352.02082070265976, 593.8540073404303, 933.6780691614684, 455.6224400156853, 521.8248953791962, 384.2371451821891, 960.6224042087231, 493.93517898678215, 470.1405678534758, 334.632283048097, 363.81143991271404, 487.4914213906193, 1360.2050486032454, 530.3127445317438, 628.1707673832794, 548.5314503886958, 1128.4152716561077, 602.9504487625196, 762.2242623635655, 554.897969952773, 433.58077859913226, 424.59274210631276, 824.7923787341703, 500.7665329017002, 707.2410668183218, 527.0368711545843, 8.366807605815373, 2.8129773323393765, 3.860354641788895, 10.752818314345054, 2.8012758446812627, 29.781781241198033, 5.754465453368009, 16.51204585443871, 8.725879989022419, 199.62059055377844, 5.862104149283125, 2.7418308397086952, 2.765035599656574, 16.790932067933625, 7.8006576749970495, 2.797758934572007, 8.260899500758786, 4.750212218883619, 31.28798630822716, 7.590038741187833, 7.663703487559452, 10.446542219542822, 8.030492229844373, 2.7704805979179703, 3.8390948293232263, 2.858127072674422, 111.06518284778875, 20.885894147232463, 8.662320552084042, 3.7909766837524232, 57.959299290537714, 30.946508880289148, 46.24934485203981, 37.89391717936674, 30.70671972174552, 29.30259136669235, 45.681145101696266, 327.088098248238, 1108.9791179260374, 1360.2050486032454, 35.00503398432296, 46.88809872494206, 111.73925429647431, 707.2410668183218, 46.713644088730746, 66.00994271534924, 98.0590622290081, 31.565077802275106, 527.0368711545843, 411.9343462165857, 310.72783244718767, 487.4914213906193, 306.4318354769803, 230.5894277142383, 319.9705622187174, 602.9504487625196, 572.2577388501712, 377.3621780275135, 1128.4152716561077, 161.82799730690388, 223.06522205301923, 388.6057787709544, 362.1937730123834, 430.51142761006184, 960.6224042087231, 378.8532211209495, 372.12942539005627, 554.897969952773, 530.3127445317438, 339.45862847538365, 332.375910929823, 933.6780691614684, 424.59274210631276, 521.8248953791962, 889.1891306064423, 821.4456059008129, 762.2242623635655, 635.6314567423365, 693.2323973783047, 548.5314503886958, 824.7923787341703, 470.1405678534758, 628.1707673832794, 508.828069847511, 593.8540073404303, 493.93517898678215, 52.00203549477913, 4.737931284098864, 6.5549528636256555, 24.397177949597307, 5.667741391561009, 6.618151789923827, 11.418141482656175, 4.660492358842633, 7.976158866387589, 9.355123487250117, 9.406650607637705, 8.333005308922926, 11.377979200456394, 5.896090640591655, 2.8130150377126775, 9.41042423621236, 46.22321164183351, 7.7458912906755835, 9.471409133316962, 8.491103661366592, 16.17583594999313, 40.445308517801394, 19.926986547930465, 4.870730814269149, 2.8368806191880087, 21.494729792079063, 4.701928138427979, 3.7398215286449217, 5.555155187647909, 33.47437507779313, 52.00367104806363, 91.43876321309588, 25.562424596913765, 120.99134022273103, 55.56933326956318, 487.2537300854063, 210.25642002934356, 73.5619663042061, 141.7150297793175, 125.86956876312959, 229.52347148720247, 223.00351402703157, 63.579752335536334, 430.51142761006184, 210.68590814419247, 762.2242623635655, 42.131394125821274, 824.7923787341703, 155.44542109164217, 130.548657603119, 933.6780691614684, 143.15149213951534, 1128.4152716561077, 349.55090985038015, 821.4456059008129, 138.9178125766528, 693.2323973783047, 548.5314503886958, 388.6057787709544, 378.314997805217, 707.2410668183218, 378.8532211209495, 333.99458724540756, 405.42653450262003, 1360.2050486032454, 554.897969952773, 602.9504487625196, 635.6314567423365, 500.7665329017002, 1108.9791179260374, 960.6224042087231, 521.8248953791962, 390.57508876103054, 889.1891306064423, 572.2577388501712, 593.8540073404303, 435.6804915763389, 493.93517898678215, 628.1707673832794, 527.0368711545843, 530.3127445317438, 10.054140187281071, 21.887717669141068, 4.600031114533374, 8.139684793095158, 4.562337282988108, 24.251027273037856, 2.7996274920219824, 4.529574205034339, 2.752903501953715, 77.39514347239829, 42.51889429340806, 5.4747743053244955, 4.5451443193366305, 2.7794079723154823, 5.761943268907013, 5.395490038109119, 2.7373371590835496, 56.236600195252315, 2.7294669225244146, 8.34320763326386, 3.581934543223843, 8.380174334721858, 14.59258745092398, 4.636496747656595, 9.358023123514101, 24.55994983738316, 22.199648569287046, 67.6260777061263, 5.673138234247541, 7.510748843991895, 76.05613640228428, 96.97020096234198, 51.51144540141997, 46.18923728466131, 192.73030950833765, 74.09110501155241, 181.29041170965496, 297.4388866141693, 46.17255091036723, 121.3170468616421, 377.4767927232455, 37.65467831357871, 88.97884078619029, 62.21355130463359, 390.57508876103054, 628.1707673832794, 167.7706557263604, 99.35984475241973, 960.6224042087231, 192.35777521866527, 824.7923787341703, 346.0880406828208, 525.5147949966467, 889.1891306064423, 593.8540073404303, 332.375910929823, 419.52208749345243, 169.1631521704892, 508.828069847511, 1128.4152716561077, 276.9964893765867, 933.6780691614684, 1360.2050486032454, 424.59274210631276, 548.5314503886958, 527.0368711545843, 693.2323973783047, 500.7665329017002, 762.2242623635655, 1108.9791179260374, 530.3127445317438, 635.6314567423365, 487.4914213906193, 602.9504487625196, 821.4456059008129, 493.93517898678215, 707.2410668183218, 521.8248953791962, 20.4249438095907, 15.600600240958892, 22.043525252446894, 14.814753624180991, 21.02164793203298, 13.154327405893888, 7.471138786344392, 5.50771443580732, 3.674672585381014, 7.3472240887300915, 6.80576755262715, 1.8316252052741189, 106.66300528140827, 3.668191941044012, 8.12983275969654, 2.7124833829220725, 4.5357642118932535, 13.32199032962203, 7.6734625618348185, 5.554575850897921, 1.827531860666473, 2.79785264385677, 2.755211994152254, 2.7133823407273163, 2.747841801877903, 2.756572466867798, 2.7304446987905333, 21.429892577056513, 8.481320654955162, 4.658497638006256, 10.235950709031368, 10.191708046081578, 76.5843699327917, 7.488806280362035, 6.360196004394362, 17.360284569997305, 38.24520279665924, 37.518241412934444, 17.726040812701733, 48.698414059619566, 28.688556328234224, 282.71915673837657, 353.017189364967, 62.857964243822785, 48.875974370615104, 154.17079591650958, 22.971362292853907, 1360.2050486032454, 286.96056507235556, 487.2537300854063, 100.58756708154546, 57.970812569913946, 98.24056843624096, 89.72873662951432, 195.6996781155507, 1128.4152716561077, 396.47332657925415, 348.3075874386722, 525.5147949966467, 572.2577388501712, 310.72783244718767, 238.65574582044908, 362.1937730123834, 125.7907508722538, 521.8248953791962, 762.2242623635655, 443.2154865735635, 1108.9791179260374, 242.7665963595096, 470.1405678534758, 824.7923787341703, 628.1707673832794, 384.2371451821891, 500.7665329017002, 635.6314567423365, 424.59274210631276, 889.1891306064423, 593.8540073404303, 707.2410668183218, 821.4456059008129, 602.9504487625196, 493.93517898678215, 960.6224042087231, 933.6780691614684, 693.2323973783047, 530.3127445317438, 554.897969952773, 3.687840526999024, 6.494943552003867, 12.879999444901905, 10.965421207411083, 7.591197615822848, 2.755745038292288, 39.57393344816843, 36.90645626546116, 79.95775547092472, 4.574182899735978, 3.6364971660848546, 9.290382037956784, 7.53655067472158, 138.1538000698965, 5.585294766885672, 3.6791413152917745, 2.749010393637176, 3.6733047130365097, 3.7411918438072016, 3.668660486348213, 11.371098152566281, 4.5775849090817236, 5.473172431899498, 5.5266963914325995, 3.675404396226085, 6.808525584260586, 10.534996957826475, 5.5426425452389685, 7.310964115111437, 3.7479208979105296, 7.445189849260146, 27.428020252044583, 408.8061099256945, 73.57160611134104, 14.351214907408544, 77.94695414834513, 76.99548743518658, 121.3170468616421, 16.862103364302918, 94.46112384651794, 52.12155771926942, 191.49231328202802, 45.69709680253871, 356.5159060689147, 67.8132721310206, 508.828069847511, 95.11182628374777, 572.2577388501712, 144.12752818643435, 933.6780691614684, 151.6431813934304, 1360.2050486032454, 297.4388866141693, 192.73030950833765, 707.2410668183218, 154.9906006401464, 331.5978546882472, 554.897969952773, 336.42963131145535, 251.72492751522807, 268.1043611907822, 525.5147949966467, 276.9964893765867, 435.6804915763389, 1108.9791179260374, 762.2242623635655, 548.5314503886958, 419.52208749345243, 527.0368711545843, 1128.4152716561077, 455.6224400156853, 960.6224042087231, 593.8540073404303, 443.2154865735635, 889.1891306064423, 635.6314567423365, 821.4456059008129, 628.1707673832794, 824.7923787341703, 693.2323973783047, 498.9571718002326, 384.2371451821891, 602.9504487625196, 433.58077859913226, 16.55867360120237, 5.594058852886972, 5.602927817305155, 3.691576894375809, 38.8933880332336, 36.99933352049683, 6.455134492141363, 4.390907343631401, 7.480725620561293, 15.848604290704685, 4.673842324720941, 2.7463215352059267, 21.60905826621314, 5.580404481242007, 3.739857089987646, 5.552259751476436, 5.539891470591092, 6.541285255137069, 63.579752335536334, 2.754946924297598, 6.185532045768508, 7.920481651066008, 1.83921509303798, 18.60814204852455, 14.236191860440258, 2.660252396260662, 73.5619663042061, 2.6527055813895495, 7.504054086503006, 7.9073879043504345, 22.820390400511474, 57.418360362441504, 26.149239687569843, 14.037435424015325, 12.416219105896054, 48.12670228038322, 41.78266122944135, 93.34929562475801, 192.85776872794736, 112.41783553654759, 104.18211147933438, 99.35984475241973, 85.98901599902689, 50.667829567668406, 206.02784381717993, 248.64195162045147, 30.873367793685066, 327.088098248238, 960.6224042087231, 933.6780691614684, 525.5147949966467, 443.2154865735635, 821.4456059008129, 889.1891306064423, 1360.2050486032454, 355.9549936996267, 693.2323973783047, 824.7923787341703, 377.3621780275135, 602.9504487625196, 411.9343462165857, 290.6660628844038, 455.6224400156853, 527.0368711545843, 548.5314503886958, 572.2577388501712, 1108.9791179260374, 487.4914213906193, 339.45862847538365, 635.6314567423365, 1128.4152716561077, 707.2410668183218, 372.12942539005627, 498.9571718002326, 593.8540073404303, 762.2242623635655, 530.3127445317438, 508.828069847511, 554.897969952773, 628.1707673832794, 25.56614920577243, 7.047949857090524, 5.273768949174638, 14.967097908578044, 10.209129500646783, 3.5005603762195636, 4.371982721817181, 30.641486488231372, 9.87771950111921, 21.79468790415084, 2.766399201109503, 6.155686593548908, 5.15650200346742, 7.52512260300376, 23.999455062379123, 2.7610776863672917, 2.650158919660503, 2.7607549077228826, 25.3553173884115, 5.28690281582951, 4.312989244218039, 20.694328572159666, 4.320379807314516, 54.88001705894889, 35.19206751498985, 7.832060044508911, 6.960465827488966, 22.50985207430339, 3.745513625195487, 6.726604460649467, 15.157719374306176, 98.0590622290081, 25.78552444788673, 19.008595485802743, 114.69591700708946, 11.398663117649859, 10.731289343863939, 125.168992202832, 44.739412720741875, 175.48625030355714, 166.01649871806083, 239.3789526258279, 109.51347928509514, 227.71675207454584, 259.49608863599144, 390.57508876103054, 707.2410668183218, 185.44242177840044, 179.62944334903872, 1128.4152716561077, 762.2242623635655, 443.2154865735635, 889.1891306064423, 525.5147949966467, 362.1937730123834, 693.2323973783047, 824.7923787341703, 554.897969952773, 408.8061099256945, 572.2577388501712, 960.6224042087231, 206.02784381717993, 348.3075874386722, 530.3127445317438, 635.6314567423365, 602.9504487625196, 933.6780691614684, 548.5314503886958, 1108.9791179260374, 493.93517898678215, 411.9343462165857, 527.0368711545843, 508.828069847511, 1360.2050486032454, 821.4456059008129, 500.7665329017002, 521.8248953791962, 498.9571718002326, 628.1707673832794, 593.8540073404303, 36.182586203167006, 9.77132989019366, 8.76279274319335, 12.416219105896054, 2.6807586097212606, 22.051172883465853, 2.6935441114061622, 10.772078970168478, 11.538978019677929, 6.11747713632954, 11.465365841591597, 10.255763793365839, 3.594706360917694, 2.67997573864658, 3.62325782019641, 3.5886612704967935, 4.5287039945764045, 2.700615548561725, 20.516579928646813, 3.619951660812692, 14.206083765508383, 3.5969885128625574, 6.107818893046605, 4.335367255003722, 1.7736943494126987, 2.700710098913281, 3.6258438929442507, 2.6907361046768554, 3.638485184953295, 22.50985207430339, 16.054311674517773, 21.06671897807739, 82.56412905777461, 57.68452547057366, 34.05504758169849, 89.75851240809064, 121.50159224153961, 37.58941330707023, 36.827094714718044, 192.73030950833765, 77.39514347239829, 960.6224042087231, 29.83461672929948, 239.26671749023623, 824.7923787341703, 1128.4152716561077, 346.0880406828208, 191.49231328202802, 530.3127445317438, 226.67733067998296, 821.4456059008129, 116.83164367613655, 384.2371451821891, 419.52208749345243, 762.2242623635655, 500.7665329017002, 572.2577388501712, 297.4388866141693, 378.8532211209495, 508.828069847511, 554.897969952773, 108.33756614469975, 298.11873256147305, 372.12942539005627, 443.2154865735635, 889.1891306064423, 593.8540073404303, 327.67414090017144, 933.6780691614684, 602.9504487625196, 1360.2050486032454, 1108.9791179260374, 521.8248953791962, 628.1707673832794, 390.57508876103054, 493.93517898678215, 470.1405678534758, 693.2323973783047, 707.2410668183218, 498.9571718002326, 525.5147949966467, 635.6314567423365, 18.608612029659536, 4.230001660642571, 4.267167984093323, 53.506621197747606, 5.1439461267882205, 5.31866685133919, 3.4264271492061362, 4.332987776455589, 2.596969356755918, 5.186054010955915, 7.046488941399421, 227.71675207454584, 2.6199159188305345, 2.6033169991312266, 3.4859257198658575, 1.7375290029336898, 3.5199106878649276, 10.49360147602502, 21.18828612358228, 2.6856296717265904, 3.627254375247225, 2.708845138967823, 1.7491664522050745, 7.926045125731032, 6.726604460649467, 2.7131302820265475, 1.7875853280559948, 2.6395826707524797, 5.2652944296416395, 3.4755239455319242, 4.331870602860418, 6.207894551354617, 8.366807605815373, 48.23957941817397, 100.58756708154546, 166.01649871806083, 80.46465994894038, 64.37373206932551, 288.44535192853357, 628.1707673832794, 10.552482642288622, 140.19852620478133, 20.495030029016057, 22.15050348412775, 1360.2050486032454, 261.08496016448544, 114.69591700708946, 171.23775479207447, 78.44004980148804, 172.5959696895388, 44.37603043849878, 346.0880406828208, 1108.9791179260374, 327.67414090017144, 824.7923787341703, 508.828069847511, 821.4456059008129, 181.29041170965496, 158.34864891040024, 276.53084873634253, 273.7079339758841, 1128.4152716561077, 889.1891306064423, 493.93517898678215, 635.6314567423365, 355.9549936996267, 347.34880957566793, 455.6224400156853, 933.6780691614684, 762.2242623635655, 286.96056507235556, 419.52208749345243, 602.9504487625196, 500.7665329017002, 707.2410668183218, 527.0368711545843, 525.5147949966467, 960.6224042087231, 530.3127445317438, 498.9571718002326, 470.1405678534758, 433.58077859913226, 593.8540073404303, 554.897969952773, 693.2323973783047, 521.8248953791962], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.4297, -8.8557, -9.0507, -9.1511, -8.6758, -9.3831, -9.4932, -9.4871, -7.1198, -8.5898, -9.805, -9.6446, -9.5203, -9.6752, -10.0151, -9.684, -8.2036, -9.6752, -9.4072, -9.3199, -9.4234, -9.2283, -9.8262, -10.0183, -10.2623, -10.2679, -7.94, -9.3001, -9.1973, -10.3017, -8.3753, -7.5866, -6.38, -7.7948, -5.7114, -5.6905, -6.9383, -6.9107, -6.427, -5.9438, -7.3519, -5.071, -7.3908, -7.0649, -7.7008, -8.1647, -5.3323, -5.4106, -5.5656, -5.664, -5.9927, -6.2058, -5.7562, -5.3868, -5.9935, -5.884, -6.1515, -5.4026, -5.9709, -6.0181, -6.2933, -6.2411, -6.0132, -5.2305, -5.9604, -5.8346, -5.9535, -5.4469, -5.9253, -5.7823, -6.0249, -6.1576, -6.1701, -5.906, -6.1174, -6.0162, -6.1162, -9.3149, -10.4696, -10.1888, -9.1768, -10.5862, -8.2379, -9.8864, -8.8335, -9.4719, -6.342, -9.8763, -10.6407, -10.6477, -8.8557, -9.6246, -10.6511, -9.5684, -10.1411, -8.259, -9.676, -9.6723, -9.366, -9.6304, -10.6967, -10.373, -10.6713, -7.0117, -8.6836, -9.5658, -10.3938, -7.6746, -8.2997, -7.9091, -8.1089, -8.3168, -8.3688, -7.9659, -6.0917, -4.9337, -4.793, -8.2338, -7.9621, -7.1473, -5.4633, -7.9757, -7.666, -7.3278, -8.3533, -5.8207, -6.0583, -6.3194, -5.9286, -6.3406, -6.5925, -6.3202, -5.7832, -5.8344, -6.1907, -5.2739, -6.9143, -6.6513, -6.1954, -6.2545, -6.1178, -5.4599, -6.2259, -6.2482, -5.9395, -5.9851, -6.3345, -6.3573, -5.5806, -6.1751, -6.0243, -5.6367, -5.7014, -5.7791, -5.9252, -5.8768, -6.019, -5.8616, -6.136, -6.0126, -6.1035, -6.046, -6.1908, -7.5215, -9.9265, -9.6709, -8.3583, -9.8338, -9.6943, -9.1513, -10.049, -9.5125, -9.3564, -9.3516, -9.4774, -9.1837, -9.8424, -10.5879, -9.3845, -7.7937, -9.6003, -9.4056, -9.5208, -8.8807, -7.9645, -8.6772, -10.0865, -10.6334, -8.6103, -10.1355, -10.3681, -9.9757, -8.182, -7.7606, -7.2079, -8.4699, -6.9777, -7.7522, -5.715, -6.5164, -7.4957, -6.9109, -7.0218, -6.4828, -6.5183, -7.6514, -5.9547, -6.5948, -5.471, -8.0328, -5.4285, -6.9087, -7.0618, -5.3701, -6.9865, -5.2225, -6.2238, -5.5039, -7.0177, -5.6697, -5.8773, -6.1698, -6.1922, -5.7045, -6.2105, -6.3132, -6.1621, -5.2189, -5.9397, -5.8957, -5.8628, -6.0558, -5.4963, -5.6197, -6.0602, -6.2566, -5.76, -6.0553, -6.0506, -6.2123, -6.1623, -6.1026, -6.178, -6.1913, -8.9093, -8.1338, -9.7088, -9.1536, -9.7504, -8.1116, -10.2719, -9.7929, -10.3026, -6.993, -7.6171, -9.6678, -9.8546, -10.3648, -9.6483, -9.7318, -10.4306, -7.4161, -10.446, -9.3312, -10.1775, -9.3283, -8.775, -9.9242, -9.2224, -8.2596, -8.3613, -7.251, -9.7365, -9.462, -7.1533, -6.9479, -7.5924, -7.7295, -6.3738, -7.3016, -6.4703, -6.0649, -7.7642, -6.9118, -5.8957, -7.9698, -7.2061, -7.5281, -5.9215, -5.5092, -6.667, -7.1376, -5.2784, -6.6303, -5.44, -6.1546, -5.8489, -5.4449, -5.7607, -6.2151, -6.0519, -6.7503, -5.927, -5.3592, -6.3979, -5.5424, -5.285, -6.108, -5.9489, -5.9766, -5.8295, -6.0402, -5.808, -5.6368, -6.0609, -5.9791, -6.1162, -6.0139, -5.8632, -6.136, -6.0203, -6.1465, -8.3189, -8.6615, -8.3233, -8.7332, -8.3896, -8.8648, -9.4323, -9.7454, -10.1572, -9.4685, -9.5501, -10.8704, -6.8276, -10.1996, -9.4051, -10.5066, -10.0169, -8.9411, -9.4981, -9.8236, -10.9442, -10.5213, -10.5379, -10.5582, -10.5508, -10.5482, -10.5596, -8.5036, -9.4348, -10.0349, -9.2496, -9.2659, -7.3054, -9.5708, -9.7311, -8.7593, -8.0033, -8.0224, -8.7459, -7.7766, -8.2924, -6.1627, -5.9999, -7.6041, -7.8349, -6.7897, -8.5224, -4.8216, -6.236, -5.786, -7.1915, -7.6849, -7.2182, -7.2984, -6.6194, -5.1161, -6.0229, -6.1389, -5.7982, -5.7594, -6.2789, -6.5062, -6.1731, -7.0544, -5.9483, -5.6565, -6.0808, -5.3745, -6.5535, -6.055, -5.6381, -5.8564, -6.2241, -6.0316, -5.8702, -6.1644, -5.6456, -5.9402, -5.8384, -5.7574, -5.95, -6.087, -5.7311, -5.7787, -6.0264, -6.1312, -6.1376, -9.9275, -9.3991, -8.7422, -8.9184, -9.2992, -10.3134, -7.6569, -7.7268, -7.0046, -9.8658, -10.0997, -9.1689, -9.3786, -6.473, -9.6873, -10.1074, -10.3991, -10.1099, -10.0943, -10.1143, -8.992, -9.9051, -9.7288, -9.7206, -10.1344, -9.5199, -9.0893, -9.7319, -9.457, -10.1279, -9.4482, -8.2171, -5.6824, -7.3033, -8.8444, -7.2788, -7.3532, -6.9481, -8.7253, -7.1846, -7.7162, -6.5751, -7.8429, -6.0557, -7.5017, -5.7486, -7.2122, -5.6732, -6.88, -5.2906, -6.8409, -5.0267, -6.2923, -6.6673, -5.6052, -6.8568, -6.2562, -5.8582, -6.2612, -6.4934, -6.4457, -5.9325, -6.4232, -6.0878, -5.4062, -5.6978, -5.9381, -6.1361, -5.9875, -5.4712, -6.0934, -5.6081, -5.9305, -6.1209, -5.6866, -5.9128, -5.7696, -5.9385, -5.7849, -5.9201, -6.1473, -6.2369, -6.1635, -6.232, -8.4767, -9.6122, -9.635, -10.0754, -7.733, -7.7876, -9.5917, -9.9799, -9.4484, -8.7045, -9.9312, -10.4655, -8.4104, -9.7686, -10.176, -9.7819, -9.7901, -9.6327, -7.3662, -10.5059, -9.6984, -9.4534, -10.9168, -8.6118, -8.8806, -10.5652, -7.2459, -10.5714, -9.5317, -9.4823, -8.4307, -7.5268, -8.3066, -8.9296, -9.0571, -7.8159, -7.9481, -7.232, -6.5777, -7.0739, -7.1534, -7.2195, -7.3485, -7.8156, -6.596, -6.4414, -8.2641, -6.2119, -5.2864, -5.3258, -5.8279, -5.9871, -5.499, -5.445, -5.1089, -6.2155, -5.6768, -5.551, -6.1766, -5.8137, -6.1314, -6.4105, -6.0633, -5.9918, -5.9658, -5.9358, -5.4779, -6.0604, -6.3195, -5.8996, -5.5159, -5.8443, -6.2827, -6.1091, -6.014, -5.9192, -6.1258, -6.1488, -6.1164, -6.1634, -7.9599, -9.2734, -9.5701, -8.5609, -8.9554, -10.052, -9.8301, -7.8886, -9.0261, -8.2423, -10.3067, -9.5142, -9.6929, -9.3205, -8.1709, -10.3436, -10.3874, -10.3515, -8.1374, -9.7076, -9.9157, -8.3559, -9.9314, -7.3998, -7.8543, -9.3754, -9.4989, -8.3272, -10.1247, -9.5397, -8.7275, -6.9323, -8.2299, -8.5284, -6.8808, -9.0316, -9.0932, -6.8661, -7.818, -6.6035, -6.6603, -6.3441, -7.0402, -6.4045, -6.3017, -5.9648, -5.4821, -6.6261, -6.672, -5.1548, -5.4978, -5.9434, -5.3913, -5.8221, -6.1532, -5.6631, -5.5508, -5.8566, -6.097, -5.8811, -5.5309, -6.6257, -6.2644, -5.9931, -5.879, -5.921, -5.6671, -6.0049, -5.5907, -6.0716, -6.1911, -6.0604, -6.0803, -5.5873, -5.8507, -6.1263, -6.1297, -6.1527, -6.0954, -6.1362, -7.6881, -9.0019, -9.1235, -8.7932, -10.3536, -8.2508, -10.3628, -8.9871, -8.9335, -9.5705, -8.9441, -9.0651, -10.1212, -10.4217, -10.1297, -10.1423, -9.9271, -10.4508, -8.4246, -10.1665, -8.8029, -10.1835, -9.6574, -10.0097, -10.9055, -10.4862, -10.1944, -10.5012, -10.2, -8.3778, -8.7226, -8.4579, -7.1213, -7.4966, -8.0304, -7.1469, -6.8908, -7.9843, -8.027, -6.5104, -7.3732, -5.1346, -8.2363, -6.3969, -5.3145, -5.054, -6.0927, -6.6178, -5.8034, -6.537, -5.5011, -7.0827, -6.1214, -6.0638, -5.5913, -5.9264, -5.832, -6.3528, -6.1724, -5.9435, -5.905, -7.155, -6.3961, -6.2324, -6.1086, -5.6255, -5.9125, -6.3528, -5.6214, -5.9492, -5.4278, -5.5874, -6.0496, -5.9374, -6.244, -6.1157, -6.1433, -5.9791, -5.9812, -6.1486, -6.1493, -6.1423, -8.1976, -9.7601, -9.7865, -7.3226, -9.6697, -9.6458, -10.1097, -9.9019, -10.4177, -9.7409, -9.4368, -5.9785, -10.4689, -10.4763, -10.1949, -10.8943, -10.1945, -9.1149, -8.4185, -10.4893, -10.1923, -10.4868, -10.9264, -9.4157, -9.6052, -10.5193, -10.9373, -10.5476, -9.8599, -10.2796, -10.0641, -9.7176, -9.4322, -7.7656, -7.0757, -6.6178, -7.3316, -7.5501, -6.171, -5.488, -9.2303, -6.9224, -8.6345, -8.569, -4.9289, -6.3888, -7.1192, -6.7769, -7.4581, -6.7748, -7.9631, -6.1992, -5.2084, -6.2483, -5.4662, -5.8928, -5.5086, -6.7705, -6.8836, -6.4351, -6.4473, -5.3295, -5.5287, -5.9933, -5.8138, -6.276, -6.3006, -6.1007, -5.57, -5.7202, -6.4463, -6.1854, -5.9473, -6.0739, -5.903, -6.089, -6.1119, -5.7836, -6.1079, -6.1721, -6.2036, -6.2356, -6.1517, -6.1843, -6.1499, -6.2268], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8465, 0.8315, 0.7997, 0.7784, 0.7756, 0.7651, 0.7559, 0.7545, 0.742, 0.7396, 0.7373, 0.7359, 0.734, 0.7314, 0.7274, 0.7201, 0.7174, 0.714, 0.7124, 0.7077, 0.7074, 0.7071, 0.7069, 0.7055, 0.6935, 0.6846, 0.6718, 0.6705, 0.6666, 0.6653, 0.6534, 0.6076, 0.5381, 0.5828, 0.425, 0.4222, 0.5013, 0.4966, 0.4575, 0.3681, 0.4991, 0.243, 0.4793, 0.4342, 0.5123, 0.565, 0.2026, 0.2035, 0.2183, 0.2066, 0.2556, 0.2557, 0.1823, 0.0993, 0.2101, 0.1839, 0.2224, 0.0551, 0.1519, 0.1541, 0.2189, 0.1875, 0.1228, -0.1206, 0.0914, 0.0478, 0.0644, -0.1503, -0.0019, -0.0933, -0.0185, 0.0955, 0.104, -0.2959, -0.0083, -0.2524, -0.0582, 0.886, 0.8213, 0.7857, 0.7732, 0.7089, 0.6934, 0.6888, 0.6876, 0.6871, 0.6868, 0.6805, 0.6758, 0.6605, 0.6486, 0.6464, 0.6453, 0.6453, 0.6259, 0.623, 0.6224, 0.6165, 0.6129, 0.6116, 0.6094, 0.607, 0.6037, 0.6034, 0.6025, 0.6004, 0.5988, 0.5909, 0.5933, 0.5821, 0.5816, 0.584, 0.5787, 0.5377, 0.4433, 0.3803, 0.3168, 0.5359, 0.5154, 0.4617, 0.3005, 0.5055, 0.4694, 0.4118, 0.5199, 0.2373, 0.2461, 0.2669, 0.2073, 0.2596, 0.2921, 0.2368, 0.1402, 0.1413, 0.2014, 0.0227, 0.3244, 0.2664, 0.1672, 0.1785, 0.1424, -0.0023, 0.1621, 0.1578, 0.0669, 0.0666, 0.1633, 0.1616, -0.0945, 0.099, 0.0436, -0.1017, -0.0872, -0.0901, -0.0547, -0.0929, -0.001, -0.2515, 0.0361, -0.1302, -0.0104, -0.1074, -0.068, 0.8524, 0.8431, 0.7741, 0.7724, 0.7566, 0.7411, 0.7387, 0.7371, 0.7363, 0.7329, 0.7322, 0.7276, 0.7098, 0.7085, 0.703, 0.6989, 0.698, 0.6777, 0.6714, 0.6654, 0.661, 0.6607, 0.6559, 0.6555, 0.6492, 0.6471, 0.6418, 0.638, 0.6348, 0.6324, 0.6133, 0.6016, 0.6142, 0.5518, 0.5554, 0.4215, 0.4605, 0.5314, 0.4605, 0.4682, 0.4064, 0.3997, 0.5215, 0.3056, 0.38, 0.218, 0.5516, 0.1816, 0.3702, 0.3916, 0.116, 0.3748, 0.0741, 0.2448, 0.1103, 0.3736, 0.1141, 0.1406, 0.1929, 0.1973, 0.0594, 0.1775, 0.2009, 0.1581, -0.1091, 0.0668, 0.0276, 0.0078, 0.0533, -0.1823, -0.162, 0.0077, 0.101, -0.2251, -0.0797, -0.112, 0.036, -0.0395, -0.2202, -0.1201, -0.1396, 1.1079, 1.1055, 1.0904, 1.0749, 1.0569, 1.0251, 1.0238, 1.0217, 1.01, 0.9833, 0.9581, 0.9573, 0.9565, 0.9382, 0.9256, 0.9078, 0.8876, 0.8796, 0.8751, 0.8725, 0.8718, 0.871, 0.8697, 0.867, 0.8666, 0.8645, 0.8638, 0.8603, 0.8529, 0.8469, 0.8404, 0.8029, 0.791, 0.7629, 0.6902, 0.7183, 0.6549, 0.5651, 0.7286, 0.615, 0.496, 0.727, 0.6307, 0.6666, 0.4361, 0.3732, 0.5357, 0.5888, 0.1792, 0.4356, 0.17, 0.324, 0.2119, 0.09, 0.1779, 0.3039, 0.2342, 0.444, 0.1661, -0.0625, 0.3033, -0.0563, -0.1752, 0.1661, 0.0691, 0.0814, -0.0456, 0.0689, -0.119, -0.3228, -0.0091, -0.1085, 0.0197, -0.0905, -0.2491, -0.0132, -0.2564, -0.0786, 0.9895, 0.9164, 0.9089, 0.8964, 0.8901, 0.8837, 0.8819, 0.8737, 0.8665, 0.8624, 0.8573, 0.8496, 0.8279, 0.826, 0.8246, 0.8208, 0.7964, 0.7947, 0.7893, 0.787, 0.7781, 0.7751, 0.7738, 0.7689, 0.7636, 0.7631, 0.7611, 0.7568, 0.7525, 0.7517, 0.7497, 0.7378, 0.6815, 0.741, 0.744, 0.7118, 0.6779, 0.678, 0.7042, 0.663, 0.6763, 0.5181, 0.4588, 0.5802, 0.601, 0.4975, 0.6686, 0.2883, 0.4299, 0.3504, 0.5227, 0.5804, 0.5196, 0.53, 0.4292, 0.1806, 0.3197, 0.3333, 0.2626, 0.2162, 0.3074, 0.344, 0.26, 0.4362, 0.1196, 0.0325, 0.1503, -0.0605, 0.2796, 0.1172, -0.0281, 0.026, 0.1499, 0.0775, 0.0004, 0.1097, -0.1106, -0.0017, -0.0745, -0.1433, -0.0266, 0.0358, -0.2735, -0.2926, -0.2425, -0.0794, -0.1312, 1.0926, 1.0551, 1.0273, 1.0121, 0.9991, 0.9981, 0.9902, 0.99, 0.9391, 0.939, 0.9344, 0.9274, 0.9269, 0.9239, 0.9178, 0.9151, 0.9149, 0.9142, 0.9115, 0.9111, 0.9021, 0.8989, 0.8965, 0.895, 0.8892, 0.8871, 0.8812, 0.8809, 0.8788, 0.8761, 0.8694, 0.7966, 0.6296, 0.7236, 0.8169, 0.6903, 0.6283, 0.5787, 0.7748, 0.5925, 0.6554, 0.4953, 0.6603, 0.3931, 0.6068, 0.3445, 0.558, 0.3024, 0.4745, 0.1955, 0.4628, 0.0831, 0.3377, 0.3966, 0.1586, 0.4251, 0.2651, 0.1483, 0.2457, 0.3034, 0.2881, 0.1284, 0.278, 0.1605, -0.0922, -0.0088, 0.0799, 0.15, 0.0705, -0.1746, 0.1101, -0.1505, 0.0081, 0.1103, -0.1517, -0.0422, -0.1554, -0.0561, -0.1748, -0.1362, -0.0346, 0.137, -0.2402, 0.0211, 1.0416, 0.9913, 0.9669, 0.9437, 0.9314, 0.9267, 0.8686, 0.8658, 0.8645, 0.8577, 0.8521, 0.8495, 0.8417, 0.8374, 0.8302, 0.8291, 0.8231, 0.8144, 0.8067, 0.8059, 0.8046, 0.8024, 0.7991, 0.7898, 0.7888, 0.7816, 0.7812, 0.7782, 0.778, 0.7751, 0.7669, 0.7481, 0.7548, 0.7539, 0.7492, 0.6355, 0.6446, 0.5569, 0.4856, 0.5291, 0.5256, 0.507, 0.5225, 0.5844, 0.4012, 0.3678, 0.6313, 0.3231, 0.1713, 0.1602, 0.2329, 0.2441, 0.1151, 0.0899, 0.001, 0.2349, 0.107, 0.059, 0.2154, 0.1097, 0.173, 0.2425, 0.1402, 0.0662, 0.0522, 0.0399, -0.1639, 0.0755, 0.1784, -0.029, -0.2193, -0.0804, 0.1232, 0.0036, -0.0755, -0.2302, -0.074, -0.0557, -0.11, -0.281, 1.1241, 1.0991, 1.0923, 1.0584, 1.0465, 1.0203, 1.0198, 1.0143, 1.0088, 1.0012, 1.0009, 0.9936, 0.9921, 0.9865, 0.9763, 0.966, 0.9632, 0.9582, 0.9549, 0.9524, 0.9478, 0.9394, 0.9305, 0.9203, 0.9101, 0.8916, 0.8861, 0.8841, 0.88, 0.8794, 0.8792, 0.8073, 0.8455, 0.8519, 0.7021, 0.8601, 0.8588, 0.6295, 0.7064, 0.5542, 0.5529, 0.5031, 0.5889, 0.4927, 0.4648, 0.3928, 0.2817, 0.4763, 0.4623, 0.1419, 0.1912, 0.2877, 0.1436, 0.2387, 0.2798, 0.1207, 0.0592, 0.1498, 0.215, 0.0945, -0.0733, 0.3715, 0.2078, 0.0586, -0.0085, 0.0024, -0.181, 0.013, -0.2767, 0.0512, 0.1132, -0.0024, 0.0128, -0.4774, -0.2366, -0.0172, -0.0618, -0.04, -0.213, -0.1976, 1.0486, 1.0438, 1.0312, 1.013, 0.9855, 0.981, 0.9716, 0.9612, 0.946, 0.9436, 0.9418, 0.9323, 0.9246, 0.9177, 0.9082, 0.9051, 0.8876, 0.881, 0.8793, 0.8723, 0.8687, 0.8616, 0.8582, 0.8487, 0.8467, 0.8455, 0.8427, 0.8342, 0.8336, 0.8334, 0.8267, 0.8196, 0.7904, 0.7736, 0.7668, 0.6812, 0.6345, 0.7142, 0.692, 0.5535, 0.6031, 0.3231, 0.6933, 0.4508, 0.2956, 0.2426, 0.3858, 0.4525, 0.2483, 0.3647, 0.113, 0.4817, 0.2526, 0.2223, 0.0977, 0.1826, 0.1437, 0.2772, 0.2157, 0.1497, 0.1014, 0.4849, 0.2316, 0.1736, 0.1225, -0.0906, 0.0261, 0.1804, -0.1353, -0.0258, -0.318, -0.2734, 0.0182, -0.055, 0.1135, 0.0071, 0.0289, -0.1952, -0.2174, -0.0359, -0.0884, -0.2717, 1.204, 1.1229, 1.0878, 1.0228, 1.0177, 1.0082, 0.984, 0.957, 0.9532, 0.9384, 0.9358, 0.9186, 0.8932, 0.8921, 0.8816, 0.8784, 0.8723, 0.8596, 0.8533, 0.848, 0.8445, 0.8419, 0.8396, 0.8394, 0.814, 0.8079, 0.8071, 0.807, 0.8042, 0.7998, 0.7951, 0.7818, 0.7688, 0.6835, 0.6385, 0.5954, 0.6058, 0.6104, 0.4897, 0.3944, 0.7385, 0.4598, 0.6706, 0.6583, 0.181, 0.3716, 0.4637, 0.4052, 0.5047, 0.3994, 0.5695, 0.2793, 0.1056, 0.2849, 0.1439, 0.2003, 0.1056, 0.3546, 0.3768, 0.2678, 0.2658, -0.0328, 0.0062, 0.1295, 0.0568, 0.1744, 0.1743, 0.1029, -0.0839, -0.0312, 0.2196, 0.1007, -0.0239, 0.0352, -0.1391, -0.0311, -0.051, -0.326, -0.0561, -0.0594, -0.0314, 0.0176, -0.2131, -0.1778, -0.366, -0.1589]}, \"token.table\": {\"Topic\": [1, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 9, 10, 1, 6, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 8, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 8, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 9, 1, 2, 3, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 1, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 6, 7, 4, 1, 2, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 10, 2, 5, 1, 2, 3, 6, 7, 9, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 8, 9, 1, 9, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 2, 3, 6, 1, 7, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 1, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 6, 8, 9, 10, 4, 1, 3, 4, 5, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 6, 1, 2, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 8, 1, 3, 5, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 9, 1, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 7, 1, 1, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 9, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 5, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 6, 7, 10, 2, 1, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 2, 4, 5, 7, 8, 1, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 4, 5, 8, 9, 6, 1, 2, 3, 10, 4, 1, 2, 3, 4, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 6, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 9, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 9, 1, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 9], \"Freq\": [0.2868678452616257, 0.23084715396154276, 0.23084715396154276, 0.23084715396154276, 0.11704838246037988, 0.24310048664848127, 0.17107071282670905, 0.09904093900493681, 0.0630260520940507, 0.04501860863860764, 0.0630260520940507, 0.09003721727721528, 0.0630260520940507, 0.04501860863860764, 0.12285488018507713, 0.12285488018507713, 0.36856464055523136, 0.12285488018507713, 0.12285488018507713, 0.12285488018507713, 0.12285488018507713, 0.12285488018507713, 0.26681459594238, 0.26681459594238, 0.16107963164489145, 0.08053981582244572, 0.08053981582244572, 0.16107963164489145, 0.08053981582244572, 0.16107963164489145, 0.08053981582244572, 0.16107963164489145, 0.15647967918046918, 0.11735975938535188, 0.23471951877070377, 0.07823983959023459, 0.07823983959023459, 0.07823983959023459, 0.039119919795117294, 0.07823983959023459, 0.07823983959023459, 0.039119919795117294, 0.0675002788009769, 0.0675002788009769, 0.1350005576019538, 0.1350005576019538, 0.20250083640293073, 0.0675002788009769, 0.1350005576019538, 0.0675002788009769, 0.0675002788009769, 0.0675002788009769, 0.13367925479237536, 0.11279187123106671, 0.12950177808011362, 0.12114682465559017, 0.11696934794332844, 0.07519458082071113, 0.07937205753297287, 0.12532430136785191, 0.07101710410844941, 0.02924233698583211, 0.04568772382375319, 0.13706317147125957, 0.09137544764750638, 0.31981406676627233, 0.04568772382375319, 0.09137544764750638, 0.04568772382375319, 0.04568772382375319, 0.09137544764750638, 0.09137544764750638, 0.17384562265915507, 0.13532892781850395, 0.11242710926460327, 0.13532892781850395, 0.068705455661702, 0.07495140617640218, 0.09889421648275287, 0.07078743916660206, 0.09577124122540279, 0.03643471133575106, 0.1763911894177799, 0.1287178949805421, 0.10011391831819941, 0.14301988331171342, 0.0762772710995805, 0.10011391831819941, 0.07150994165585671, 0.059591618046547265, 0.08581192998702807, 0.057207953324685375, 0.1621919791987342, 0.14417064817665262, 0.14236851507444448, 0.09190878821261605, 0.07929385649715895, 0.10091945372365685, 0.07388745719053448, 0.08830452200819974, 0.07568959029274264, 0.04325119445299579, 0.3554952215588466, 0.19602056853205507, 0.12364374322791166, 0.1387222484996082, 0.11158093901055442, 0.08142392846716134, 0.11158093901055442, 0.06031402108678618, 0.07539252635848272, 0.05126691792376825, 0.04523551581508963, 0.1055809104985619, 0.1055809104985619, 0.2111618209971238, 0.1055809104985619, 0.1055809104985619, 0.1055809104985619, 0.1055809104985619, 0.1055809104985619, 0.3685437120269428, 0.21116391892401265, 0.13312507932166015, 0.1377155992982691, 0.11705825940352875, 0.06656253966083008, 0.10099143948539735, 0.07344831962574352, 0.05279097973100316, 0.06426727967252559, 0.03901941980117625, 0.15602582322991276, 0.1448811215706333, 0.1448811215706333, 0.08915761327423587, 0.15602582322991276, 0.07801291161495638, 0.07801291161495638, 0.0668682099556769, 0.04457880663711793, 0.04457880663711793, 0.11745689065340313, 0.08809266799005235, 0.1761853359801047, 0.058728445326701566, 0.08809266799005235, 0.11745689065340313, 0.08809266799005235, 0.058728445326701566, 0.1468211133167539, 0.029364222663350783, 0.0775628978980845, 0.0775628978980845, 0.11634434684712675, 0.11634434684712675, 0.0775628978980845, 0.0775628978980845, 0.0775628978980845, 0.19390724474521123, 0.11634434684712675, 0.03878144894904225, 0.21456960402534292, 0.21456960402534292, 0.21456960402534292, 0.15789111292631472, 0.059209167347368016, 0.15789111292631472, 0.11841833469473603, 0.0986819455789467, 0.0986819455789467, 0.15789111292631472, 0.0986819455789467, 0.03947277823157868, 0.03947277823157868, 0.06497366813940537, 0.15160522565861256, 0.15160522565861256, 0.2382367831778197, 0.06497366813940537, 0.08663155751920718, 0.08663155751920718, 0.06497366813940537, 0.06497366813940537, 0.04331577875960359, 0.18229532988722663, 0.10937719793233598, 0.10937719793233598, 0.07291813195489065, 0.036459065977445325, 0.18229532988722663, 0.07291813195489065, 0.036459065977445325, 0.10937719793233598, 0.036459065977445325, 0.1194943890872991, 0.20911518090277342, 0.2389887781745982, 0.1194943890872991, 0.029873597271824776, 0.029873597271824776, 0.029873597271824776, 0.08962079181547433, 0.029873597271824776, 0.05974719454364955, 0.21073051202242116, 0.14048700801494743, 0.14048700801494743, 0.07024350400747371, 0.07024350400747371, 0.07024350400747371, 0.21073051202242116, 0.07024350400747371, 0.07024350400747371, 0.07024350400747371, 0.361658996406485, 0.25052250386341024, 0.1002090015453641, 0.11423826176171506, 0.10221318157627136, 0.07816302120538399, 0.08417556129810583, 0.08417556129810583, 0.07215048111266215, 0.06613794101994029, 0.04810032074177476, 0.17682500098781062, 0.08841250049390531, 0.09472767910061283, 0.15787946516768805, 0.08209732188719779, 0.09472767910061283, 0.06946696467378274, 0.07578214328049027, 0.08841250049390531, 0.07578214328049027, 0.28566854803971464, 0.14152877482978413, 0.14796190095841066, 0.19299378385879654, 0.1029300180580248, 0.0514650090290124, 0.06433126128626551, 0.08363063967214517, 0.09649689192939827, 0.06433126128626551, 0.0514650090290124, 0.34128722142333956, 0.08532180535583489, 0.17064361071166978, 0.08532180535583489, 0.08532180535583489, 0.08532180535583489, 0.08532180535583489, 0.08532180535583489, 0.08532180535583489, 0.07765264532627025, 0.17471845198410807, 0.13589212932097294, 0.2523710973103783, 0.03882632266313513, 0.07765264532627025, 0.09706580665783783, 0.0582394839947027, 0.03882632266313513, 0.03882632266313513, 0.29362918174569913, 0.14681459087284957, 0.14681459087284957, 0.14681459087284957, 0.14681459087284957, 0.14681459087284957, 0.26537338980658887, 0.13268669490329443, 0.13268669490329443, 0.26537338980658887, 0.13268669490329443, 0.13268669490329443, 0.21495423697503915, 0.10747711848751958, 0.10747711848751958, 0.05373855924375979, 0.10747711848751958, 0.05373855924375979, 0.05373855924375979, 0.10747711848751958, 0.05373855924375979, 0.16121567773127934, 0.24905073596450747, 0.24905073596450747, 0.12452536798225373, 0.12452536798225373, 0.12452536798225373, 0.12452536798225373, 0.26729449911939185, 0.26729449911939185, 0.09407582361849287, 0.09407582361849287, 0.07055686771386965, 0.2822274708554786, 0.09407582361849287, 0.07055686771386965, 0.09407582361849287, 0.047037911809246435, 0.07055686771386965, 0.07055686771386965, 0.10497341274390759, 0.09729243132362167, 0.1459386469854325, 0.17410224552648088, 0.0768098142028592, 0.09473210418352636, 0.06656850564247799, 0.11265439416419351, 0.0768098142028592, 0.048646215661810835, 0.1358163847197243, 0.10385958831508327, 0.11983798651740378, 0.11983798651740378, 0.08788119011276277, 0.08788119011276277, 0.07989199101160252, 0.14380558382088454, 0.07190279191044227, 0.04793519460696151, 0.3647198016440156, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.15722785890703428, 0.17412985564397057, 0.13059739173297794, 0.08706492782198529, 0.13059739173297794, 0.17412985564397057, 0.08706492782198529, 0.04353246391099264, 0.04353246391099264, 0.08706492782198529, 0.04353246391099264, 0.17416031974575852, 0.13932825579660682, 0.13932825579660682, 0.06966412789830341, 0.06966412789830341, 0.06966412789830341, 0.17416031974575852, 0.08708015987287926, 0.034832063949151705, 0.034832063949151705, 0.15934275018985705, 0.10622850012657137, 0.053114250063285684, 0.23901412528478558, 0.07967137509492853, 0.10622850012657137, 0.026557125031642842, 0.026557125031642842, 0.1327856251582142, 0.07967137509492853, 0.2314611317984067, 0.2314611317984067, 0.2314611317984067, 0.15287515541608124, 0.15287515541608124, 0.15287515541608124, 0.15287515541608124, 0.0788890980244402, 0.0788890980244402, 0.0788890980244402, 0.2629636600814674, 0.1314818300407337, 0.0788890980244402, 0.09203728102851358, 0.05259273201629348, 0.0788890980244402, 0.0788890980244402, 0.18135025016682965, 0.14837747740922425, 0.11775990270573354, 0.13424628908453623, 0.10127351632693084, 0.0800767338398988, 0.06123514940698144, 0.06123514940698144, 0.06359034746109611, 0.04945915913640808, 0.13650669833067108, 0.23888672207867442, 0.10238002374800333, 0.10238002374800333, 0.13650669833067108, 0.03412667458266777, 0.03412667458266777, 0.06825334916533554, 0.06825334916533554, 0.06825334916533554, 0.1725348670947886, 0.24154881393270405, 0.12077440696635203, 0.0862674335473943, 0.12077440696635203, 0.05176046012843658, 0.03450697341895772, 0.05176046012843658, 0.05176046012843658, 0.06901394683791544, 0.14496725465333227, 0.09664483643555484, 0.19328967287110968, 0.04832241821777742, 0.14496725465333227, 0.09664483643555484, 0.04832241821777742, 0.19328967287110968, 0.04832241821777742, 0.04832241821777742, 0.256561906428004, 0.11224583406225176, 0.07215803618287614, 0.08819315533462639, 0.07215803618287614, 0.1202633936381269, 0.09621071491050151, 0.07215803618287614, 0.05612291703112588, 0.064140476607001, 0.1415876670015816, 0.1887835560021088, 0.0943917780010544, 0.0943917780010544, 0.0943917780010544, 0.0943917780010544, 0.0943917780010544, 0.0943917780010544, 0.0471958890005272, 0.1415876670015816, 0.09795154424641656, 0.09795154424641656, 0.1959030884928331, 0.09795154424641656, 0.09795154424641656, 0.09795154424641656, 0.1959030884928331, 0.14830561089181887, 0.1853820136147736, 0.086511606353561, 0.12976740953034152, 0.07415280544590944, 0.086511606353561, 0.0926910068073868, 0.086511606353561, 0.05561460408443208, 0.04943520363060629, 0.13288820033321924, 0.13288820033321924, 0.13288820033321924, 0.13288820033321924, 0.13288820033321924, 0.2657764006664385, 0.09091559759359864, 0.08265054326690786, 0.23142152114734202, 0.09918065192028944, 0.10744570624698022, 0.08265054326690786, 0.10744570624698022, 0.08265054326690786, 0.07438548894021707, 0.04132527163345393, 0.20530799958610513, 0.20530799958610513, 0.20530799958610513, 0.11951989900006153, 0.3585596970001846, 0.11951989900006153, 0.11951989900006153, 0.11951989900006153, 0.11951989900006153, 0.18325503871408705, 0.1359634158201291, 0.11231760437315012, 0.1773435858523423, 0.07093743434093693, 0.10640615151140538, 0.05320307575570269, 0.03546871717046846, 0.05911452861744744, 0.07093743434093693, 0.1564347073026497, 0.1506408292544034, 0.1158775609649257, 0.09270204877194055, 0.10428980486843313, 0.08111429267544798, 0.052144902434216565, 0.09849592682018683, 0.0753204146272017, 0.0753204146272017, 0.271961116116472, 0.12087160716287644, 0.08309922992447755, 0.07554475447679777, 0.12087160716287644, 0.06043580358143822, 0.05288132813375844, 0.09065370537215733, 0.04532685268607867, 0.067990279029118, 0.18672842903792386, 0.11444645650711463, 0.10239946108531309, 0.08432896795261077, 0.10239946108531309, 0.0602349771090077, 0.07830547024171, 0.13251694963981694, 0.04818798168720616, 0.09035246566351154, 0.14271002966558927, 0.14271002966558927, 0.14271002966558927, 0.04757000988852976, 0.2378500494426488, 0.09514001977705952, 0.04757000988852976, 0.09514001977705952, 0.04757000988852976, 0.04757000988852976, 0.1906923895146087, 0.06934268709622135, 0.08667835887027668, 0.15602104596649805, 0.10401403064433203, 0.06934268709622135, 0.06934268709622135, 0.06934268709622135, 0.15602104596649805, 0.05200701532216601, 0.08524648342192932, 0.05683098894795288, 0.11366197789590576, 0.17049296684385865, 0.08524648342192932, 0.05683098894795288, 0.08524648342192932, 0.1989084613178351, 0.11366197789590576, 0.05683098894795288, 0.15012771744421044, 0.15012771744421044, 0.15012771744421044, 0.07506385872210522, 0.22519157616631566, 0.15012771744421044, 0.07506385872210522, 0.07506385872210522, 0.19392991592509098, 0.19392991592509098, 0.19392991592509098, 0.19392991592509098, 0.17689754881110692, 0.1371858541800421, 0.12274523795056398, 0.1516264704095202, 0.05776246491791246, 0.11552492983582492, 0.06498277303265151, 0.06859292709002104, 0.0505421568031734, 0.0505421568031734, 0.18124404534980185, 0.12769466831463314, 0.14417139971006965, 0.09062202267490092, 0.11945630261691485, 0.08650283982604179, 0.09474120552376006, 0.05766855988402786, 0.04531101133745046, 0.05354937703516873, 0.12533140149940658, 0.15666425187425823, 0.12010925977026463, 0.09399855112455494, 0.07310998420798717, 0.1409978266868324, 0.06788784247884523, 0.07833212593712911, 0.10966497631198076, 0.036554992103993586, 0.36276934926229853, 0.17095356444226156, 0.1196674951095831, 0.13676285155380927, 0.08547678222113078, 0.08547678222113078, 0.09117523436920617, 0.06838142577690463, 0.13106439940573386, 0.04558761718460309, 0.06838142577690463, 0.08156388826241767, 0.14953379514776574, 0.2310976834101834, 0.06796990688534806, 0.05437592550827845, 0.08156388826241767, 0.17672175790190497, 0.08156388826241767, 0.040781944131208836, 0.027187962754139226, 0.12844213113845784, 0.2140702185640964, 0.17125617485127712, 0.12844213113845784, 0.08562808742563856, 0.04281404371281928, 0.08562808742563856, 0.04281404371281928, 0.1070351092820482, 0.02140702185640964, 0.3792418056003209, 0.09481045140008022, 0.1422156771001203, 0.04740522570004011, 0.04740522570004011, 0.04740522570004011, 0.04740522570004011, 0.04740522570004011, 0.04740522570004011, 0.04740522570004011, 0.10364932821359302, 0.08291946257087442, 0.12437919385631162, 0.12437919385631162, 0.10364932821359302, 0.08291946257087442, 0.14510905949903022, 0.10364932821359302, 0.04145973128543721, 0.10364932821359302, 0.15980574622935234, 0.2556891939669637, 0.0958834477376114, 0.0958834477376114, 0.12784459698348186, 0.06392229849174093, 0.06392229849174093, 0.031961149245870464, 0.0958834477376114, 0.031961149245870464, 0.1678878761315777, 0.26862060181052433, 0.10073272567894663, 0.06715515045263108, 0.10073272567894663, 0.06715515045263108, 0.06715515045263108, 0.03357757522631554, 0.06715515045263108, 0.03357757522631554, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.12646401214866754, 0.2092014143908101, 0.19747892134304917, 0.11001108860206393, 0.08205745133432638, 0.08476264203765582, 0.07935226063099693, 0.07033495828656547, 0.057710735004361406, 0.05230035359770253, 0.05680900476991826, 0.14978759455235854, 0.14265485195462718, 0.13552210935689582, 0.12838936675916446, 0.10699113896597039, 0.07132742597731359, 0.07132742597731359, 0.07846016857504495, 0.035663712988656796, 0.07846016857504495, 0.19008348522326596, 0.22176406609381027, 0.19008348522326596, 0.06336116174108865, 0.06336116174108865, 0.06336116174108865, 0.06336116174108865, 0.06336116174108865, 0.031680580870544324, 0.031680580870544324, 0.1295615051370642, 0.1295615051370642, 0.0647807525685321, 0.09717112885279815, 0.1295615051370642, 0.09717112885279815, 0.16195188142133024, 0.09717112885279815, 0.03239037628426605, 0.0647807525685321, 0.14570429580139413, 0.14570429580139413, 0.12488939640119497, 0.09713619720092942, 0.06938299800066387, 0.13876599600132775, 0.08325959760079664, 0.07632129780073026, 0.07632129780073026, 0.04856809860046471, 0.3323305592825254, 0.1661652796412627, 0.06646611185650508, 0.06646611185650508, 0.09969916778475763, 0.06646611185650508, 0.06646611185650508, 0.03323305592825254, 0.03323305592825254, 0.03323305592825254, 0.27865544408474247, 0.27865544408474247, 0.15000514899976153, 0.10312853993733606, 0.14062982718727643, 0.08437789631236586, 0.20625707987467212, 0.08437789631236586, 0.08437789631236586, 0.06562725268739568, 0.03750128724994038, 0.05625193087491057, 0.25343079260268175, 0.25343079260268175, 0.2938683968464312, 0.1469341984232156, 0.1469341984232156, 0.1469341984232156, 0.1469341984232156, 0.1469341984232156, 0.36658558253569684, 0.09164639563392421, 0.09164639563392421, 0.09164639563392421, 0.18329279126784842, 0.09164639563392421, 0.09164639563392421, 0.17283724116349816, 0.09876413780771325, 0.11522482744233212, 0.17283724116349816, 0.07407310335578493, 0.09876413780771325, 0.08230344817309436, 0.04115172408654718, 0.13168551707695098, 0.02469103445192831, 0.1529681186830613, 0.11472608901229596, 0.11472608901229596, 0.07648405934153064, 0.07648405934153064, 0.07648405934153064, 0.1912101483538266, 0.07648405934153064, 0.03824202967076532, 0.03824202967076532, 0.0713855490852251, 0.2039587116720717, 0.18356284050486452, 0.16316696933765734, 0.030593806750810755, 0.040791742334414335, 0.05098967791801792, 0.17336490492126094, 0.040791742334414335, 0.040791742334414335, 0.17755783307759004, 0.15954471957696498, 0.15954471957696498, 0.07719905785982176, 0.07205245400250031, 0.07205245400250031, 0.08491896364580394, 0.0823456617171432, 0.06175924628785741, 0.05403934050187523, 0.16292352265306675, 0.16292352265306675, 0.10861568176871117, 0.08146176132653338, 0.08146176132653338, 0.10861568176871117, 0.08146176132653338, 0.054307840884355585, 0.13576960221088896, 0.054307840884355585, 0.22119563809011922, 0.17695651047209537, 0.08847825523604769, 0.044239127618023844, 0.044239127618023844, 0.16221013459942077, 0.05898550349069846, 0.044239127618023844, 0.1032246311087223, 0.044239127618023844, 0.34567908488489896, 0.11522636162829965, 0.057613180814149824, 0.06913581697697979, 0.046090544651319856, 0.10370372546546967, 0.11522636162829965, 0.057613180814149824, 0.046090544651319856, 0.034567908488489896, 0.14543222826165392, 0.13364042597016845, 0.08843851718614089, 0.13364042597016845, 0.07861201527656968, 0.121848623678683, 0.07861201527656968, 0.07664671489465544, 0.08057731565848392, 0.06288961222125575, 0.254273185689046, 0.2860344089531852, 0.1430172044765926, 0.1430172044765926, 0.1430172044765926, 0.17355255915070658, 0.17355255915070658, 0.34710511830141316, 0.09750614582668836, 0.09750614582668836, 0.09750614582668836, 0.19501229165337672, 0.09750614582668836, 0.09750614582668836, 0.09750614582668836, 0.09750614582668836, 0.19501229165337672, 0.09750614582668836, 0.26063852972285506, 0.13031926486142753, 0.13031926486142753, 0.13031926486142753, 0.26063852972285506, 0.13031926486142753, 0.13678086559514696, 0.13678086559514696, 0.13678086559514696, 0.2735617311902939, 0.13678086559514696, 0.1349035347181613, 0.11414914476152109, 0.10377194978320099, 0.22310969203388215, 0.0415087799132804, 0.12971493722900124, 0.057074572380760545, 0.03632018242412035, 0.11933774225068114, 0.0415087799132804, 0.2672347423290979, 0.13942682208474672, 0.13942682208474672, 0.08133231288276892, 0.10457011656356004, 0.04647560736158224, 0.0580945092019778, 0.03485670552118668, 0.0580945092019778, 0.04647560736158224, 0.22001501596894316, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.14191464831865636, 0.19059233424953742, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.09529616712476871, 0.26862981878141723, 0.13431490939070861, 0.13431490939070861, 0.26862981878141723, 0.13431490939070861, 0.13431490939070861, 0.13431490939070861, 0.180031748029572, 0.180031748029572, 0.180031748029572, 0.180031748029572, 0.180031748029572, 0.180031748029572, 0.180031748029572, 0.3401598531168907, 0.11338661770563022, 0.11338661770563022, 0.11338661770563022, 0.11338661770563022, 0.11338661770563022, 0.11338661770563022, 0.11338661770563022, 0.3014781338560295, 0.20098542257068636, 0.10049271128534318, 0.10049271128534318, 0.10049271128534318, 0.10049271128534318, 0.10049271128534318, 0.35549045653631534, 0.173073218321328, 0.043268304580332, 0.25960982748199196, 0.12980491374099598, 0.06490245687049799, 0.12980491374099598, 0.043268304580332, 0.021634152290166, 0.10817076145082999, 0.021634152290166, 0.2110625798555025, 0.2110625798555025, 0.2207713031588184, 0.282506759029651, 0.08520045113592649, 0.1973063078937245, 0.07174774832499073, 0.07174774832499073, 0.0672635140546788, 0.07174774832499073, 0.0672635140546788, 0.049326576973431124, 0.03138963989218344, 0.33321848664130843, 0.16660924332065422, 0.16660924332065422, 0.2726138697407933, 0.2726138697407933, 0.1045713128843472, 0.0697142085895648, 0.1045713128843472, 0.1394284171791296, 0.174285521473912, 0.1045713128843472, 0.0697142085895648, 0.1394284171791296, 0.0697142085895648, 0.0697142085895648, 0.142411617856309, 0.11867634821359081, 0.23735269642718163, 0.0712058089281545, 0.09494107857087265, 0.09494107857087265, 0.09494107857087265, 0.047470539285436326, 0.0712058089281545, 0.047470539285436326, 0.21253026960291305, 0.31879540440436954, 0.10626513480145652, 0.10626513480145652, 0.10626513480145652, 0.10626513480145652, 0.10626513480145652, 0.17643712564037436, 0.17643712564037436, 0.35287425128074873, 0.11688996718899973, 0.11688996718899973, 0.11688996718899973, 0.12987774132111082, 0.06493887066055541, 0.15585328958533298, 0.10390219305688865, 0.06493887066055541, 0.09091441892477757, 0.038963322396333246, 0.26739243900826937, 0.26739243900826937, 0.06968051182090297, 0.06968051182090297, 0.13936102364180594, 0.13936102364180594, 0.06968051182090297, 0.20904153546270893, 0.06968051182090297, 0.13936102364180594, 0.06968051182090297, 0.06968051182090297, 0.19282483327158328, 0.19282483327158328, 0.19282483327158328, 0.19282483327158328, 0.19282483327158328, 0.14271719980876765, 0.09133900787761129, 0.097047695869962, 0.13891140781386718, 0.11607665584446435, 0.09895059186741223, 0.10465927985976294, 0.097047695869962, 0.06279556791585776, 0.04757239993625588, 0.1882857381955186, 0.1748367568958387, 0.1210408316971191, 0.12552382546367907, 0.08965987533119933, 0.058278918965279564, 0.0537959251987196, 0.07172790026495947, 0.058278918965279564, 0.058278918965279564, 0.18641724217213362, 0.09942252915847127, 0.12427816144808908, 0.11185034530328017, 0.11185034530328017, 0.06213908072404454, 0.07456689686885346, 0.07456689686885346, 0.06213908072404454, 0.08699471301366236, 0.10378100407981755, 0.12323994234478335, 0.11675362958979475, 0.14918519336473773, 0.14918519336473773, 0.09729469132482896, 0.07783575305986316, 0.09080837856984036, 0.03891787652993158, 0.051890502039908776, 0.15095843059762604, 0.11123252780877707, 0.12712288892431667, 0.0913695764143526, 0.11123252780877707, 0.11917770836654687, 0.07547921529881302, 0.09534216669323749, 0.06753403474104322, 0.05164367362550364, 0.36221976721027255, 0.22774336184761587, 0.22774336184761587, 0.22774336184761587, 0.18922057654138033, 0.11697272004376238, 0.10321122356802565, 0.12041309416269658, 0.08256897885442051, 0.08256897885442051, 0.10665159768695982, 0.08256897885442051, 0.06536710825974958, 0.05504598590294701, 0.16121974951485668, 0.10747983300990446, 0.10747983300990446, 0.10747983300990446, 0.05373991650495223, 0.05373991650495223, 0.16121974951485668, 0.05373991650495223, 0.05373991650495223, 0.05373991650495223, 0.12933768876246843, 0.1583726801173083, 0.1583726801173083, 0.11877951008798121, 0.08446542939589775, 0.06862816138416693, 0.08974451873314136, 0.07126770605278873, 0.08446542939589775, 0.03695362536070527, 0.17250060084874266, 0.1035003605092456, 0.12075042059411988, 0.06900024033949707, 0.1552505407638684, 0.12075042059411988, 0.06900024033949707, 0.08625030042437133, 0.06900024033949707, 0.0517501802546228, 0.2749899021856314, 0.18413955150551273, 0.14321965117095434, 0.10229975083639596, 0.08183980066911677, 0.16367960133823353, 0.10229975083639596, 0.06137985050183758, 0.06137985050183758, 0.04091990033455838, 0.04091990033455838, 0.08788906908529838, 0.08788906908529838, 0.2636672072558951, 0.08788906908529838, 0.08788906908529838, 0.08788906908529838, 0.08788906908529838, 0.08788906908529838, 0.17791374748366218, 0.17791374748366218, 0.11860916498910813, 0.059304582494554066, 0.059304582494554066, 0.17791374748366218, 0.059304582494554066, 0.059304582494554066, 0.11860916498910813, 0.059304582494554066, 0.1729394405077664, 0.10456803379539364, 0.09250249143438669, 0.13272096597107655, 0.11261172870273162, 0.06837140671237277, 0.12065542361006959, 0.06837140671237277, 0.07239325416604175, 0.048262169444027836, 0.127255053789346, 0.18239891043139597, 0.13998055916828062, 0.08766459261043837, 0.08342275748412684, 0.10180404303147682, 0.07635303227360761, 0.10039009798937297, 0.05514385664204994, 0.04383229630521918, 0.19233292958577106, 0.1255225435191348, 0.12754710067266922, 0.11135064344439377, 0.09312962906258389, 0.0688349432201707, 0.07490861468077399, 0.07895772898784285, 0.0688349432201707, 0.05871215745249854, 0.15384013190797463, 0.03846003297699366, 0.30768026381594926, 0.07692006595398732, 0.07692006595398732, 0.11538009893098097, 0.05769004946549049, 0.03846003297699366, 0.11538009893098097, 0.01923001648849683, 0.25233265370988367, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.12616632685494183, 0.15109958667350387, 0.30219917334700774, 0.15109958667350387, 0.15109958667350387, 0.15109958667350387, 0.15634847219823472, 0.16429839451339923, 0.11659886062241234, 0.12454878293757682, 0.07949922315164477, 0.08214919725669961, 0.10334899009713822, 0.060949404416261, 0.06359937852131582, 0.04769953389098687, 0.15396592626143346, 0.15396592626143346, 0.15396592626143346, 0.15396592626143346, 0.3079318525228669, 0.15396592626143346, 0.07039237671031807, 0.14078475342063615, 0.14078475342063615, 0.07039237671031807, 0.07039237671031807, 0.07039237671031807, 0.14078475342063615, 0.07039237671031807, 0.14078475342063615, 0.07039237671031807, 0.12245505972667602, 0.10548109105169122, 0.15882784974450057, 0.13336689673202337, 0.08729469604277894, 0.07274558003564911, 0.08850712237670642, 0.08002013803921403, 0.09214440137848888, 0.058196464028519294, 0.10979865101673897, 0.11268808920139, 0.11268808920139, 0.15602966197115536, 0.1011303364627859, 0.0895725837241818, 0.07512539280092666, 0.07512539280092666, 0.1011303364627859, 0.06645707824697358, 0.06852794292739918, 0.06852794292739918, 0.06852794292739918, 0.2741117717095967, 0.06852794292739918, 0.13705588585479836, 0.06852794292739918, 0.06852794292739918, 0.13705588585479836, 0.06852794292739918, 0.28476515104935957, 0.09492171701645319, 0.09492171701645319, 0.18984343403290638, 0.09492171701645319, 0.09492171701645319, 0.09492171701645319, 0.23088501377599402, 0.23088501377599402, 0.11544250688799701, 0.11544250688799701, 0.11544250688799701, 0.11544250688799701, 0.11544250688799701, 0.11544250688799701, 0.10026904143733067, 0.10026904143733067, 0.20053808287466135, 0.08912803683318282, 0.055705023020739264, 0.14483305985392209, 0.08912803683318282, 0.06684602762488712, 0.13369205524977423, 0.03342301381244356, 0.07655396632702331, 0.13123537084632567, 0.2405981798849304, 0.09842652813474426, 0.06561768542316283, 0.10936280903860474, 0.09842652813474426, 0.05468140451930237, 0.09842652813474426, 0.021872561807720944, 0.045882740069406916, 0.09176548013881383, 0.09176548013881383, 0.13764822020822073, 0.13764822020822073, 0.045882740069406916, 0.09176548013881383, 0.22941370034703457, 0.09176548013881383, 0.045882740069406916, 0.25074726237314804, 0.12537363118657402, 0.25074726237314804, 0.12537363118657402, 0.12537363118657402, 0.17139925794744287, 0.13926189708229733, 0.11783698983886698, 0.07498717535200626, 0.07498717535200626, 0.08569962897372144, 0.14997435070401252, 0.06427472173029108, 0.07498717535200626, 0.04284981448686072, 0.08884998414908037, 0.17769996829816073, 0.04442499207454018, 0.04442499207454018, 0.08884998414908037, 0.08884998414908037, 0.04442499207454018, 0.17769996829816073, 0.17769996829816073, 0.04442499207454018, 0.17501275438623123, 0.13490566483938657, 0.15313616008795233, 0.12032126864053397, 0.060160634320266984, 0.09297552576768534, 0.08750637719311562, 0.07656808004397617, 0.05833758479541041, 0.04010708954684466, 0.23185775418767682, 0.23185775418767682, 0.18992290238706763, 0.18992290238706763, 0.18992290238706763, 0.18992290238706763, 0.12247435381320927, 0.08907225731869765, 0.13917540206046508, 0.1447424181428837, 0.08907225731869765, 0.09463927340111626, 0.07793822515386045, 0.12247435381320927, 0.08350524123627905, 0.05010314474176743, 0.10036640488462174, 0.10036640488462174, 0.25091601221155435, 0.10036640488462174, 0.10036640488462174, 0.05018320244231087, 0.10036640488462174, 0.10036640488462174, 0.05018320244231087, 0.05018320244231087, 0.2840981174458634, 0.21395672564964127, 0.21395672564964127, 0.21395672564964127, 0.21395672564964127, 0.3632528344311043, 0.26047806695524756, 0.26047806695524756, 0.08721919682426656, 0.08721919682426656, 0.08721919682426656, 0.08721919682426656, 0.08721919682426656, 0.08721919682426656, 0.08721919682426656, 0.1744383936485331, 0.08721919682426656, 0.23640652657523825, 0.23640652657523825, 0.36916100725530565, 0.3662407081318863, 0.26652259924368804, 0.13326129962184402, 0.13326129962184402, 0.13326129962184402, 0.13326129962184402, 0.13326129962184402, 0.16372456641411579, 0.16372456641411579, 0.16372456641411579, 0.16372456641411579, 0.16372456641411579, 0.16372456641411579, 0.09784924204864136, 0.13343078461178365, 0.19569848409728272, 0.09784924204864136, 0.07116308512628462, 0.06226769948549904, 0.14232617025256925, 0.08895385640785577, 0.06226769948549904, 0.04447692820392789, 0.29374935516486017, 0.14687467758243009, 0.14687467758243009, 0.14687467758243009, 0.1338484036500274, 0.1338484036500274, 0.1338484036500274, 0.1338484036500274, 0.2676968073000548, 0.1338484036500274, 0.1338484036500274, 0.18595288459647336, 0.12301498519459006, 0.16878800294141427, 0.11729335797623704, 0.06865952662023632, 0.08296359466611888, 0.08010278105694237, 0.05721627218353026, 0.06865952662023632, 0.051494644965177236, 0.2895234918451238, 0.15441252898406604, 0.11580939673804953, 0.09650783061504128, 0.07720626449203302, 0.03860313224601651, 0.07720626449203302, 0.05790469836902477, 0.05790469836902477, 0.03860313224601651, 0.18087384334820278, 0.11911204318052379, 0.1499929432643633, 0.10146581456118693, 0.08381958594185007, 0.05293868585801057, 0.07499647163218165, 0.07940802878701586, 0.10146581456118693, 0.05735024301284479, 0.35742893629717853, 0.14173585178258, 0.12681628843704526, 0.1193565067642779, 0.13427607010981263, 0.07086792589129, 0.11562661592789422, 0.08951738007320843, 0.07459781672767368, 0.07086792589129, 0.05967825338213895, 0.13362643928812323, 0.06681321964406162, 0.13362643928812323, 0.06681321964406162, 0.06681321964406162, 0.13362643928812323, 0.06681321964406162, 0.20043965893218482, 0.20468042963191385, 0.10234021481595693, 0.10234021481595693, 0.10234021481595693, 0.10234021481595693, 0.10234021481595693, 0.10234021481595693, 0.20468042963191385, 0.2757978637596507, 0.2757978637596507, 0.27483965143940425, 0.27483965143940425, 0.15836701998068622, 0.15116851907247322, 0.1943595245217513, 0.11517601453140817, 0.07198500908213011, 0.07918350999034311, 0.0647865081739171, 0.0647865081739171, 0.05758800726570409, 0.05038950635749107, 0.2929104408788913, 0.14645522043944564, 0.14645522043944564, 0.14645522043944564, 0.14645522043944564, 0.19143210808400063, 0.14038354592826713, 0.12124033511986705, 0.10635117115777812, 0.102097124311467, 0.07657284323360025, 0.07657284323360025, 0.06593772611782243, 0.07019177296413356, 0.048921538732577936, 0.12000472373745, 0.2400094474749, 0.12000472373745, 0.12000472373745, 0.12000472373745, 0.12000472373745, 0.12000472373745, 0.11932925975736167, 0.11932925975736167, 0.11932925975736167, 0.23865851951472333, 0.11932925975736167, 0.11932925975736167, 0.11932925975736167, 0.11932925975736167, 0.14813299824935655, 0.10456446935248698, 0.20041523292560004, 0.13506243958029568, 0.08713705779373915, 0.08278020490405219, 0.043568528896869574, 0.06970964623499132, 0.07406649912467828, 0.05228223467624349, 0.1866699515253277, 0.16615677003902796, 0.11897645262053853, 0.11487381632327859, 0.08410404409382896, 0.07179613520204911, 0.09025799853971889, 0.06564218075615919, 0.05948822631026927, 0.04307768112122947, 0.1205830751745972, 0.1205830751745972, 0.14355127996975858, 0.12919615197278272, 0.12632512637338755, 0.07464666558427446, 0.08325974238245998, 0.09474384478004066, 0.05742051198790343, 0.05167846078911309, 0.1714039187245785, 0.22853855829943798, 0.14283659893714873, 0.08570195936228925, 0.11426927914971899, 0.057134639574859494, 0.057134639574859494, 0.057134639574859494, 0.057134639574859494, 0.028567319787429747, 0.14247616744710992, 0.07123808372355496, 0.14247616744710992, 0.07123808372355496, 0.07123808372355496, 0.07123808372355496, 0.14247616744710992, 0.07123808372355496, 0.07123808372355496, 0.07123808372355496, 0.20932906128619613, 0.09303513834942051, 0.09303513834942051, 0.11629392293677564, 0.09303513834942051, 0.09303513834942051, 0.13955270752413076, 0.06977635376206538, 0.046517569174710255, 0.046517569174710255, 0.08772958632767953, 0.17545917265535907, 0.08772958632767953, 0.08772958632767953, 0.08772958632767953, 0.08772958632767953, 0.17545917265535907, 0.08772958632767953, 0.08772958632767953, 0.30189728291776785, 0.10978083015191557, 0.08233562261393668, 0.10978083015191557, 0.10978083015191557, 0.08233562261393668, 0.05489041507595779, 0.05489041507595779, 0.05489041507595779, 0.05489041507595779, 0.15255639831510293, 0.15255639831510293, 0.30511279663020585, 0.15255639831510293, 0.15255639831510293, 0.2186182804491093, 0.2186182804491093, 0.2186182804491093, 0.27088694848088357, 0.27088694848088357, 0.28855837008350216, 0.14427918504175108, 0.14427918504175108, 0.14427918504175108, 0.14427918504175108, 0.14427918504175108, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.1262549481274805, 0.26378426548647876, 0.2718025523628725, 0.27569061790209914, 0.27569061790209914, 0.37235215656412535, 0.16955939579390375, 0.11175505631870929, 0.1387304147404667, 0.08092607526527223, 0.08477969789695187, 0.07707245263359261, 0.1001941884236704, 0.11946230158206855, 0.06936520737023336, 0.0500970942118352, 0.37821506367024277, 0.1260716878900809, 0.1260716878900809, 0.1260716878900809, 0.1260716878900809, 0.1260716878900809, 0.33889349276264885, 0.11296449758754962, 0.11296449758754962, 0.11296449758754962, 0.16944674638132443, 0.05648224879377481, 0.05648224879377481, 0.05648224879377481, 0.05648224879377481, 0.07822844120570305, 0.039114220602851524, 0.07822844120570305, 0.11734266180855456, 0.07822844120570305, 0.07822844120570305, 0.07822844120570305, 0.23468532361710912, 0.11734266180855456, 0.07822844120570305, 0.12105219291290825, 0.2421043858258165, 0.12105219291290825, 0.12105219291290825, 0.12105219291290825, 0.12105219291290825, 0.12105219291290825, 0.12300376029350531, 0.12300376029350531, 0.12300376029350531, 0.12300376029350531, 0.24600752058701061, 0.12300376029350531, 0.12300376029350531, 0.12300376029350531, 0.3762490468336866, 0.12541634894456222, 0.12541634894456222, 0.12541634894456222, 0.12541634894456222, 0.12541634894456222, 0.1786678659566035, 0.23822382127547131, 0.23822382127547131, 0.05955595531886783, 0.05955595531886783, 0.05955595531886783, 0.05955595531886783, 0.05955595531886783, 0.35719037723756863, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.14866341641611078, 0.2156800822744944, 0.2156800822744944, 0.2156800822744944, 0.14343518188796878, 0.1251243076043983, 0.10376162094023274, 0.13122793236558847, 0.08545074665666225, 0.08545074665666225, 0.09155437141785241, 0.08545074665666225, 0.08239893427606718, 0.06713987237309177, 0.2092682829559227, 0.1379268228573127, 0.2092682829559227, 0.05707316807888801, 0.11414633615777602, 0.04756097339907334, 0.04756097339907334, 0.06658536275870268, 0.052317070738980676, 0.05707316807888801, 0.0659729854673981, 0.0659729854673981, 0.1319459709347962, 0.1319459709347962, 0.0659729854673981, 0.0659729854673981, 0.0659729854673981, 0.1979189564021943, 0.0659729854673981, 0.0659729854673981, 0.14317994717565885, 0.09545329811710591, 0.1272710641561412, 0.11136218113662356, 0.15908883019517653, 0.07954441509758826, 0.0636355320780706, 0.09545329811710591, 0.0636355320780706, 0.04772664905855296, 0.13514507133568404, 0.11129594109997511, 0.15104449149282334, 0.0953965209428358, 0.13514507133568404, 0.07154739070712685, 0.0953965209428358, 0.0794971007856965, 0.0794971007856965, 0.03974855039284825, 0.14179177118471425, 0.1382469769050964, 0.14267796975461872, 0.10545762981863123, 0.10723002695844015, 0.07266828273216605, 0.0664648927428348, 0.08773365842054194, 0.08773365842054194, 0.049627119914649985, 0.2056021938288697, 0.11711517369998907, 0.09369213895999126, 0.11451261428443377, 0.10410237662221251, 0.09889725779110188, 0.05725630714221688, 0.06766654480443814, 0.08848702012888064, 0.05205118831110626, 0.23434746504653425, 0.23434746504653425, 0.20192157347832299, 0.1163243847212078, 0.111934785297766, 0.1141295850094869, 0.08559718875711517, 0.09657118731571968, 0.09437638760399877, 0.05925959221646435, 0.06364919163990615, 0.05706479250474345, 0.12362489669237435, 0.08990901577627225, 0.07867038880423823, 0.21353391246864661, 0.11238626972034033, 0.07867038880423823, 0.08990901577627225, 0.07867038880423823, 0.05619313486017016, 0.07867038880423823, 0.35698019596987707, 0.35249985256208166, 0.08333522552081377, 0.08333522552081377, 0.08333522552081377, 0.12500283828122066, 0.12500283828122066, 0.08333522552081377, 0.04166761276040688, 0.20833806380203443, 0.12500283828122066, 0.08333522552081377, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.12768032858750925, 0.17179557047348884, 0.16760543460828178, 0.12570407595621136, 0.08799285316934795, 0.12570407595621136, 0.07961258143893385, 0.09218298903455499, 0.05447176624769158, 0.046091494517277495, 0.041901358652070446, 0.1798972737029817, 0.1291570170175253, 0.13376976762529408, 0.1083996392825659, 0.0830295109398377, 0.08764226154760646, 0.08072313563595332, 0.07611038502818455, 0.06919125911653141, 0.05074025668545637, 0.09946151350316797, 0.09946151350316797, 0.09946151350316797, 0.2983845405095039, 0.09946151350316797, 0.09946151350316797, 0.09946151350316797, 0.09946151350316797, 0.09946151350316797, 0.25710926737158063, 0.11583624198301495, 0.11583624198301495, 0.08274017284501069, 0.21512444939702777, 0.0937721958910121, 0.07170814979900926, 0.09928820741401281, 0.060676126753007834, 0.07170814979900926, 0.07170814979900926, 0.08247073319749997, 0.08247073319749997, 0.04123536659874998, 0.32988293278999986, 0.08247073319749997, 0.08247073319749997, 0.08247073319749997, 0.04123536659874998, 0.04123536659874998, 0.08247073319749997, 0.37164551301105586, 0.10513939633718031, 0.11565333597089833, 0.08411151706974425, 0.1366812152383344, 0.052569698168590154, 0.14719515487205242, 0.11565333597089833, 0.09462545670346227, 0.09462545670346227, 0.052569698168590154, 0.2670652604867906, 0.1335326302433953, 0.1335326302433953, 0.1335326302433953, 0.1335326302433953, 0.1335326302433953, 0.2056904889120013, 0.07713393334200049, 0.1285565555700008, 0.07713393334200049, 0.05142262222800033, 0.07713393334200049, 0.2056904889120013, 0.07713393334200049, 0.025711311114000164, 0.025711311114000164, 0.18510755770667725, 0.13883066828000795, 0.09255377885333863, 0.04627688942666931, 0.04627688942666931, 0.13883066828000795, 0.18510755770667725, 0.04627688942666931, 0.04627688942666931, 0.04627688942666931, 0.21845580581499313, 0.21845580581499313, 0.21845580581499313, 0.21845580581499313, 0.19197153634160324, 0.15357722907328258, 0.18237295952452307, 0.06719003771956113, 0.07678861453664129, 0.05759146090248097, 0.14397865225620243, 0.05759146090248097, 0.04799288408540081, 0.019197153634160323, 0.336722094606599, 0.08418052365164976, 0.08418052365164976, 0.08418052365164976, 0.1683610473032995, 0.08418052365164976, 0.12364297011867575, 0.09891437609494061, 0.07418578207120545, 0.2060716168644596, 0.049457188047470305, 0.15661442881698928, 0.08242864674578383, 0.049457188047470305, 0.09067151142036221, 0.06594291739662707, 0.14188523191520147, 0.14188523191520147, 0.14188523191520147, 0.14188523191520147, 0.14188523191520147, 0.28377046383040294, 0.14188523191520147, 0.36531853472328424, 0.0989164159058578, 0.13188855454114373, 0.1516718377223153, 0.10551084363291499, 0.052755421816457496, 0.13848298226820094, 0.08572756045174343, 0.07913313272468624, 0.0989164159058578, 0.052755421816457496, 0.17192777124902983, 0.11939428558960405, 0.10665889512671295, 0.1639681522097229, 0.09233158085596047, 0.08118811420093075, 0.06208502850659411, 0.06208502850659411, 0.06526887612231688, 0.0748204189694852, 0.10758512568502734, 0.10086105532971314, 0.10422309050737023, 0.19836007548176915, 0.06387866837548499, 0.12103326639565576, 0.08405087944142761, 0.08405087944142761, 0.09077494979674182, 0.047068492487199465, 0.14482127219050567, 0.17700377712172913, 0.11263876725928218, 0.10620226627303749, 0.12229351873864922, 0.08045626232805869, 0.07080151084869166, 0.05792850887620226, 0.07723801183493635, 0.051492007889957565, 0.1005543334851629, 0.13407244464688386, 0.2011086669703258, 0.06703622232344193, 0.033518111161720966, 0.1005543334851629, 0.1005543334851629, 0.06703622232344193, 0.13407244464688386, 0.033518111161720966, 0.15681597221783242, 0.09408958333069944, 0.12545277777426592, 0.08363518518284395, 0.13939197530473993, 0.09060478394808096, 0.08363518518284395, 0.09408958333069944, 0.06969598765236996, 0.06272638888713296, 0.1580911359047268, 0.10978551104494916, 0.09661124971955526, 0.10100267016135323, 0.07026272706876746, 0.0790455679523634, 0.0658713066269695, 0.12295977237034306, 0.0658713066269695, 0.12735119281214102, 0.27801034015651593, 0.27801034015651593, 0.16374896206591444, 0.11981631370676667, 0.1397856993245611, 0.11981631370676667, 0.09784998952719277, 0.07388672678583943, 0.07388672678583943, 0.07388672678583943, 0.08187448103295722, 0.051920402606265556, 0.17512420613293916, 0.13072652007106725, 0.15539190121655164, 0.11839382949832507, 0.07646268155100161, 0.07646268155100161, 0.08386229589464692, 0.07152960532190472, 0.06659652909280785, 0.04686422417642034, 0.27063215262853235, 0.09508697254516002, 0.14628765006947694, 0.07314382503473847, 0.08045820753821233, 0.11703012005558155, 0.08045820753821233, 0.04388629502084308, 0.05120067752431693, 0.04388629502084308, 0.1969400285053894, 0.21208926146734242, 0.1514923296195303, 0.09089539777171818, 0.07574616480976515, 0.06059693184781212, 0.04544769888585909, 0.06059693184781212, 0.04544769888585909, 0.07574616480976515, 0.2204700141550335, 0.2204700141550335, 0.2204700141550335, 0.2204700141550335, 0.41082346591346175, 0.20541173295673087, 0.2638267614744796, 0.08794225382482654, 0.08794225382482654, 0.17588450764965308, 0.08794225382482654, 0.08794225382482654, 0.08794225382482654, 0.13604718514766279, 0.09069812343177519, 0.09069812343177519, 0.13604718514766279, 0.045349061715887595, 0.09069812343177519, 0.13604718514766279, 0.09069812343177519, 0.18139624686355038, 0.045349061715887595, 0.173325574985004, 0.086662787492502, 0.086662787492502, 0.173325574985004, 0.086662787492502, 0.086662787492502, 0.086662787492502, 0.173325574985004, 0.13609438443458546, 0.09072958962305697, 0.09072958962305697, 0.09072958962305697, 0.22682397405764243, 0.09072958962305697, 0.09072958962305697, 0.09072958962305697, 0.04536479481152848, 0.04536479481152848, 0.31711543308643736, 0.0975739794112115, 0.0975739794112115, 0.0975739794112115, 0.07318048455840863, 0.0975739794112115, 0.07318048455840863, 0.04878698970560575, 0.04878698970560575, 0.024393494852802875, 0.17061898898759165, 0.23460110985793853, 0.08530949449379582, 0.06398212087034687, 0.10663686811724478, 0.06398212087034687, 0.08530949449379582, 0.08530949449379582, 0.04265474724689791, 0.06398212087034687, 0.14556628811466357, 0.18600136814651458, 0.11909969027563384, 0.09483864225652325, 0.11983487354894022, 0.09410345898321687, 0.08307570988362113, 0.047051729491608435, 0.04999246258483396, 0.061020211684429684, 0.1641925969259748, 0.15590004152567305, 0.1359979085649488, 0.1028276869637418, 0.08790108724319863, 0.0679989542824744, 0.09287662048337968, 0.07629150968277616, 0.0679989542824744, 0.04975533240181054, 0.0776397548988882, 0.1552795097977764, 0.1552795097977764, 0.0776397548988882, 0.0776397548988882, 0.2329192646966646, 0.0776397548988882, 0.0776397548988882, 0.0776397548988882, 0.14240470968079477, 0.14240470968079477, 0.04746823656026492, 0.04746823656026492, 0.09493647312052984, 0.09493647312052984, 0.04746823656026492, 0.14240470968079477, 0.14240470968079477, 0.04746823656026492, 0.12619407761811643, 0.12619407761811643, 0.06309703880905822, 0.06309703880905822, 0.12619407761811643, 0.06309703880905822, 0.18929111642717464, 0.06309703880905822, 0.06309703880905822, 0.06309703880905822, 0.1848109465943111, 0.14374184735113085, 0.12320729772954073, 0.08213819848636049, 0.16427639697272098, 0.08213819848636049, 0.06160364886477036, 0.041069099243180245, 0.06160364886477036, 0.041069099243180245, 0.17087715200594325, 0.10874000582196389, 0.09320571927596905, 0.20194572509793293, 0.09320571927596905, 0.0776714327299742, 0.031068573091989682, 0.062137146183979364, 0.062137146183979364, 0.09320571927596905, 0.3734126849942902, 0.12447089499809673, 0.12447089499809673, 0.12447089499809673, 0.12447089499809673, 0.12447089499809673, 0.2582518039805462, 0.08155320125701458, 0.054368800838009726, 0.09514540146651701, 0.054368800838009726, 0.1766986027235316, 0.09514540146651701, 0.08155320125701458, 0.054368800838009726, 0.04077660062850729, 0.25904355759827247, 0.25904355759827247, 0.22872917475400215, 0.18027750389001754, 0.09013875194500877, 0.15774281590376532, 0.11267343993126096, 0.06760406395875657, 0.06760406395875657, 0.09013875194500877, 0.06760406395875657, 0.06760406395875657, 0.09013875194500877, 0.33311286660404393, 0.07402508146756533, 0.11103762220134798, 0.07402508146756533, 0.03701254073378266, 0.14805016293513065, 0.11103762220134798, 0.03701254073378266, 0.07402508146756533, 0.03701254073378266, 0.12756489784948613, 0.15552432751512693, 0.12232250478717847, 0.07863589593461473, 0.11183771866256317, 0.11708011172487083, 0.08562575335102493, 0.08387828899692239, 0.08038336028871729, 0.036696751436153544, 0.12424288696553999, 0.1628962295770413, 0.11596002783450399, 0.08835049739771732, 0.11596002783450399, 0.08558954435403866, 0.09387240348507465, 0.09939430957243199, 0.06902382609196665, 0.04693620174253733, 0.16019295839342393, 0.11506818138119183, 0.12183689793302664, 0.06994340436895974, 0.1037869871281338, 0.097018270576299, 0.10604322597874541, 0.1015307482775222, 0.07896835977140616, 0.045124777012232095, 0.18961771166643607, 0.18961771166643607, 0.18961771166643607, 0.18961771166643607, 0.15269708536482055, 0.10179805690988036, 0.16167926685686881, 0.12275648039132632, 0.08982181492048268, 0.08982181492048268, 0.08982181492048268, 0.08682775442313324, 0.056887149449639025, 0.0538930889522896, 0.18001297285511242, 0.18001297285511242, 0.18001297285511242, 0.18001297285511242, 0.16960390552945315, 0.3392078110589063, 0.16960390552945315, 0.17515985443322293, 0.08757992721661147, 0.2627397816498344, 0.08757992721661147, 0.08757992721661147, 0.08757992721661147, 0.08757992721661147, 0.08757992721661147, 0.13054196967683993, 0.09790647725762994, 0.13054196967683993, 0.06527098483841996, 0.13054196967683993, 0.13054196967683993, 0.03263549241920998, 0.19581295451525987, 0.03263549241920998, 0.03263549241920998, 0.19790471684253333, 0.10994706491251852, 0.1264391246493963, 0.10994706491251852, 0.09070632855282777, 0.08795765193001481, 0.07971162206157592, 0.07421426881595, 0.06871691557032407, 0.04947617921063333, 0.1788155376166081, 0.11324984049051845, 0.09536828672885764, 0.19073657345771527, 0.07152621504664324, 0.09536828672885764, 0.04768414336442882, 0.07152621504664324, 0.07152621504664324, 0.05364466128498242, 0.20137455225392797, 0.1274323338481888, 0.13372528860612407, 0.10068727612696399, 0.0896746053005773, 0.08338165054264206, 0.08023517316367443, 0.07551545709522299, 0.05191687675296581, 0.05349011544244962, 0.09119576722909108, 0.09119576722909108, 0.09119576722909108, 0.18239153445818215, 0.09119576722909108, 0.27358730168727324, 0.09119576722909108, 0.09119576722909108, 0.09119576722909108, 0.1179061658770979, 0.1179061658770979, 0.1179061658770979, 0.1179061658770979, 0.2358123317541958, 0.1179061658770979, 0.1179061658770979, 0.37125807435837793, 0.17058721144050124, 0.34117442288100247, 0.17058721144050124, 0.17058721144050124, 0.1762692814293173, 0.1762692814293173, 0.3525385628586346, 0.20130700414423522, 0.12258359470235553, 0.1057142926790956, 0.12370821483723952, 0.08097264971164769, 0.07422492890234372, 0.09109423092560365, 0.08772037052095166, 0.06297872755350375, 0.05060790606977981, 0.12701546214279524, 0.15524112039674973, 0.21169243690465872, 0.11290263301581799, 0.049394901944420366, 0.07056414563488624, 0.08467697476186349, 0.07056414563488624, 0.08467697476186349, 0.03528207281744312, 0.14637695069256879, 0.09758463379504585, 0.09758463379504585, 0.1951692675900917, 0.09758463379504585, 0.09758463379504585, 0.048792316897522926, 0.048792316897522926, 0.09758463379504585, 0.09758463379504585, 0.14305515210730094, 0.10861594882220998, 0.14305515210730094, 0.18544186384279754, 0.07947508450405608, 0.08742259295446168, 0.07947508450405608, 0.06887840657018193, 0.058281728636307796, 0.04503588121896512, 0.19623796040438415, 0.09811898020219208, 0.19623796040438415, 0.09811898020219208, 0.19623796040438415, 0.09811898020219208, 0.09811898020219208, 0.09811898020219208, 0.09811898020219208, 0.09811898020219208, 0.15990505661309123, 0.17622189912463115, 0.13379810859462737, 0.09137431806462357, 0.08811094956231558, 0.0848475810600076, 0.07505747555308365, 0.0848475810600076, 0.06526737004615969, 0.0424237905300038, 0.1068932977061048, 0.1068932977061048, 0.3206798931183144, 0.2137865954122096, 0.1068932977061048, 0.1068932977061048, 0.1068932977061048, 0.2725790526872659, 0.19243856496884634, 0.12829237664589754, 0.08980466365212829, 0.06414618832294877, 0.06414618832294877, 0.1667800896396668, 0.05131695065835902, 0.08980466365212829, 0.1154631389813078, 0.05131695065835902, 0.18260735528506272, 0.12361113280835016, 0.1460858842280502, 0.0786616299689501, 0.09270834960626262, 0.0730429421140251, 0.10394572531611264, 0.06742425425910009, 0.07023359818656259, 0.06180556640417508, 0.052607779504113605, 0.052607779504113605, 0.052607779504113605, 0.10521555900822721, 0.15782333851234082, 0.10521555900822721, 0.10521555900822721, 0.15782333851234082, 0.10521555900822721, 0.052607779504113605, 0.15348735437050048, 0.1918591929631256, 0.0959295964815628, 0.13430143507418793, 0.05755775788893768, 0.17267327366681304, 0.01918591929631256, 0.05755775788893768, 0.05755775788893768, 0.05755775788893768, 0.0758074757448415, 0.12634579290806916, 0.151614951489683, 0.10107663432645533, 0.0758074757448415, 0.2274224272345245, 0.0758074757448415, 0.05053831716322767, 0.0758074757448415, 0.025269158581613833, 0.16166758050895477, 0.16166758050895477, 0.16166758050895477, 0.16166758050895477, 0.16166758050895477, 0.2877264020251057, 0.22081372533899418, 0.22081372533899418, 0.22081372533899418, 0.22081372533899418, 0.17198317475947214, 0.15854698923138838, 0.09942777290781983, 0.1182384326471371, 0.08061711316850258, 0.08599158737973607, 0.09405329869658634, 0.06449369053480206, 0.08330435027411932, 0.042995793689868035, 0.20558203365797748, 0.09344637893544432, 0.07475710314835544, 0.07475710314835544, 0.09344637893544432, 0.05606782736126659, 0.07475710314835544, 0.1495142062967109, 0.03737855157417772, 0.1495142062967109, 0.19440328015728664, 0.19440328015728664, 0.19440328015728664, 0.19440328015728664, 0.15833675815565998, 0.23750513723348995, 0.10555783877043998, 0.10555783877043998, 0.07916837907782999, 0.07916837907782999, 0.10555783877043998, 0.05277891938521999, 0.05277891938521999, 0.026389459692609994, 0.19538976464935134, 0.19538976464935134, 0.19538976464935134, 0.19538976464935134, 0.09769488232467567, 0.09769488232467567, 0.09769488232467567, 0.27027513872541015, 0.13513756936270507, 0.08108254161762304, 0.027027513872541015, 0.027027513872541015, 0.10811005549016406, 0.21622011098032812, 0.05405502774508203, 0.05405502774508203, 0.027027513872541015, 0.17847811583640405, 0.17847811583640405, 0.17847811583640405, 0.17847811583640405, 0.17847811583640405, 0.18665515870492605, 0.13999136902869452, 0.09332757935246302, 0.04666378967623151, 0.18665515870492605, 0.04666378967623151, 0.09332757935246302, 0.04666378967623151, 0.09332757935246302, 0.04666378967623151, 0.12820019544818936, 0.12820019544818936, 0.12820019544818936, 0.06410009772409468, 0.2564003908963787, 0.12820019544818936, 0.06410009772409468, 0.06410009772409468, 0.06410009772409468, 0.15753665868668495, 0.13375754039435514, 0.1486194893270613, 0.10997842210202535, 0.08025452423661308, 0.10997842210202535, 0.08322691402315431, 0.08025452423661308, 0.06539257530390696, 0.03269628765195348, 0.3614807290281663, 0.1907348279345931, 0.1318314251900864, 0.1374412730705156, 0.08975756608686733, 0.07292802244557971, 0.1290265012498718, 0.07573294638579431, 0.058903402744506686, 0.07292802244557971, 0.03926893516300446, 0.3773358618539391, 0.13024708487171574, 0.27051317627202504, 0.14527559466460602, 0.11521857507882548, 0.060114039171561114, 0.030057019585780557, 0.07013304570015463, 0.09017105875734167, 0.060114039171561114, 0.030057019585780557, 0.2722344259791499, 0.2722344259791499, 0.18270939065827016, 0.18270939065827016, 0.18270939065827016, 0.18270939065827016, 0.18270939065827016, 0.1827288376850091, 0.14300517731870277, 0.21450776597805418, 0.07150258865935138, 0.055613124512828856, 0.07150258865935138, 0.07944732073261265, 0.06355785658609013, 0.06355785658609013, 0.055613124512828856, 0.3629484780562913, 0.1552320327230596, 0.13696944063799377, 0.05478777625519751, 0.11870684855292793, 0.06391907229773043, 0.1004442564678621, 0.1004442564678621, 0.13696944063799377, 0.07305036834026335, 0.05478777625519751, 0.13796650183582762, 0.1481862427125556, 0.11752702008237167, 0.08686779745218776, 0.13796650183582762, 0.06642831569873181, 0.11241714964400769, 0.0715381861370958, 0.0715381861370958, 0.04598883394527587, 0.21910229896364677, 0.08764091958545871, 0.08764091958545871, 0.08764091958545871, 0.043820459792729353, 0.043820459792729353, 0.17528183917091741, 0.043820459792729353, 0.08764091958545871, 0.08764091958545871, 0.13610582553673573, 0.13610582553673573, 0.13610582553673573, 0.13610582553673573, 0.27221165107347145, 0.13610582553673573, 0.13610582553673573, 0.27213308853101903, 0.27213308853101903, 0.11251567951367934, 0.09644201101172514, 0.12858934801563351, 0.22503135902735869, 0.04822100550586257, 0.12858934801563351, 0.08036834250977096, 0.04822100550586257, 0.11251567951367934, 0.03214733700390838, 0.357416964826827, 0.2783444351977022, 0.14735881863407765, 0.09823921242271842, 0.09823921242271842, 0.08186601035226536, 0.06549280828181228, 0.08186601035226536, 0.04911960621135921, 0.04911960621135921, 0.04911960621135921, 0.15558683744528515, 0.1707660410984837, 0.11763882831228877, 0.1214336292255884, 0.07399861780934293, 0.09297262237584111, 0.08917782146254148, 0.07589601826599275, 0.05312721278619492, 0.04933241187289529, 0.19701811986414544, 0.1212419199163972, 0.11787408880760838, 0.13471324435155244, 0.0892475243829035, 0.08756360882850908, 0.07746011550214266, 0.062304875512593005, 0.07072445328456503, 0.040413973305465735, 0.1613776654522644, 0.1234064500517316, 0.1946024789277306, 0.0996744404263986, 0.07594243080106561, 0.0664496269509324, 0.0854352346511988, 0.0901816365762654, 0.0617032250258658, 0.037971215400532804, 0.14846586507631193, 0.1214720714260734, 0.10797517460095414, 0.22944724602702754, 0.0404906904753578, 0.09447827777583487, 0.05398758730047707, 0.05398758730047707, 0.09447827777583487, 0.0404906904753578, 0.34987947511524553, 0.18599774882570574, 0.2789966232385586, 0.09299887441285287, 0.09299887441285287, 0.09299887441285287, 0.09299887441285287, 0.09299887441285287, 0.09299887441285287, 0.11777032054735932, 0.23554064109471864, 0.11777032054735932, 0.11777032054735932, 0.11777032054735932, 0.11777032054735932, 0.09009151625791845, 0.045045758128959223, 0.09009151625791845, 0.27027454877375534, 0.09009151625791845, 0.09009151625791845, 0.13513727438687767, 0.09009151625791845, 0.045045758128959223, 0.3597888507051073, 0.20094412025647, 0.10595235431704782, 0.1132594132354649, 0.1461411783683418, 0.08037764810258799, 0.09499176593942217, 0.06576353026575381, 0.06576353026575381, 0.06211000080654527, 0.06576353026575381, 0.15076073720978456, 0.09877427610296229, 0.12476750665637341, 0.17675396776319569, 0.08837698388159784, 0.07797969166023339, 0.08837698388159784, 0.09877427610296229, 0.04158916888545781, 0.06238375332818671, 0.21267870766190303, 0.21267870766190303, 0.21267870766190303, 0.1332685065104423, 0.10661480520835384, 0.1332685065104423, 0.10661480520835384, 0.18657590911461922, 0.05330740260417692, 0.07996110390626539, 0.05330740260417692, 0.07996110390626539, 0.05330740260417692, 0.1236412143510178, 0.0618206071755089, 0.2472824287020356, 0.1236412143510178, 0.1236412143510178, 0.0618206071755089, 0.1236412143510178, 0.1236412143510178, 0.0618206071755089, 0.0618206071755089, 0.18093991946982735, 0.18093991946982735, 0.18093991946982735, 0.18093991946982735, 0.174733663824317, 0.12134282210022013, 0.16502623805629937, 0.07765940614414088, 0.09222054479616729, 0.05339084172409685, 0.12619653498422892, 0.11163539633220251, 0.04853712884008805, 0.03882970307207044, 0.17320052181629483, 0.10825032613518427, 0.08660026090814742, 0.2381507174974054, 0.08660026090814742, 0.08660026090814742, 0.021650065227036854, 0.04330013045407371, 0.06495019568111056, 0.08660026090814742, 0.1891466582298243, 0.1891466582298243, 0.1891466582298243, 0.1891466582298243, 0.1891466582298243, 0.08685971716976883, 0.11581295622635844, 0.17371943433953765, 0.09409802693391624, 0.05790647811317922, 0.2171492929244221, 0.09409802693391624, 0.07238309764147403, 0.06514478787732662, 0.021714929292442207, 0.2711613999246732, 0.2711613999246732, 0.36392196934939625, 0.3305281585403476, 0.11017605284678254, 0.2203521056935651, 0.11017605284678254, 0.11017605284678254, 0.11017605284678254, 0.10630776476252372, 0.10630776476252372, 0.3189232942875712, 0.10630776476252372, 0.10630776476252372, 0.10630776476252372, 0.10630776476252372, 0.3685779509464099, 0.18156351634694368, 0.18156351634694368, 0.18156351634694368, 0.18156351634694368, 0.18156351634694368, 0.18102525536090255, 0.1451973402373906, 0.11502646434390684, 0.11125510485722137, 0.08296990870708035, 0.07165583024702393, 0.07731286947705214, 0.08108422896373761, 0.08862694793710854, 0.047141993583568374, 0.15961941068315247, 0.13301617556929374, 0.13301617556929374, 0.10641294045543499, 0.10641294045543499, 0.07980970534157623, 0.07980970534157623, 0.053206470227717496, 0.13301617556929374, 0.026603235113858748, 0.26346303985437874, 0.13173151992718937, 0.13173151992718937, 0.26346303985437874, 0.13173151992718937, 0.13173151992718937, 0.09318544752236986, 0.09318544752236986, 0.09318544752236986, 0.09318544752236986, 0.09318544752236986, 0.09318544752236986, 0.09318544752236986, 0.18637089504473972, 0.09318544752236986, 0.09318544752236986, 0.11282835355805367, 0.05641417677902683, 0.11282835355805367, 0.11282835355805367, 0.1692425303370805, 0.11282835355805367, 0.05641417677902683, 0.11282835355805367, 0.11282835355805367, 0.05641417677902683, 0.09281201761657808, 0.1237493568221041, 0.09281201761657808, 0.2474987136442082, 0.09281201761657808, 0.08249957121473607, 0.06187467841105205, 0.06187467841105205, 0.07218712481289406, 0.07218712481289406, 0.3706799541605514, 0.1853399770802757, 0.19346323009346691, 0.1105504171962668, 0.08291281289720011, 0.13818802149533352, 0.0276376042990667, 0.08291281289720011, 0.1105504171962668, 0.0552752085981334, 0.19346323009346691, 0.0276376042990667, 0.27599471239002027, 0.27599471239002027, 0.14366854529343426, 0.14366854529343426, 0.14366854529343426, 0.14366854529343426, 0.14366854529343426, 0.11985797836470723, 0.11985797836470723, 0.23971595672941445, 0.11985797836470723, 0.11985797836470723, 0.11985797836470723, 0.11985797836470723, 0.15204122098282927, 0.15204122098282927, 0.15204122098282927, 0.07602061049141463, 0.22806183147424391, 0.15204122098282927, 0.07602061049141463, 0.07602061049141463, 0.07602061049141463, 0.2306609662297552, 0.2306609662297552, 0.21305557963956342, 0.1306740888455989, 0.11647038353629467, 0.10226667822699044, 0.07670000867024283, 0.09658519610326875, 0.06533704442279945, 0.08238149079396452, 0.0624963033609386, 0.053974080175356064, 0.18952885949180204, 0.09476442974590102, 0.09476442974590102, 0.18952885949180204, 0.09476442974590102, 0.09476442974590102, 0.09476442974590102, 0.09476442974590102, 0.09476442974590102, 0.1624514154193602, 0.1624514154193602, 0.1624514154193602, 0.1624514154193602, 0.1624514154193602, 0.3637672677828309, 0.18801704035066785, 0.18801704035066785, 0.18801704035066785, 0.18801704035066785, 0.2791787476663299, 0.13048521535616658, 0.26097043071233317, 0.13048521535616658, 0.13048521535616658, 0.13048521535616658, 0.13048521535616658, 0.10932941208008651, 0.14577254944011533, 0.14577254944011533, 0.14577254944011533, 0.054664706040043254, 0.054664706040043254, 0.054664706040043254, 0.1822156868001442, 0.036443137360028834, 0.054664706040043254, 0.12973156742425448, 0.23784120694446656, 0.1513534953282969, 0.10810963952021208, 0.08648771161616965, 0.043243855808084826, 0.06486578371212724, 0.043243855808084826, 0.08648771161616965, 0.043243855808084826, 0.13026464683452746, 0.22796313196042306, 0.13026464683452746, 0.0976984851258956, 0.0976984851258956, 0.032566161708631865, 0.0976984851258956, 0.032566161708631865, 0.0976984851258956, 0.032566161708631865, 0.09748213430092446, 0.09748213430092446, 0.04874106715046223, 0.1462232014513867, 0.09748213430092446, 0.09748213430092446, 0.09748213430092446, 0.04874106715046223, 0.1462232014513867, 0.04874106715046223, 0.3629833994914295, 0.13314251624855414, 0.13314251624855414, 0.13314251624855414, 0.2662850324971083, 0.13314251624855414, 0.13314251624855414, 0.13314251624855414, 0.18503099376472748, 0.1348082954571586, 0.16124129456640537, 0.12687839572438456, 0.07136909759496632, 0.08987219697143907, 0.05815259804034292, 0.06079589795126761, 0.06343919786219228, 0.04757939839664421, 0.18027678259445046, 0.1421413093533167, 0.10747269731592239, 0.10747269731592239, 0.07973780768600694, 0.09013839129722523, 0.07973780768600694, 0.05893664046357035, 0.07280408527852808, 0.08320466888974637, 0.12112369464274271, 0.24224738928548542, 0.18168554196411407, 0.12112369464274271, 0.060561847321371355, 0.060561847321371355, 0.12112369464274271, 0.060561847321371355, 0.060561847321371355, 0.060561847321371355, 0.19223353536031443, 0.07393597513858247, 0.059148780110865974, 0.26616951049889687, 0.07393597513858247, 0.08872317016629896, 0.029574390055432987, 0.059148780110865974, 0.10351036519401546, 0.059148780110865974, 0.18966871134010602, 0.1354776509572186, 0.08128659057433116, 0.054191060382887436, 0.054191060382887436, 0.24385977172299347, 0.054191060382887436, 0.054191060382887436, 0.08128659057433116, 0.054191060382887436, 0.2735776818787362, 0.11399070078280675, 0.1367888409393681, 0.07979349054796472, 0.06839442046968405, 0.06839442046968405, 0.09119256062624541, 0.07979349054796472, 0.056995350391403375, 0.045596280313122704, 0.18207560573065204, 0.12316879211191167, 0.1488735835091802, 0.10603226451373265, 0.06747507741782988, 0.10496123153884647, 0.09746400071464316, 0.06319094551828512, 0.05997784659362656, 0.047125450894992295, 0.19146698091032444, 0.11966686306895277, 0.11966686306895277, 0.09573349045516222, 0.07180011784137166, 0.07180011784137166, 0.16753360829653388, 0.04786674522758111, 0.07180011784137166, 0.04786674522758111, 0.21918589923826257, 0.21918589923826257, 0.06705497049605111, 0.08940662732806814, 0.08940662732806814, 0.1788132546561363, 0.13410994099210222, 0.06705497049605111, 0.08940662732806814, 0.15646159782411925, 0.08940662732806814, 0.06705497049605111, 0.11999531153059788, 0.1569169458477049, 0.11999531153059788, 0.11076490295132112, 0.08307367721349083, 0.06461286005493731, 0.08307367721349083, 0.10153449437204436, 0.11076490295132112, 0.03692163431710704, 0.3641234237072016, 0.3641234237072016, 0.12937158977528582, 0.15945800600209648, 0.13538887302064795, 0.15344072275673434, 0.08424196543506983, 0.09025924868043196, 0.08123332381238876, 0.06017283245362131, 0.06318147407630237, 0.04512962434021598, 0.16564023775340805, 0.17189081276297064, 0.12813678769603265, 0.11251035017212623, 0.0781321876195321, 0.09688391264821981, 0.06875632510518825, 0.07500690011475082, 0.05312988758128183, 0.050004600076500545, 0.16286678215895722, 0.08143339107947861, 0.08143339107947861, 0.2850168687781751, 0.08143339107947861, 0.08143339107947861, 0.08143339107947861, 0.08143339107947861, 0.040716695539739306, 0.040716695539739306, 0.1540678486891526, 0.0941525741989266, 0.10271189912610175, 0.1540678486891526, 0.0770339243445763, 0.10271189912610175, 0.10271189912610175, 0.0770339243445763, 0.11127122405327688, 0.042796624635875725, 0.1422561102951488, 0.0711280551475744, 0.0711280551475744, 0.266730206803404, 0.0711280551475744, 0.088910068934468, 0.0711280551475744, 0.0533460413606808, 0.12447409650825521, 0.0355640275737872, 0.19225406894244337, 0.13374196100343888, 0.10448590703393662, 0.1421008335661538, 0.07522985306443436, 0.0710504167830769, 0.07522985306443436, 0.045973799094932113, 0.10866534331529408, 0.045973799094932113, 0.21051691038658837, 0.21051691038658837, 0.21051691038658837, 0.17562884150635868, 0.15013433225543565, 0.11330893000410237, 0.09347986725338446, 0.14163616250512798, 0.07931625100287167, 0.08214897425297422, 0.062319911502256305, 0.05665446500205119, 0.05098901850184607, 0.17519500904708493, 0.11679667269805663, 0.12263650633295946, 0.12847633996786229, 0.07591783725373681, 0.09343733815844531, 0.06423816998393114, 0.06423816998393114, 0.07007800361883398, 0.07591783725373681, 0.33628583800197065, 0.16814291900098532, 0.16814291900098532, 0.34995864946771943, 0.14998227834330832, 0.09998818556220555, 0.04999409278110278, 0.09998818556220555, 0.04999409278110278, 0.04999409278110278, 0.04999409278110278, 0.04999409278110278, 0.04999409278110278, 0.18058280268284896, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.09029140134142448, 0.20917656111105282, 0.18302949097217122, 0.052294140277763204, 0.0784412104166448, 0.18302949097217122, 0.052294140277763204, 0.0784412104166448, 0.0784412104166448, 0.052294140277763204, 0.052294140277763204, 0.12457712548177961, 0.12457712548177961, 0.06228856274088981, 0.12457712548177961, 0.12457712548177961, 0.06228856274088981, 0.12457712548177961, 0.06228856274088981, 0.1868656882226694, 0.06228856274088981, 0.18609212763744645, 0.13956909572808485, 0.23261515954680806, 0.04652303190936161, 0.13956909572808485, 0.04652303190936161, 0.04652303190936161, 0.04652303190936161, 0.04652303190936161, 0.04652303190936161, 0.12255966697598379, 0.13021964616198278, 0.19915945883597366, 0.09957972941798683, 0.09957972941798683, 0.08425977104598885, 0.08425977104598885, 0.07659979185998987, 0.053619854301992904, 0.061279833487991896, 0.11460162198632673, 0.22920324397265346, 0.11460162198632673, 0.11460162198632673, 0.11460162198632673, 0.11460162198632673, 0.11460162198632673, 0.17885704086290408, 0.1556288537378516, 0.17885704086290408, 0.11381811691275713, 0.09523556721271516, 0.0487791929626102, 0.07897583622517843, 0.04645637425010495, 0.055747649100125946, 0.04645637425010495, 0.3609482054310378, 0.1141188693269925, 0.1141188693269925, 0.1141188693269925, 0.1141188693269925, 0.1141188693269925, 0.1141188693269925, 0.1141188693269925, 0.228237738653985, 0.12819431920531985, 0.2563886384106397, 0.2563886384106397, 0.12819431920531985, 0.12819431920531985, 0.13380062357989408, 0.13380062357989408, 0.12656815744044037, 0.11210322516153289, 0.10487075902207915, 0.09763829288262542, 0.08678959367344481, 0.07232466139453735, 0.06509219525508361, 0.06509219525508361, 0.3730287376019946, 0.17904158003062642, 0.17904158003062642, 0.17904158003062642, 0.17904158003062642, 0.17904158003062642, 0.21738983391668637, 0.43477966783337274, 0.11070870760123762, 0.0402577118549955, 0.11070870760123762, 0.2012885592749775, 0.10064427963748875, 0.11070870760123762, 0.14090199149248425, 0.09057985167373987, 0.030193283891246623, 0.060386567782493246, 0.16346607886139086, 0.16346607886139086, 0.16346607886139086, 0.16346607886139086, 0.16346607886139086, 0.16346607886139086, 0.16346607886139086, 0.16270563526958878, 0.14855731915918977, 0.13440900304879072, 0.08135281763479439, 0.15209439818678952, 0.0884269756899939, 0.04951910638639658, 0.08488989666239415, 0.0565932644415961, 0.03890786930359732, 0.1296292089496585, 0.09333303044375411, 0.1711105558135492, 0.12444404059167215, 0.0674071886538224, 0.07259235701180874, 0.1348143773076448, 0.07259235701180874, 0.0777775253697951, 0.057036851937849735, 0.11520548479121565, 0.11520548479121565, 0.11520548479121565, 0.05760274239560782, 0.17280822718682345, 0.05760274239560782, 0.05760274239560782, 0.11520548479121565, 0.05760274239560782, 0.05760274239560782, 0.26020273606973854, 0.18214191524881698, 0.08673424535657952, 0.12142794349921132, 0.06505068401743463, 0.07806082082092156, 0.06071397174960566, 0.04336712267828976, 0.052040547213947706, 0.04336712267828976, 0.16469734592490384, 0.1570370042539781, 0.1187352958993493, 0.09575427088657201, 0.11107495422842353, 0.07277324587379473, 0.07277324587379473, 0.07277324587379473, 0.0574525625319432, 0.07277324587379473, 0.10797322276469279, 0.089977685637244, 0.23394198265683439, 0.1259687598921416, 0.089977685637244, 0.089977685637244, 0.089977685637244, 0.0719821485097952, 0.053986611382346394, 0.0359910742548976, 0.1958389720573805, 0.09791948602869024, 0.04895974301434512, 0.09791948602869024, 0.24479871507172563, 0.09791948602869024, 0.04895974301434512, 0.04895974301434512, 0.04895974301434512, 0.04895974301434512, 0.12296504153872917, 0.08197669435915278, 0.28691843025703473, 0.12296504153872917, 0.04098834717957639, 0.04098834717957639, 0.12296504153872917, 0.04098834717957639, 0.08197669435915278, 0.04098834717957639, 0.228656489736491, 0.12924062463366884, 0.11929903812338662, 0.06959110557197552, 0.14912379765423325, 0.029824759530846654, 0.06959110557197552, 0.06959110557197552, 0.04970793255141109, 0.09941586510282217, 0.1610852104087031, 0.1610852104087031, 0.1610852104087031, 0.1610852104087031, 0.1610852104087031, 0.1610852104087031, 0.2372762971121712, 0.09295360093054128, 0.08806130614472332, 0.07093827439436046, 0.0635998322156335, 0.16144572793199277, 0.08316901135890536, 0.09295360093054128, 0.05626139003690657, 0.051369095251088605, 0.22231367937270324, 0.09527729115972995, 0.08469092547531552, 0.11645002252855884, 0.0529318284220722, 0.1587954852662166, 0.07410455979090108, 0.1058636568441444, 0.06351819410648664, 0.04234546273765776, 0.2582014031629359, 0.12910070158146794, 0.2582014031629359, 0.12910070158146794, 0.12910070158146794, 0.08718706176246789, 0.10462447411496147, 0.11334318029120825, 0.13949929881994863, 0.09590576793871468, 0.05231223705748073, 0.09590576793871468, 0.1569367111724422, 0.08718706176246789, 0.0784683555862211, 0.2048375126970718, 0.12261400407923312, 0.14857932259012954, 0.10818882712873511, 0.07068336705744027, 0.07501092014258967, 0.09232113248318728, 0.08510854400793828, 0.05625819010694225, 0.034620424681195236, 0.26698607989920414, 0.26698607989920414, 0.26698607989920414, 0.36217742258302227, 0.181173931695964, 0.181173931695964, 0.06039131056532133, 0.06039131056532133, 0.06039131056532133, 0.12078262113064266, 0.2415652422612853, 0.06039131056532133, 0.06039131056532133, 0.06039131056532133, 0.15491543998307458, 0.15491543998307458, 0.15491543998307458, 0.15491543998307458, 0.15491543998307458, 0.15491543998307458, 0.2673898964420866, 0.2673898964420866, 0.2673898964420866, 0.1614232174259686, 0.12611188861403796, 0.14124531524772252, 0.1059339864357919, 0.12358965084175721, 0.06557818207929975, 0.08827832202982658, 0.07062265762386126, 0.05801146876245747, 0.05296699321789595, 0.3052565354804671, 0.16066133446340372, 0.11246293412438262, 0.0642645337853615, 0.11246293412438262, 0.04819840033902112, 0.04819840033902112, 0.03213226689268075, 0.04819840033902112, 0.0642645337853615, 0.17876107962001897, 0.17876107962001897, 0.17876107962001897, 0.17876107962001897, 0.1606689504681112, 0.13272652429974405, 0.19559698317857016, 0.09081288504719329, 0.11875531121556045, 0.06287045887882613, 0.06287045887882613, 0.0698560654209179, 0.055884852336734334, 0.055884852336734334, 0.09171840907898766, 0.2109523408816716, 0.1375776136184815, 0.0978329696842535, 0.07948928786845598, 0.07643200756582305, 0.11311937119741812, 0.07643200756582305, 0.06726016665792428, 0.04891648484212675, 0.1336768718333193, 0.1336768718333193, 0.1336768718333193, 0.1336768718333193, 0.1336768718333193, 0.1495622819017854, 0.12332328507691076, 0.16530567999671017, 0.09970818793452359, 0.09314843872830493, 0.08527673968084254, 0.06559749206218657, 0.0918364888870612, 0.07609309079213643, 0.049854093967261794, 0.2146614805257216, 0.2146614805257216, 0.2146614805257216, 0.2146614805257216, 0.1431905027533806, 0.2147857541300709, 0.11634228348712174, 0.12529168990920803, 0.06264584495460401, 0.0715952513766903, 0.0715952513766903, 0.06264584495460401, 0.0715952513766903, 0.053696438532517723, 0.23762547970608086, 0.1000528335604551, 0.05002641678022755, 0.08754622936539821, 0.03751981258517066, 0.225118875511024, 0.08754622936539821, 0.08754622936539821, 0.06253302097528445, 0.025013208390113777, 0.19738422009389311, 0.14181002220337952, 0.13414461559779142, 0.1034829891754391, 0.10156663752404209, 0.0651559561474987, 0.07090501110168976, 0.07090501110168976, 0.07090501110168976, 0.04407608798213147, 0.1068601762147049, 0.1068601762147049, 0.3205805286441147, 0.1068601762147049, 0.1068601762147049, 0.1068601762147049, 0.1068601762147049, 0.20208276580646048, 0.12417133802565644, 0.1485186592071577, 0.08765035625340455, 0.07791142778080404, 0.07425932960357885, 0.0925198204897048, 0.05965093689467809, 0.07669406172172898, 0.055998838717452905, 0.17377808731386404, 0.3475561746277281, 0.17377808731386404, 0.17377808731386404, 0.17377808731386404, 0.17377808731386404, 0.18010684743884395, 0.18010684743884395, 0.18010684743884395, 0.18010684743884395, 0.11652342282418639, 0.17235756292744236, 0.13594399329488413, 0.1043855662800003, 0.0728271392651165, 0.0873925671181398, 0.09953042366232587, 0.08496499580930257, 0.0728271392651165, 0.05340656879441876, 0.18449025167735417, 0.12411162385567462, 0.13752909670493674, 0.09727667815715038, 0.08385920530788825, 0.08050483709557273, 0.09056794173251931, 0.06708736424631061, 0.08721357352020379, 0.05031552318473295, 0.17256030035155218, 0.12402771587767812, 0.16716779096556617, 0.08088764078979008, 0.08628015017577609, 0.08628015017577609, 0.04853258447387405, 0.12402771587767812, 0.05392509385986005, 0.05392509385986005, 0.3663719064509217, 0.07887891795486214, 0.07887891795486214, 0.15775783590972428, 0.1183183769322932, 0.07887891795486214, 0.1183183769322932, 0.07887891795486214, 0.19719729488715534, 0.1183183769322932, 0.03943945897743107, 0.18853549337497855, 0.15907682253513816, 0.12667228461131372, 0.11488881627537756, 0.053025607511712716, 0.07953841126756908, 0.10015948085545735, 0.058917341679680796, 0.06480907584764888, 0.055971474595696756, 0.07864138843468525, 0.1572827768693705, 0.22019588761711872, 0.07864138843468525, 0.04718483306081116, 0.07864138843468525, 0.18873933224324463, 0.0629131107477482, 0.04718483306081116, 0.04718483306081116, 0.10389242900688075, 0.12467091480825691, 0.14544940060963307, 0.12467091480825691, 0.06233545740412846, 0.08311394320550461, 0.16622788641100922, 0.10389242900688075, 0.06233545740412846, 0.041556971602752305, 0.18050895135916853, 0.18050895135916853, 0.18050895135916853, 0.18050895135916853, 0.18050895135916853, 0.19779797195709165, 0.09889898597854582, 0.24724746494636457, 0.04944949298927291, 0.17307322546245518, 0.04944949298927291, 0.07417423948390937, 0.04944949298927291, 0.024724746494636456, 0.04944949298927291, 0.14363761392506733, 0.23939602320844555, 0.09575840928337823, 0.09575840928337823, 0.14363761392506733, 0.04787920464168911, 0.04787920464168911, 0.04787920464168911, 0.04787920464168911, 0.04787920464168911, 0.40320897543687345, 0.20160448771843673, 0.3628782728824912, 0.15298307472227476, 0.12748589560189563, 0.10198871648151651, 0.12748589560189563, 0.08924012692132695, 0.07649153736113738, 0.10198871648151651, 0.08924012692132695, 0.05099435824075826, 0.08924012692132695, 0.1826559314102591, 0.3653118628205182, 0.1826559314102591, 0.16156911331554638, 0.2585105813048742, 0.19388293597865566, 0.06462764532621855, 0.06462764532621855, 0.032313822663109275, 0.09694146798932783, 0.06462764532621855, 0.06462764532621855, 0.032313822663109275, 0.33426582566547175, 0.11142194188849058, 0.11142194188849058, 0.11142194188849058, 0.11142194188849058, 0.11142194188849058, 0.11142194188849058, 0.2784977726498304, 0.0928325908832768, 0.0928325908832768, 0.0928325908832768, 0.0928325908832768, 0.0928325908832768, 0.0928325908832768, 0.0928325908832768, 0.1856651817665536, 0.0928325908832768, 0.18041935626156919, 0.18041935626156919, 0.18041935626156919, 0.18041935626156919, 0.18041935626156919, 0.1698580745737296, 0.10652116541064398, 0.14106857040869067, 0.12667381832617122, 0.07197376041259729, 0.08636851249511673, 0.08924746291162063, 0.08348956207861284, 0.06333690916308561, 0.06045795874658172, 0.13175163317328484, 0.2635032663465697, 0.13175163317328484, 0.13175163317328484, 0.13175163317328484, 0.10123794261283625, 0.10123794261283625, 0.10123794261283625, 0.10123794261283625, 0.10123794261283625, 0.10123794261283625, 0.10123794261283625, 0.2024758852256725, 0.10123794261283625, 0.3686658529582333, 0.17919847985238416, 0.17919847985238416, 0.17919847985238416, 0.17919847985238416, 0.2032081285780377, 0.15240609643352826, 0.12252254811322862, 0.09263899979292894, 0.07769722563277912, 0.08068558046480909, 0.08068558046480909, 0.06275545147262929, 0.08068558046480909, 0.04183696764841952, 0.3575002067360895, 0.14300008269443581, 0.07150004134721791, 0.07150004134721791, 0.14300008269443581, 0.07150004134721791, 0.07150004134721791, 0.07150004134721791, 0.27818683908988595, 0.27818683908988595, 0.23078763467410615, 0.23078763467410615, 0.23078763467410615, 0.23078763467410615, 0.15745336562447348, 0.12111797355728729, 0.09689437884582983, 0.14534156826874475, 0.024223594711457457, 0.08478258149010111, 0.09689437884582983, 0.08478258149010111, 0.15745336562447348, 0.03633539206718619, 0.2762467827472305, 0.2762467827472305, 0.2762467827472305, 0.25243521476673036, 0.09645898450436038, 0.2011272442856876, 0.10261594096208551, 0.12724376679298605, 0.07388347749270156, 0.05336028930028446, 0.02873246346938394, 0.034889419927109075, 0.02873246346938394, 0.15323608864043275, 0.2407995678635372, 0.197017828251985, 0.10945434902888054, 0.02189086980577611, 0.06567260941732833, 0.04378173961155222, 0.06567260941732833, 0.04378173961155222, 0.06567260941732833, 0.24429826070862184, 0.09161184776573318, 0.1628655071390812, 0.0814327535695406, 0.15268641294288865, 0.07125365937334803, 0.06107456517715546, 0.0407163767847703, 0.0407163767847703, 0.06107456517715546, 0.23075294028587282, 0.0769176467619576, 0.24998235197636223, 0.0769176467619576, 0.11537647014293641, 0.0384588233809788, 0.057688235071468204, 0.0384588233809788, 0.0384588233809788, 0.057688235071468204, 0.29184919347597643, 0.1162863662525505, 0.07752424416836699, 0.103365658891156, 0.29717626931207347, 0.038762122084183497, 0.103365658891156, 0.06460353680697249, 0.051682829445578, 0.12920707361394498, 0.025841414722789, 0.1914509086325721, 0.2871763629488582, 0.09572545431628605, 0.09572545431628605, 0.09572545431628605, 0.09572545431628605, 0.09572545431628605, 0.09572545431628605, 0.09572545431628605, 0.36440081245138695, 0.09110020311284674, 0.09110020311284674, 0.09110020311284674, 0.09110020311284674, 0.09110020311284674, 0.09110020311284674, 0.09110020311284674, 0.13129936954040083, 0.10941614128366738, 0.15318259779713433, 0.06564968477020042, 0.0875329130269339, 0.1750658260538678, 0.10941614128366738, 0.06564968477020042, 0.06564968477020042, 0.04376645651346695, 0.2129161372605984, 0.15484809982588973, 0.09678006239118109, 0.10968407071000523, 0.09032805823176902, 0.13549208734765353, 0.04516402911588451, 0.06452004159412073, 0.058068037434708654, 0.03226002079706036, 0.2720789040321121, 0.16974743033609124, 0.18280492497732903, 0.15668993569485345, 0.05222997856495115, 0.18280492497732903, 0.06528747320618894, 0.026114989282475576, 0.05222997856495115, 0.039172483923713364, 0.07834496784742673, 0.21527640002626375, 0.10763820001313187, 0.10763820001313187, 0.21527640002626375, 0.10763820001313187, 0.10763820001313187, 0.10763820001313187], \"Term\": [\"accept\", \"accepted\", \"accepted\", \"accepted\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"aggregates\", \"aggregates\", \"aggressive\", \"aggressive\", \"aggressive\", \"aggressive\", \"aggressive\", \"aggressive\", \"aggressive\", \"aggressive\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"agp\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"aic\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"alg\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alto\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"animals\", \"animals\", \"animals\", \"animals\", \"animals\", \"animals\", \"animals\", \"animals\", \"annealed\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"ascent\", \"assad\", \"assad\", \"assad\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"auction\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"axes\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bags\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"barlow\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"bde\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bethe\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bidder\", \"bijk\", \"bijk\", \"bijk\", \"bijk\", \"bijk\", \"bijk\", \"bivariate\", \"bivariate\", \"bivariate\", \"bivariate\", \"bivariate\", \"bivariate\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bkm\", \"bmi\", \"bmi\", \"bmi\", \"bmi\", \"bmi\", \"bmi\", \"bonf\", \"bonf\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bounds\", \"bourlard\", \"braun\", \"braun\", \"braun\", \"braun\", \"braun\", \"braun\", \"braun\", \"braun\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"brownian\", \"bruck\", \"bruck\", \"bruck\", \"cached\", \"cached\", \"cached\", \"cached\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"canonical\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"centers\", \"chiang\", \"chiang\", \"chiang\", \"chiang\", \"chiang\", \"chiang\", \"chiang\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"chuang\", \"chuang\", \"chuang\", \"chuang\", \"chuang\", \"chuang\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"citeseer\", \"citeseer\", \"citeseer\", \"cjk\", \"cjk\", \"cjk\", \"cjk\", \"cjk\", \"cjk\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"coefficients\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"collapsed\", \"colliculus\", \"colliculus\", \"colliculus\", \"colliculus\", \"colliculus\", \"colliculus\", \"colliculus\", \"colliculus\", \"compo\", \"compo\", \"compo\", \"compo\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convolve\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpe\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"csp\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cues\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cursor\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"dd\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decorrelated\", \"decorrelated\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"degraded\", \"degraded\", \"delgado\", \"delgado\", \"delgado\", \"delgado\", \"delgado\", \"delgado\", \"demers\", \"demers\", \"demers\", \"demers\", \"demers\", \"demers\", \"demers\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"despot\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"display\", \"display\", \"display\", \"display\", \"display\", \"display\", \"display\", \"display\", \"display\", \"display\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"displays\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distances\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"djk\", \"dkv\", \"dkv\", \"dkv\", \"dkv\", \"dna\", \"dna\", \"dna\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dnf\", \"dof\", \"dof\", \"dof\", \"dof\", \"dof\", \"dof\", \"dts\", \"dts\", \"dts\", \"dts\", \"dts\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"eav\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentric\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"eccentricity\", \"edist\", \"edist\", \"edist\", \"edist\", \"edist\", \"edist\", \"edist\", \"eemp\", \"eemp\", \"eemp\", \"eemp\", \"eemp\", \"eemp\", \"eemp\", \"ejk\", \"ejk\", \"ejk\", \"ejk\", \"ejk\", \"ejk\", \"ejk\", \"ejk\", \"ekft\", \"ekft\", \"ekft\", \"ekft\", \"ekft\", \"ekft\", \"ekft\", \"elecbic\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electric\", \"electrosensory\", \"electrosensory\", \"elicitation\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"em\", \"emg\", \"emg\", \"emg\", \"emp\", \"emp\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"eod\", \"eod\", \"eod\", \"eod\", \"eod\", \"eod\", \"eod\", \"eqk\", \"eqk\", \"eqk\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equipotential\", \"equipotential\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"equivalence\", \"erf\", \"erf\", \"erf\", \"erf\", \"erf\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"etann\", \"eubank\", \"eubank\", \"eubank\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"experts\", \"exponentiated\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"fahlman\", \"fahlman\", \"fahlman\", \"fahlman\", \"fahlman\", \"fahlman\", \"fahlman\", \"fahlman\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"fat\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fitness\", \"fmite\", \"fmite\", \"fmite\", \"fmite\", \"fmite\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"fptas\", \"fptas\", \"fptas\", \"fptas\", \"fptas\", \"fptas\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"frontal\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fw\", \"fxy\", \"fxy\", \"fxy\", \"fxy\", \"fxy\", \"fxy\", \"fxy\", \"gains\", \"gains\", \"gains\", \"gains\", \"gains\", \"gains\", \"gains\", \"gains\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gates\", \"gbp\", \"gbp\", \"gbp\", \"gbp\", \"gbp\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"genesis\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"glaus\", \"glaus\", \"gmeans\", \"gmeans\", \"gmeans\", \"gmeans\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"greg\", \"gs\", \"gs\", \"gs\", \"gs\", \"guaranteeing\", \"hairston\", \"hairston\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hallucinations\", \"hamerly\", \"hamerly\", \"harmonium\", \"hasnt\", \"haynes\", \"haynes\", \"haynes\", \"haynes\", \"haynes\", \"haynes\", \"hex\", \"hex\", \"hex\", \"hex\", \"hex\", \"hex\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hommel\", \"hommel\", \"hommel\", \"hommel\", \"horiuchi\", \"horiuchi\", \"horiuchi\", \"horiuchi\", \"horiuchi\", \"horiuchi\", \"horiuchi\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"hs\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"humieres\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ife\", \"ife\", \"ife\", \"ife\", \"ife\", \"ife\", \"ife\", \"ife\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwtll\", \"iiwtll\", \"iixtll\", \"iixtll\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"ijk\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"impedance\", \"impedance\", \"impedance\", \"impedance\", \"impedance\", \"impedance\", \"impedance\", \"incoherence\", \"incoherence\", \"incoherence\", \"incoherence\", \"incoherence\", \"incoherence\", \"incoherence\", \"incoherence\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"intensities\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interface\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"interpreted\", \"inverter\", \"inverter\", \"inverter\", \"inverter\", \"inverter\", \"investors\", \"investors\", \"investors\", \"italy\", \"italy\", \"jacobian\", \"jacobian\", \"jacobian\", \"jacobian\", \"jacobian\", \"jacobian\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"jt\", \"juti\", \"kale\", \"kauffman\", \"kauffman\", \"kcjk\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kinematic\", \"kinematic\", \"kinematic\", \"kinematic\", \"kinematic\", \"kinematic\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"kinematics\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"klc\", \"kolen\", \"kolen\", \"kolen\", \"kolen\", \"kolen\", \"kolen\", \"kolen\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kreutz\", \"kreutz\", \"kreutz\", \"kreutz\", \"kreutz\", \"kreutz\", \"kti\", \"kti\", \"kti\", \"kti\", \"kti\", \"kti\", \"kti\", \"kti\", \"ktr\", \"kurihara\", \"kurihara\", \"kurihara\", \"kurihara\", \"kurihara\", \"kurihara\", \"kurihara\", \"kurihara\", \"kvk\", \"kvk\", \"kvk\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"lawrence\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lilliefors\", \"lilliefors\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"lippman\", \"lissmann\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lkl\", \"lmf\", \"lmf\", \"lmf\", \"lmf\", \"lmf\", \"lmf\", \"lmf\", \"lmf\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"logitboost\", \"loglog\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"lowpass\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"ltp\", \"ltp\", \"ltp\", \"ltp\", \"ltp\", \"ltp\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"lucid\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"maker\", \"maker\", \"maker\", \"maker\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manipulator\", \"manipulator\", \"manipulator\", \"manipulator\", \"manipulator\", \"manipulator\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"marginalised\", \"marginalised\", \"marginalised\", \"marginalised\", \"marginalised\", \"marginalised\", \"marginalised\", \"masnadi\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"means\", \"men\", \"men\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mikio\", \"mikio\", \"mikio\", \"mikio\", \"mine\", \"mine\", \"minp\", \"minp\", \"minp\", \"minp\", \"minp\", \"minp\", \"minp\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistakes\", \"mistakes\", \"mistakes\", \"mistakes\", \"mistakes\", \"mistakes\", \"mistakes\", \"mistakes\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modifiers\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"moody\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mta\", \"mtm\", \"mtm\", \"mtm\", \"mtm\", \"mtm\", \"mtm\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"multivariate\", \"mussa\", \"mussa\", \"nats\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nd\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"nearness\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuronl\", \"neuronl\", \"neuronl\", \"neuronl\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nij\", \"nij\", \"nij\", \"nij\", \"nik\", \"nik\", \"nik\", \"nlpd\", \"nlpd\", \"nlpd\", \"nlpd\", \"nlpd\", \"nlpd\", \"nlpd\", \"nlpd\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"nnc\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"oco\", \"oco\", \"oco\", \"oco\", \"oco\", \"oco\", \"oco\", \"oco\", \"oco\", \"oculomotor\", \"oculomotor\", \"oculomotor\", \"oculomotor\", \"oculomotor\", \"oculomotor\", \"oculomotor\", \"odelia\", \"ofil\", \"ofil\", \"ofil\", \"ofil\", \"oki\", \"oki\", \"oki\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"ot\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pap\", \"pap\", \"pap\", \"pap\", \"pap\", \"pap\", \"pap\", \"papadimitriou\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"payoff\", \"pelf\", \"pelf\", \"pelf\", \"pelf\", \"pelf\", \"pelleg\", \"percep\", \"percep\", \"percep\", \"percep\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pg\", \"pgmeans\", \"pgmeans\", \"pgmeans\", \"pgmeans\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"planar\", \"planar\", \"planar\", \"planar\", \"planar\", \"planar\", \"planar\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasma\", \"plasmas\", \"plasmas\", \"plasmas\", \"plasmas\", \"plasmas\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"pmlp\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointer\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"polack\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"portfolio\", \"portfolio\", \"portfolios\", \"portfolios\", \"portfolios\", \"portfolios\", \"portfolios\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"postponing\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"pretrained\", \"pretrained\", \"pretrained\", \"pretrained\", \"pretrained\", \"pretrained\", \"pretrained\", \"pretraining\", \"pretraining\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primate\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"priors\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"proprioception\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"ptv\", \"ptv\", \"ptv\", \"ptv\", \"ptv\", \"ptv\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"quic\", \"ramanujan\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rasnow\", \"rasnow\", \"rasnow\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"rcc\", \"realization\", \"realization\", \"realization\", \"realization\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"reeves\", \"reeves\", \"reeves\", \"reeves\", \"reeves\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regreti\", \"regreti\", \"replica\", \"replicates\", \"replicates\", \"replicates\", \"replicates\", \"replicates\", \"replicates\", \"residue\", \"residue\", \"residue\", \"residue\", \"residue\", \"residue\", \"residue\", \"responsibility\", \"restored\", \"restored\", \"restored\", \"restored\", \"restored\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retton\", \"retton\", \"retton\", \"retton\", \"retton\", \"retton\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"rin\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risks\", \"risks\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"rosenblatt\", \"rosenblatt\", \"roychowdhury\", \"roychowdhury\", \"roychowdhury\", \"roychowdhury\", \"roychowdhury\", \"rror\", \"rror\", \"rror\", \"rror\", \"rror\", \"rror\", \"rror\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"saccadic\", \"sahar\", \"sahar\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"satises\", \"satises\", \"satises\", \"satises\", \"satises\", \"satises\", \"satises\", \"satises\", \"satises\", \"sato\", \"sato\", \"sato\", \"sato\", \"sato\", \"satyen\", \"saund\", \"saund\", \"saund\", \"saund\", \"savageboost\", \"sbmil\", \"sbmil\", \"sbmil\", \"sbmil\", \"sbmil\", \"sbmil\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"scenarios\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schedules\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schizophrenia\", \"schnurrenberger\", \"schwing\", \"schwing\", \"schwing\", \"schwing\", \"schwing\", \"schwing\", \"schwing\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"selectbatch\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"sensing\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"serial\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shraibman\", \"shraibman\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"sinb\", \"sinb\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"soft\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"somewhere\", \"somewhere\", \"somewhere\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"spectrometry\", \"spectrometry\", \"spectrometry\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"speeds\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spherical\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"spontaneous\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"st\", \"st\", \"st\", \"st\", \"st\", \"st\", \"st\", \"st\", \"st\", \"st\", \"staged\", \"staged\", \"staged\", \"staged\", \"staged\", \"staged\", \"staged\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stateedu\", \"stevens\", \"stevens\", \"stevens\", \"stevens\", \"stevens\", \"stevens\", \"stevens\", \"stevens\", \"stmil\", \"stmil\", \"stmil\", \"stmil\", \"stmil\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"subbands\", \"sublinear\", \"sublinear\", \"sublinear\", \"sublinear\", \"sublinear\", \"submatrix\", \"submatrix\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"symptoms\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"tanh\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"terrorism\", \"terrorism\", \"terrorism\", \"terrorism\", \"terrorism\", \"terrorism\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"tests\", \"textures\", \"textures\", \"textures\", \"textures\", \"textures\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tmi\", \"tmi\", \"tmi\", \"tnk\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamak\", \"tokamaks\", \"tokamaks\", \"tokamaks\", \"tokamaks\", \"tokamaks\", \"tokamaks\", \"toroidal\", \"toroidal\", \"toroidal\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trotman\", \"trotman\", \"trotman\", \"trotman\", \"trotman\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"ultrasonic\", \"ultrasonic\", \"ultrasonic\", \"ultrasonic\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"uniform\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"univariate\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"usher\", \"usher\", \"usher\", \"usher\", \"usher\", \"usher\", \"usher\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"utj\", \"utj\", \"utj\", \"utj\", \"utj\", \"utj\", \"vacuum\", \"vacuum\", \"vacuum\", \"vacuum\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"vasconcelos\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vbem\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vertices\", \"vessel\", \"vessel\", \"vessel\", \"vessel\", \"vessel\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"vis\", \"viterbi\", \"viterbi\", \"viz\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vut\", \"vut\", \"vut\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"warping\", \"warping\", \"warping\", \"warping\", \"warping\", \"warping\", \"warping\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wealth\", \"wealth\", \"wealth\", \"wealth\", \"wealth\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"werbos\", \"werbos\", \"werbos\", \"werbos\", \"werbos\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"wiesel\", \"windsor\", \"windsor\", \"windsor\", \"windsor\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"workspace\", \"workspace\", \"workspace\", \"workspace\", \"workspace\", \"workspace\", \"workspace\", \"workspace\", \"wq\", \"wq\", \"wrapper\", \"wrapper\", \"wrapper\", \"wrapper\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wtll\", \"wtll\", \"wtll\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xk\", \"xmeans\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xreal\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xs\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"yn\", \"zinkevich\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 10, 1, 7, 2, 8, 6, 9, 5, 4]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el148135162240738336100392217\", ldavis_el148135162240738336100392217_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el148135162240738336100392217\", ldavis_el148135162240738336100392217_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el148135162240738336100392217\", ldavis_el148135162240738336100392217_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('/content/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, '/content/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn8nEPhD3Apc"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}